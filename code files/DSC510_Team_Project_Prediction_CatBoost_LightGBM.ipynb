{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC510_Team_Project: Prediction with CatBoost and LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, CatBoost and LightGBM algorithms are utilized for the prediction of the target variable Price for the different books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After relative research, two algorithms-models, which are variants of the original GB algorithm, were found to be ideal for the task. Specifically, CatBoost and LightGBM were utilized as successors of the baseline models of Section.\n",
    "\n",
    "The reason behind the distinct use of the two algorithms is their special treatment of the categorical variables. Unlike the rest of the regression algorithms, which only accept data with numerical values during the training process, the nature of the two algorithms, allows them to handle categorical features efficiently and fast. Therefore, the two algorithms, do not require any special pre-processing like one-hot encoding prior to the fit. Specifically, both of the algorithms do not convert to one-hot coding, and thus are much faster than one-hot encoding. \n",
    "\n",
    "The two regression models use special algorithms to find the split value of the categorical features instead of transforming them to one-hot. The basic idea of LightGBM is to sort the categories according to the training objective at each split. More specifically, LightGBM sorts the histogram (for a categorical feature) according to its accumulated values and then finds the best split on the sorted histogram (https://lightgbm.readthedocs.io/en/latest/Features.html#optimal-split-for-categorical-features). CatBoost converts categorical values into numbers using various statistics on combinations of categorical features and combinations of categorical and numerical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "asZ4J3-IqseV",
    "outputId": "d5f0e342-7c68-4e0a-9e07-a167e25fe6d4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random_state = 420\n",
    "np.random.seed(random_state) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7n9aFn8ugleR"
   },
   "source": [
    "## Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7th3_HqbmuC",
    "outputId": "5fb446cb-31bd-4e96-a876-ecfe8f26f008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7797 entries, 0 to 7796\n",
      "Data columns (total 38 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Reviews       7797 non-null   float64\n",
      " 1   Ratings       7797 non-null   int64  \n",
      " 2   Genre         7797 non-null   object \n",
      " 3   BookCategory  7797 non-null   object \n",
      " 4   Price         6237 non-null   float64\n",
      " 5   Set           7797 non-null   object \n",
      " 6   Authors       7797 non-null   object \n",
      " 7   No. Authors   7797 non-null   int64  \n",
      " 8   Print         7797 non-null   object \n",
      " 9   Type          7797 non-null   object \n",
      " 10  Year          7797 non-null   float64\n",
      " 11  Topic 0       7797 non-null   float64\n",
      " 12  Topic 1       7797 non-null   float64\n",
      " 13  Topic 2       7797 non-null   float64\n",
      " 14  Topic 3       7797 non-null   float64\n",
      " 15  Topic 4       7797 non-null   float64\n",
      " 16  Topic 5       7797 non-null   float64\n",
      " 17  Topic 6       7797 non-null   float64\n",
      " 18  Topic 7       7797 non-null   float64\n",
      " 19  Topic 8       7797 non-null   float64\n",
      " 20  Topic 9       7797 non-null   float64\n",
      " 21  Topic 10      7797 non-null   float64\n",
      " 22  Topic 11      7797 non-null   float64\n",
      " 23  Topic 12      7797 non-null   float64\n",
      " 24  Topic 13      7797 non-null   float64\n",
      " 25  Topic 14      7797 non-null   float64\n",
      " 26  Topic 15      7797 non-null   float64\n",
      " 27  Topic 16      7797 non-null   float64\n",
      " 28  Topic 17      7797 non-null   float64\n",
      " 29  Topic 18      7797 non-null   float64\n",
      " 30  Topic 19      7797 non-null   float64\n",
      " 31  Topic 20      7797 non-null   float64\n",
      " 32  Topic 21      7797 non-null   float64\n",
      " 33  Topic 22      7797 non-null   float64\n",
      " 34  Topic 23      7797 non-null   float64\n",
      " 35  Topic 24      7797 non-null   float64\n",
      " 36  Cos_Month     7797 non-null   float64\n",
      " 37  Sin_Month     7797 non-null   float64\n",
      "dtypes: float64(30), int64(2), object(6)\n",
      "memory usage: 2.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df = pd.read_csv('finalized_data_df.csv')\n",
    "display(data_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data partition and evaluation function-metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split to the original train and test and drop the 'set' attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = data_df[data_df['Set'] == 'train']\n",
    "# y_train = X_train['Price']\n",
    "# X_train =  X_train.drop(['Set', 'Price', 'No. Authors', 'Cos_Month', 'Sin_Month'], axis=1)\n",
    "\n",
    "# X_test = data_df[data_df['Set'] == 'test']\n",
    "# #y_test = X_test['Price']\n",
    "# X_test =  X_test.drop(['Set', 'Price', 'No. Authors', 'Cos_Month', 'Sin_Month'], axis=1)\n",
    "\n",
    "X_train = data_df[data_df['Set'] == 'train']\n",
    "y_train = X_train['Price']\n",
    "X_train =  X_train.drop(['Set', 'Price'], axis=1)\n",
    "\n",
    "X_test = data_df[data_df['Set'] == 'test']\n",
    "#y_test = X_test['Price']\n",
    "X_test =  X_test.drop(['Set', 'Price'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement evaluation function same as the one from Machine Hack for this task.\n",
    "\n",
    "Specifically, the evaluation metric used is the Negative Root Mean Squared Logarithmic Error (NRMSLE):\n",
    "\n",
    "$$ \\text { NRMSLE }= 1 - \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N}\\left(\\log \\left(y_{i}+1\\right)-\\log \\left(\\hat{y}_{i}+1\\right)\\right)^{2}} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $N$  is the total number of observations in the dataset\n",
    "\n",
    "- $\\hat{y_i}$ is the prediction of $i_{th}$ target\n",
    "\n",
    "- $y_i$ is the actual target for i\n",
    "\n",
    "- log(.) gives the natural logarithm of the input\n",
    "\n",
    "NRMSLE adds one to both actual and predicted values before taking the natural logarithm, to avoid taking the natural log of possible zero values. As a result, the metric can be used if actual or predicted have zero-valued elements. https://hrngok.github.io/posts/metrics/\n",
    "\n",
    "The metric is not appropriate if either is negative valued, but in our case since the target variable is the price, it cannot be negative.\n",
    "\n",
    "Futhermore, the metric-function is ideal when we donâ€™t want to penalize big differences when both the predicted and the actual are big numbers, and in cases where we want to penalize under-estimates more than over-estimates.\n",
    "(Kirubakumaresh (KK) Rajendran, Data Science Consultant)\n",
    "\n",
    "It was found that often in businesses, penalizing the under-estimate more than over-estimate is important for prediction of sales and inventory demands. To some extent having extra inventory or supply might be more preferable to not being able to providing product as much as the demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nrmsle(y_true, y_pred):\n",
    "    return 1 - np.sqrt(np.square(np.log10(y_pred +1) - np.log10(y_true +1)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing custom-user defined loss function for the CatBoost algorithm-package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Class which will be used for the CatBoostRegressor evaluation\n",
    "class NegativeRMSLEMetric(object):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return 1 - np.sqrt(error / (weight + 1e-38))\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        return True # CRUCIAL HERE\n",
    "\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        assert len(approxes) == 1\n",
    "        assert len(target) == len(approxes[0])\n",
    "\n",
    "        approx = approxes[0]\n",
    "\n",
    "        error_sum = 0.0\n",
    "        weight_sum = 0.0\n",
    "\n",
    "        for i in range(len(approx)):\n",
    "            w = 1.0 if weight is None else weight[i]\n",
    "            weight_sum += w\n",
    "            #1 - np.sqrt(np.square(np.log10(y_pred +1) - np.log10(y_true +1)).mean())\n",
    "            error_sum += w * ((np.log10(approx[i]+1)  - np.log10(target[i]+1))**2)\n",
    "\n",
    "        return error_sum, weight_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling with all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, all the features were fit in the models in order to see how the algorithms perform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Modelling with all features: Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/catboost/core.py:1953: UserWarning: Failed to import numba for optimizing custom metrics and objectives\n",
      "  _check_train_params(params)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e5a91ccdfb4ebe9a285a8219d2853f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bestTest = 0.7297529522\n",
      "bestIteration = 498\n",
      "\n",
      "0:\tloss: 0.7297530\tbest: 0.7297530 (0)\ttotal: 36s\tremaining: 6m 35s\n",
      "\n",
      "bestTest = 0.733805641\n",
      "bestIteration = 498\n",
      "\n",
      "1:\tloss: 0.7338056\tbest: 0.7338056 (1)\ttotal: 1m 10s\tremaining: 5m 50s\n",
      "\n",
      "bestTest = 0.7335189622\n",
      "bestIteration = 998\n",
      "\n",
      "2:\tloss: 0.7335190\tbest: 0.7338056 (1)\ttotal: 2m 23s\tremaining: 7m 9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6634/1355284671.py:27: RuntimeWarning: invalid value encountered in log10\n",
      "  error_sum += w * ((np.log10(approx[i]+1)  - np.log10(target[i]+1))**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bestTest = 0.7350061582\n",
      "bestIteration = 662\n",
      "\n",
      "3:\tloss: 0.7350062\tbest: 0.7350062 (3)\ttotal: 3m 32s\tremaining: 7m 5s\n",
      "\n",
      "bestTest = 0.7409201074\n",
      "bestIteration = 499\n",
      "\n",
      "4:\tloss: 0.7409201\tbest: 0.7409201 (4)\ttotal: 5m 1s\tremaining: 7m 1s\n",
      "\n",
      "bestTest = 0.7428921028\n",
      "bestIteration = 330\n",
      "\n",
      "5:\tloss: 0.7428921\tbest: 0.7428921 (5)\ttotal: 6m 36s\tremaining: 6m 36s\n",
      "\n",
      "bestTest = 0.7442468854\n",
      "bestIteration = 997\n",
      "\n",
      "6:\tloss: 0.7442469\tbest: 0.7442469 (6)\ttotal: 9m 15s\tremaining: 6m 36s\n",
      "\n",
      "bestTest = 0.7428921028\n",
      "bestIteration = 330\n",
      "\n",
      "7:\tloss: 0.7428921\tbest: 0.7442469 (6)\ttotal: 11m 52s\tremaining: 5m 56s\n",
      "\n",
      "bestTest = 0.7411198431\n",
      "bestIteration = 499\n",
      "\n",
      "8:\tloss: 0.7411198\tbest: 0.7442469 (6)\ttotal: 37m 9s\tremaining: 12m 23s\n",
      "\n",
      "bestTest = 0.743086935\n",
      "bestIteration = 499\n",
      "\n",
      "9:\tloss: 0.7430869\tbest: 0.7442469 (6)\ttotal: 59m 40s\tremaining: 11m 56s\n",
      "\n",
      "bestTest = 0.7441660202\n",
      "bestIteration = 997\n",
      "\n",
      "10:\tloss: 0.7441660\tbest: 0.7442469 (6)\ttotal: 1h 36m 30s\tremaining: 8m 46s\n",
      "\n",
      "bestTest = 0.7435953173\n",
      "bestIteration = 851\n",
      "\n",
      "11:\tloss: 0.7435953\tbest: 0.7442469 (6)\ttotal: 2h 13m 29s\tremaining: 0us\n",
      "Estimating final quality...\n",
      "Training on fold [0/10]\n",
      "\n",
      "bestTest = 0.7471938949\n",
      "bestIteration = 929\n",
      "\n",
      "Training on fold [1/10]\n",
      "\n",
      "bestTest = 0.736157404\n",
      "bestIteration = 997\n",
      "\n",
      "Training on fold [2/10]\n",
      "\n",
      "bestTest = 0.7278612649\n",
      "bestIteration = 997\n",
      "\n",
      "Training on fold [3/10]\n",
      "\n",
      "bestTest = 0.745225332\n",
      "bestIteration = 999\n",
      "\n",
      "Training on fold [4/10]\n",
      "\n",
      "bestTest = 0.7490072308\n",
      "bestIteration = 932\n",
      "\n",
      "Training on fold [5/10]\n",
      "\n",
      "bestTest = 0.7318630831\n",
      "bestIteration = 684\n",
      "\n",
      "Training on fold [6/10]\n",
      "\n",
      "bestTest = 0.7557642774\n",
      "bestIteration = 691\n",
      "\n",
      "Training on fold [7/10]\n",
      "\n",
      "bestTest = 0.7287846416\n",
      "bestIteration = 884\n",
      "\n",
      "Training on fold [8/10]\n",
      "\n",
      "bestTest = 0.7406545131\n",
      "bestIteration = 925\n",
      "\n",
      "Training on fold [9/10]\n",
      "\n",
      "bestTest = 0.7495691911\n",
      "bestIteration = 983\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': {'depth': 10, 'iterations': 1000, 'learning_rate': 0.025},\n",
       " 'cv_results': defaultdict(list,\n",
       "             {'iterations': [0,\n",
       "               1,\n",
       "               2,\n",
       "               3,\n",
       "               4,\n",
       "               5,\n",
       "               6,\n",
       "               7,\n",
       "               8,\n",
       "               9,\n",
       "               10,\n",
       "               11,\n",
       "               12,\n",
       "               13,\n",
       "               14,\n",
       "               15,\n",
       "               16,\n",
       "               17,\n",
       "               18,\n",
       "               19,\n",
       "               20,\n",
       "               21,\n",
       "               22,\n",
       "               23,\n",
       "               24,\n",
       "               25,\n",
       "               26,\n",
       "               27,\n",
       "               28,\n",
       "               29,\n",
       "               30,\n",
       "               31,\n",
       "               32,\n",
       "               33,\n",
       "               34,\n",
       "               35,\n",
       "               36,\n",
       "               37,\n",
       "               38,\n",
       "               39,\n",
       "               40,\n",
       "               41,\n",
       "               42,\n",
       "               43,\n",
       "               44,\n",
       "               45,\n",
       "               46,\n",
       "               47,\n",
       "               48,\n",
       "               49,\n",
       "               50,\n",
       "               51,\n",
       "               52,\n",
       "               53,\n",
       "               54,\n",
       "               55,\n",
       "               56,\n",
       "               57,\n",
       "               58,\n",
       "               59,\n",
       "               60,\n",
       "               61,\n",
       "               62,\n",
       "               63,\n",
       "               64,\n",
       "               65,\n",
       "               66,\n",
       "               67,\n",
       "               68,\n",
       "               69,\n",
       "               70,\n",
       "               71,\n",
       "               72,\n",
       "               73,\n",
       "               74,\n",
       "               75,\n",
       "               76,\n",
       "               77,\n",
       "               78,\n",
       "               79,\n",
       "               80,\n",
       "               81,\n",
       "               82,\n",
       "               83,\n",
       "               84,\n",
       "               85,\n",
       "               86,\n",
       "               87,\n",
       "               88,\n",
       "               89,\n",
       "               90,\n",
       "               91,\n",
       "               92,\n",
       "               93,\n",
       "               94,\n",
       "               95,\n",
       "               96,\n",
       "               97,\n",
       "               98,\n",
       "               99,\n",
       "               100,\n",
       "               101,\n",
       "               102,\n",
       "               103,\n",
       "               104,\n",
       "               105,\n",
       "               106,\n",
       "               107,\n",
       "               108,\n",
       "               109,\n",
       "               110,\n",
       "               111,\n",
       "               112,\n",
       "               113,\n",
       "               114,\n",
       "               115,\n",
       "               116,\n",
       "               117,\n",
       "               118,\n",
       "               119,\n",
       "               120,\n",
       "               121,\n",
       "               122,\n",
       "               123,\n",
       "               124,\n",
       "               125,\n",
       "               126,\n",
       "               127,\n",
       "               128,\n",
       "               129,\n",
       "               130,\n",
       "               131,\n",
       "               132,\n",
       "               133,\n",
       "               134,\n",
       "               135,\n",
       "               136,\n",
       "               137,\n",
       "               138,\n",
       "               139,\n",
       "               140,\n",
       "               141,\n",
       "               142,\n",
       "               143,\n",
       "               144,\n",
       "               145,\n",
       "               146,\n",
       "               147,\n",
       "               148,\n",
       "               149,\n",
       "               150,\n",
       "               151,\n",
       "               152,\n",
       "               153,\n",
       "               154,\n",
       "               155,\n",
       "               156,\n",
       "               157,\n",
       "               158,\n",
       "               159,\n",
       "               160,\n",
       "               161,\n",
       "               162,\n",
       "               163,\n",
       "               164,\n",
       "               165,\n",
       "               166,\n",
       "               167,\n",
       "               168,\n",
       "               169,\n",
       "               170,\n",
       "               171,\n",
       "               172,\n",
       "               173,\n",
       "               174,\n",
       "               175,\n",
       "               176,\n",
       "               177,\n",
       "               178,\n",
       "               179,\n",
       "               180,\n",
       "               181,\n",
       "               182,\n",
       "               183,\n",
       "               184,\n",
       "               185,\n",
       "               186,\n",
       "               187,\n",
       "               188,\n",
       "               189,\n",
       "               190,\n",
       "               191,\n",
       "               192,\n",
       "               193,\n",
       "               194,\n",
       "               195,\n",
       "               196,\n",
       "               197,\n",
       "               198,\n",
       "               199,\n",
       "               200,\n",
       "               201,\n",
       "               202,\n",
       "               203,\n",
       "               204,\n",
       "               205,\n",
       "               206,\n",
       "               207,\n",
       "               208,\n",
       "               209,\n",
       "               210,\n",
       "               211,\n",
       "               212,\n",
       "               213,\n",
       "               214,\n",
       "               215,\n",
       "               216,\n",
       "               217,\n",
       "               218,\n",
       "               219,\n",
       "               220,\n",
       "               221,\n",
       "               222,\n",
       "               223,\n",
       "               224,\n",
       "               225,\n",
       "               226,\n",
       "               227,\n",
       "               228,\n",
       "               229,\n",
       "               230,\n",
       "               231,\n",
       "               232,\n",
       "               233,\n",
       "               234,\n",
       "               235,\n",
       "               236,\n",
       "               237,\n",
       "               238,\n",
       "               239,\n",
       "               240,\n",
       "               241,\n",
       "               242,\n",
       "               243,\n",
       "               244,\n",
       "               245,\n",
       "               246,\n",
       "               247,\n",
       "               248,\n",
       "               249,\n",
       "               250,\n",
       "               251,\n",
       "               252,\n",
       "               253,\n",
       "               254,\n",
       "               255,\n",
       "               256,\n",
       "               257,\n",
       "               258,\n",
       "               259,\n",
       "               260,\n",
       "               261,\n",
       "               262,\n",
       "               263,\n",
       "               264,\n",
       "               265,\n",
       "               266,\n",
       "               267,\n",
       "               268,\n",
       "               269,\n",
       "               270,\n",
       "               271,\n",
       "               272,\n",
       "               273,\n",
       "               274,\n",
       "               275,\n",
       "               276,\n",
       "               277,\n",
       "               278,\n",
       "               279,\n",
       "               280,\n",
       "               281,\n",
       "               282,\n",
       "               283,\n",
       "               284,\n",
       "               285,\n",
       "               286,\n",
       "               287,\n",
       "               288,\n",
       "               289,\n",
       "               290,\n",
       "               291,\n",
       "               292,\n",
       "               293,\n",
       "               294,\n",
       "               295,\n",
       "               296,\n",
       "               297,\n",
       "               298,\n",
       "               299,\n",
       "               300,\n",
       "               301,\n",
       "               302,\n",
       "               303,\n",
       "               304,\n",
       "               305,\n",
       "               306,\n",
       "               307,\n",
       "               308,\n",
       "               309,\n",
       "               310,\n",
       "               311,\n",
       "               312,\n",
       "               313,\n",
       "               314,\n",
       "               315,\n",
       "               316,\n",
       "               317,\n",
       "               318,\n",
       "               319,\n",
       "               320,\n",
       "               321,\n",
       "               322,\n",
       "               323,\n",
       "               324,\n",
       "               325,\n",
       "               326,\n",
       "               327,\n",
       "               328,\n",
       "               329,\n",
       "               330,\n",
       "               331,\n",
       "               332,\n",
       "               333,\n",
       "               334,\n",
       "               335,\n",
       "               336,\n",
       "               337,\n",
       "               338,\n",
       "               339,\n",
       "               340,\n",
       "               341,\n",
       "               342,\n",
       "               343,\n",
       "               344,\n",
       "               345,\n",
       "               346,\n",
       "               347,\n",
       "               348,\n",
       "               349,\n",
       "               350,\n",
       "               351,\n",
       "               352,\n",
       "               353,\n",
       "               354,\n",
       "               355,\n",
       "               356,\n",
       "               357,\n",
       "               358,\n",
       "               359,\n",
       "               360,\n",
       "               361,\n",
       "               362,\n",
       "               363,\n",
       "               364,\n",
       "               365,\n",
       "               366,\n",
       "               367,\n",
       "               368,\n",
       "               369,\n",
       "               370,\n",
       "               371,\n",
       "               372,\n",
       "               373,\n",
       "               374,\n",
       "               375,\n",
       "               376,\n",
       "               377,\n",
       "               378,\n",
       "               379,\n",
       "               380,\n",
       "               381,\n",
       "               382,\n",
       "               383,\n",
       "               384,\n",
       "               385,\n",
       "               386,\n",
       "               387,\n",
       "               388,\n",
       "               389,\n",
       "               390,\n",
       "               391,\n",
       "               392,\n",
       "               393,\n",
       "               394,\n",
       "               395,\n",
       "               396,\n",
       "               397,\n",
       "               398,\n",
       "               399,\n",
       "               400,\n",
       "               401,\n",
       "               402,\n",
       "               403,\n",
       "               404,\n",
       "               405,\n",
       "               406,\n",
       "               407,\n",
       "               408,\n",
       "               409,\n",
       "               410,\n",
       "               411,\n",
       "               412,\n",
       "               413,\n",
       "               414,\n",
       "               415,\n",
       "               416,\n",
       "               417,\n",
       "               418,\n",
       "               419,\n",
       "               420,\n",
       "               421,\n",
       "               422,\n",
       "               423,\n",
       "               424,\n",
       "               425,\n",
       "               426,\n",
       "               427,\n",
       "               428,\n",
       "               429,\n",
       "               430,\n",
       "               431,\n",
       "               432,\n",
       "               433,\n",
       "               434,\n",
       "               435,\n",
       "               436,\n",
       "               437,\n",
       "               438,\n",
       "               439,\n",
       "               440,\n",
       "               441,\n",
       "               442,\n",
       "               443,\n",
       "               444,\n",
       "               445,\n",
       "               446,\n",
       "               447,\n",
       "               448,\n",
       "               449,\n",
       "               450,\n",
       "               451,\n",
       "               452,\n",
       "               453,\n",
       "               454,\n",
       "               455,\n",
       "               456,\n",
       "               457,\n",
       "               458,\n",
       "               459,\n",
       "               460,\n",
       "               461,\n",
       "               462,\n",
       "               463,\n",
       "               464,\n",
       "               465,\n",
       "               466,\n",
       "               467,\n",
       "               468,\n",
       "               469,\n",
       "               470,\n",
       "               471,\n",
       "               472,\n",
       "               473,\n",
       "               474,\n",
       "               475,\n",
       "               476,\n",
       "               477,\n",
       "               478,\n",
       "               479,\n",
       "               480,\n",
       "               481,\n",
       "               482,\n",
       "               483,\n",
       "               484,\n",
       "               485,\n",
       "               486,\n",
       "               487,\n",
       "               488,\n",
       "               489,\n",
       "               490,\n",
       "               491,\n",
       "               492,\n",
       "               493,\n",
       "               494,\n",
       "               495,\n",
       "               496,\n",
       "               497,\n",
       "               498,\n",
       "               499,\n",
       "               500,\n",
       "               501,\n",
       "               502,\n",
       "               503,\n",
       "               504,\n",
       "               505,\n",
       "               506,\n",
       "               507,\n",
       "               508,\n",
       "               509,\n",
       "               510,\n",
       "               511,\n",
       "               512,\n",
       "               513,\n",
       "               514,\n",
       "               515,\n",
       "               516,\n",
       "               517,\n",
       "               518,\n",
       "               519,\n",
       "               520,\n",
       "               521,\n",
       "               522,\n",
       "               523,\n",
       "               524,\n",
       "               525,\n",
       "               526,\n",
       "               527,\n",
       "               528,\n",
       "               529,\n",
       "               530,\n",
       "               531,\n",
       "               532,\n",
       "               533,\n",
       "               534,\n",
       "               535,\n",
       "               536,\n",
       "               537,\n",
       "               538,\n",
       "               539,\n",
       "               540,\n",
       "               541,\n",
       "               542,\n",
       "               543,\n",
       "               544,\n",
       "               545,\n",
       "               546,\n",
       "               547,\n",
       "               548,\n",
       "               549,\n",
       "               550,\n",
       "               551,\n",
       "               552,\n",
       "               553,\n",
       "               554,\n",
       "               555,\n",
       "               556,\n",
       "               557,\n",
       "               558,\n",
       "               559,\n",
       "               560,\n",
       "               561,\n",
       "               562,\n",
       "               563,\n",
       "               564,\n",
       "               565,\n",
       "               566,\n",
       "               567,\n",
       "               568,\n",
       "               569,\n",
       "               570,\n",
       "               571,\n",
       "               572,\n",
       "               573,\n",
       "               574,\n",
       "               575,\n",
       "               576,\n",
       "               577,\n",
       "               578,\n",
       "               579,\n",
       "               580,\n",
       "               581,\n",
       "               582,\n",
       "               583,\n",
       "               584,\n",
       "               585,\n",
       "               586,\n",
       "               587,\n",
       "               588,\n",
       "               589,\n",
       "               590,\n",
       "               591,\n",
       "               592,\n",
       "               593,\n",
       "               594,\n",
       "               595,\n",
       "               596,\n",
       "               597,\n",
       "               598,\n",
       "               599,\n",
       "               600,\n",
       "               601,\n",
       "               602,\n",
       "               603,\n",
       "               604,\n",
       "               605,\n",
       "               606,\n",
       "               607,\n",
       "               608,\n",
       "               609,\n",
       "               610,\n",
       "               611,\n",
       "               612,\n",
       "               613,\n",
       "               614,\n",
       "               615,\n",
       "               616,\n",
       "               617,\n",
       "               618,\n",
       "               619,\n",
       "               620,\n",
       "               621,\n",
       "               622,\n",
       "               623,\n",
       "               624,\n",
       "               625,\n",
       "               626,\n",
       "               627,\n",
       "               628,\n",
       "               629,\n",
       "               630,\n",
       "               631,\n",
       "               632,\n",
       "               633,\n",
       "               634,\n",
       "               635,\n",
       "               636,\n",
       "               637,\n",
       "               638,\n",
       "               639,\n",
       "               640,\n",
       "               641,\n",
       "               642,\n",
       "               643,\n",
       "               644,\n",
       "               645,\n",
       "               646,\n",
       "               647,\n",
       "               648,\n",
       "               649,\n",
       "               650,\n",
       "               651,\n",
       "               652,\n",
       "               653,\n",
       "               654,\n",
       "               655,\n",
       "               656,\n",
       "               657,\n",
       "               658,\n",
       "               659,\n",
       "               660,\n",
       "               661,\n",
       "               662,\n",
       "               663,\n",
       "               664,\n",
       "               665,\n",
       "               666,\n",
       "               667,\n",
       "               668,\n",
       "               669,\n",
       "               670,\n",
       "               671,\n",
       "               672,\n",
       "               673,\n",
       "               674,\n",
       "               675,\n",
       "               676,\n",
       "               677,\n",
       "               678,\n",
       "               679,\n",
       "               680,\n",
       "               681,\n",
       "               682,\n",
       "               683,\n",
       "               684,\n",
       "               685,\n",
       "               686,\n",
       "               687,\n",
       "               688,\n",
       "               689,\n",
       "               690,\n",
       "               691,\n",
       "               692,\n",
       "               693,\n",
       "               694,\n",
       "               695,\n",
       "               696,\n",
       "               697,\n",
       "               698,\n",
       "               699,\n",
       "               700,\n",
       "               701,\n",
       "               702,\n",
       "               703,\n",
       "               704,\n",
       "               705,\n",
       "               706,\n",
       "               707,\n",
       "               708,\n",
       "               709,\n",
       "               710,\n",
       "               711,\n",
       "               712,\n",
       "               713,\n",
       "               714,\n",
       "               715,\n",
       "               716,\n",
       "               717,\n",
       "               718,\n",
       "               719,\n",
       "               720,\n",
       "               721,\n",
       "               722,\n",
       "               723,\n",
       "               724,\n",
       "               725,\n",
       "               726,\n",
       "               727,\n",
       "               728,\n",
       "               729,\n",
       "               730,\n",
       "               731,\n",
       "               732,\n",
       "               733,\n",
       "               734,\n",
       "               735,\n",
       "               736,\n",
       "               737,\n",
       "               738,\n",
       "               739,\n",
       "               740,\n",
       "               741,\n",
       "               742,\n",
       "               743,\n",
       "               744,\n",
       "               745,\n",
       "               746,\n",
       "               747,\n",
       "               748,\n",
       "               749,\n",
       "               750,\n",
       "               751,\n",
       "               752,\n",
       "               753,\n",
       "               754,\n",
       "               755,\n",
       "               756,\n",
       "               757,\n",
       "               758,\n",
       "               759,\n",
       "               760,\n",
       "               761,\n",
       "               762,\n",
       "               763,\n",
       "               764,\n",
       "               765,\n",
       "               766,\n",
       "               767,\n",
       "               768,\n",
       "               769,\n",
       "               770,\n",
       "               771,\n",
       "               772,\n",
       "               773,\n",
       "               774,\n",
       "               775,\n",
       "               776,\n",
       "               777,\n",
       "               778,\n",
       "               779,\n",
       "               780,\n",
       "               781,\n",
       "               782,\n",
       "               783,\n",
       "               784,\n",
       "               785,\n",
       "               786,\n",
       "               787,\n",
       "               788,\n",
       "               789,\n",
       "               790,\n",
       "               791,\n",
       "               792,\n",
       "               793,\n",
       "               794,\n",
       "               795,\n",
       "               796,\n",
       "               797,\n",
       "               798,\n",
       "               799,\n",
       "               800,\n",
       "               801,\n",
       "               802,\n",
       "               803,\n",
       "               804,\n",
       "               805,\n",
       "               806,\n",
       "               807,\n",
       "               808,\n",
       "               809,\n",
       "               810,\n",
       "               811,\n",
       "               812,\n",
       "               813,\n",
       "               814,\n",
       "               815,\n",
       "               816,\n",
       "               817,\n",
       "               818,\n",
       "               819,\n",
       "               820,\n",
       "               821,\n",
       "               822,\n",
       "               823,\n",
       "               824,\n",
       "               825,\n",
       "               826,\n",
       "               827,\n",
       "               828,\n",
       "               829,\n",
       "               830,\n",
       "               831,\n",
       "               832,\n",
       "               833,\n",
       "               834,\n",
       "               835,\n",
       "               836,\n",
       "               837,\n",
       "               838,\n",
       "               839,\n",
       "               840,\n",
       "               841,\n",
       "               842,\n",
       "               843,\n",
       "               844,\n",
       "               845,\n",
       "               846,\n",
       "               847,\n",
       "               848,\n",
       "               849,\n",
       "               850,\n",
       "               851,\n",
       "               852,\n",
       "               853,\n",
       "               854,\n",
       "               855,\n",
       "               856,\n",
       "               857,\n",
       "               858,\n",
       "               859,\n",
       "               860,\n",
       "               861,\n",
       "               862,\n",
       "               863,\n",
       "               864,\n",
       "               865,\n",
       "               866,\n",
       "               867,\n",
       "               868,\n",
       "               869,\n",
       "               870,\n",
       "               871,\n",
       "               872,\n",
       "               873,\n",
       "               874,\n",
       "               875,\n",
       "               876,\n",
       "               877,\n",
       "               878,\n",
       "               879,\n",
       "               880,\n",
       "               881,\n",
       "               882,\n",
       "               883,\n",
       "               884,\n",
       "               885,\n",
       "               886,\n",
       "               887,\n",
       "               888,\n",
       "               889,\n",
       "               890,\n",
       "               891,\n",
       "               892,\n",
       "               893,\n",
       "               894,\n",
       "               895,\n",
       "               896,\n",
       "               897,\n",
       "               898,\n",
       "               899,\n",
       "               900,\n",
       "               901,\n",
       "               902,\n",
       "               903,\n",
       "               904,\n",
       "               905,\n",
       "               906,\n",
       "               907,\n",
       "               908,\n",
       "               909,\n",
       "               910,\n",
       "               911,\n",
       "               912,\n",
       "               913,\n",
       "               914,\n",
       "               915,\n",
       "               916,\n",
       "               917,\n",
       "               918,\n",
       "               919,\n",
       "               920,\n",
       "               921,\n",
       "               922,\n",
       "               923,\n",
       "               924,\n",
       "               925,\n",
       "               926,\n",
       "               927,\n",
       "               928,\n",
       "               929,\n",
       "               930,\n",
       "               931,\n",
       "               932,\n",
       "               933,\n",
       "               934,\n",
       "               935,\n",
       "               936,\n",
       "               937,\n",
       "               938,\n",
       "               939,\n",
       "               940,\n",
       "               941,\n",
       "               942,\n",
       "               943,\n",
       "               944,\n",
       "               945,\n",
       "               946,\n",
       "               947,\n",
       "               948,\n",
       "               949,\n",
       "               950,\n",
       "               951,\n",
       "               952,\n",
       "               953,\n",
       "               954,\n",
       "               955,\n",
       "               956,\n",
       "               957,\n",
       "               958,\n",
       "               959,\n",
       "               960,\n",
       "               961,\n",
       "               962,\n",
       "               963,\n",
       "               964,\n",
       "               965,\n",
       "               966,\n",
       "               967,\n",
       "               968,\n",
       "               969,\n",
       "               970,\n",
       "               971,\n",
       "               972,\n",
       "               973,\n",
       "               974,\n",
       "               975,\n",
       "               976,\n",
       "               977,\n",
       "               978,\n",
       "               979,\n",
       "               980,\n",
       "               981,\n",
       "               982,\n",
       "               983,\n",
       "               984,\n",
       "               985,\n",
       "               986,\n",
       "               987,\n",
       "               988,\n",
       "               989,\n",
       "               990,\n",
       "               991,\n",
       "               992,\n",
       "               993,\n",
       "               994,\n",
       "               995,\n",
       "               996,\n",
       "               997,\n",
       "               998,\n",
       "               999],\n",
       "              'test-NegativeRMSLEMetric-mean': [-0.5122798243729499,\n",
       "               -0.2386226558568217,\n",
       "               -0.07872112449732456,\n",
       "               0.03359117935512372,\n",
       "               0.11996914594389922,\n",
       "               0.18472484713087853,\n",
       "               0.241559461617111,\n",
       "               0.2905796911015011,\n",
       "               0.33295339573044436,\n",
       "               0.36872654987522735,\n",
       "               0.3996102888831504,\n",
       "               0.4279110166979264,\n",
       "               0.4524490073073519,\n",
       "               0.4752878240050077,\n",
       "               0.4963192394804892,\n",
       "               0.5145878942699318,\n",
       "               0.5312484472883924,\n",
       "               0.5465308427480874,\n",
       "               0.5611267526621906,\n",
       "               0.5738867872405229,\n",
       "               0.5858600553626696,\n",
       "               0.5963708110549085,\n",
       "               0.6064984889334978,\n",
       "               0.6156787267981476,\n",
       "               0.6248041062264567,\n",
       "               0.6327605016761246,\n",
       "               0.6399772111305014,\n",
       "               0.6465758340530222,\n",
       "               0.6527722457727658,\n",
       "               0.6585188768897352,\n",
       "               0.6639016837325252,\n",
       "               0.6690286803041268,\n",
       "               0.6733939359044572,\n",
       "               0.6775426709573112,\n",
       "               0.6814058300753878,\n",
       "               0.6852751452847986,\n",
       "               0.6886330452042395,\n",
       "               0.6918772226243792,\n",
       "               0.6948472519936203,\n",
       "               0.6975183817679055,\n",
       "               0.6999953480566259,\n",
       "               0.7022456591470285,\n",
       "               0.7043244351119406,\n",
       "               0.7062457876161841,\n",
       "               0.7080595660251502,\n",
       "               0.7097721964375633,\n",
       "               0.711178008262684,\n",
       "               0.712516069188455,\n",
       "               0.7137843944797744,\n",
       "               0.7149347352996733,\n",
       "               0.7160027662179158,\n",
       "               0.71684374332549,\n",
       "               0.7176379202945087,\n",
       "               0.7184829242346806,\n",
       "               0.7191681255108817,\n",
       "               0.71985425997288,\n",
       "               0.7203740247118239,\n",
       "               0.7209240749738199,\n",
       "               0.721325205844983,\n",
       "               0.7217713966464021,\n",
       "               0.7221388878924205,\n",
       "               0.7224459358007539,\n",
       "               0.7227425429721726,\n",
       "               0.723038038160484,\n",
       "               0.7233190272764316,\n",
       "               0.7235048242029444,\n",
       "               0.7236323479870782,\n",
       "               0.7238022429740247,\n",
       "               0.7239628587673559,\n",
       "               0.7240172230274975,\n",
       "               0.7240827535958612,\n",
       "               0.7241069813491956,\n",
       "               0.7240925042856896,\n",
       "               0.7241622375687532,\n",
       "               0.7241716309821973,\n",
       "               0.7241310123928149,\n",
       "               0.7240988136221406,\n",
       "               0.724046991706264,\n",
       "               0.7240467691516336,\n",
       "               0.7239360545387926,\n",
       "               0.7238211185661665,\n",
       "               0.7237732306749024,\n",
       "               0.7237154821978674,\n",
       "               0.7236446862885599,\n",
       "               0.723673618886983,\n",
       "               0.723611499053243,\n",
       "               0.7234977089607325,\n",
       "               0.7234520649863451,\n",
       "               0.7233626518902561,\n",
       "               0.7232414318946663,\n",
       "               0.7231307171969508,\n",
       "               0.72300558463681,\n",
       "               0.722901653103434,\n",
       "               0.7227892371721472,\n",
       "               0.722653056690694,\n",
       "               0.7225214647865876,\n",
       "               0.7224554170918948,\n",
       "               0.722301736380303,\n",
       "               0.722233468907161,\n",
       "               0.7220719831122488,\n",
       "               0.7219207562065579,\n",
       "               0.7218281477485812,\n",
       "               0.7216856128747573,\n",
       "               0.7215510755062156,\n",
       "               0.7214057934359582,\n",
       "               0.7212959754813507,\n",
       "               0.7212097891098923,\n",
       "               0.7211744028747636,\n",
       "               0.7211125126914885,\n",
       "               0.7210498952566147,\n",
       "               0.7210136642024906,\n",
       "               0.720985997509555,\n",
       "               0.7209911893585014,\n",
       "               0.7210697426643032,\n",
       "               0.7211335870350666,\n",
       "               0.7212570441422765,\n",
       "               0.7213461338234571,\n",
       "               0.7214217488903771,\n",
       "               0.7215520674206122,\n",
       "               0.7216863654476051,\n",
       "               0.7219118244666808,\n",
       "               0.7221353417362912,\n",
       "               0.7223285066927384,\n",
       "               0.7225348254879291,\n",
       "               0.7227774273025208,\n",
       "               0.7230553249003148,\n",
       "               0.7233238874865526,\n",
       "               0.72350034434741,\n",
       "               0.7237496371769339,\n",
       "               0.7240846736152362,\n",
       "               0.7244034383431238,\n",
       "               0.7246862458598439,\n",
       "               0.7249371800615518,\n",
       "               0.7252011139198309,\n",
       "               0.7254217230821045,\n",
       "               0.7257497495313382,\n",
       "               0.7260267675808214,\n",
       "               0.7262610924505729,\n",
       "               0.7264763828730327,\n",
       "               0.7266416695631623,\n",
       "               0.7268477600410718,\n",
       "               0.7270188395963053,\n",
       "               0.7271406448172495,\n",
       "               0.7273146247064478,\n",
       "               0.7274230618207316,\n",
       "               0.7275350829749923,\n",
       "               0.7277085828512699,\n",
       "               0.7278647460095573,\n",
       "               0.7280700817183982,\n",
       "               0.7282453349606455,\n",
       "               0.7283296676097302,\n",
       "               0.7283816229497313,\n",
       "               0.7285330673533115,\n",
       "               0.728608235550928,\n",
       "               0.7287131698014406,\n",
       "               0.7287952386823292,\n",
       "               0.7289632591024611,\n",
       "               0.7289865142824815,\n",
       "               0.729152509062947,\n",
       "               0.7291976661678492,\n",
       "               0.7293077347483707,\n",
       "               0.7293709212753097,\n",
       "               0.7294816451523582,\n",
       "               0.7295095190447103,\n",
       "               0.7295646169199469,\n",
       "               0.729668804069693,\n",
       "               0.7297138960717986,\n",
       "               0.7297306458963749,\n",
       "               0.7297411922119599,\n",
       "               0.7297610327759368,\n",
       "               0.7298125746690424,\n",
       "               0.7299083578934138,\n",
       "               0.7299305563024141,\n",
       "               0.7299064517857929,\n",
       "               0.7299575226124683,\n",
       "               0.7300361786412286,\n",
       "               0.7301058424512586,\n",
       "               0.7300864028663735,\n",
       "               0.7300729493762642,\n",
       "               0.7300621250976251,\n",
       "               0.7301314615616488,\n",
       "               0.7301939517455137,\n",
       "               0.7302357796301454,\n",
       "               0.7302551196028665,\n",
       "               0.7302721329508481,\n",
       "               0.7303367264324934,\n",
       "               0.7303322154273066,\n",
       "               0.7303669861044239,\n",
       "               0.7304276431009762,\n",
       "               0.730441472143242,\n",
       "               0.7305014647354529,\n",
       "               0.7305425320135529,\n",
       "               0.7305299910206855,\n",
       "               0.7305636396420954,\n",
       "               0.7306025555941166,\n",
       "               0.7306866989077141,\n",
       "               0.7307022207709076,\n",
       "               0.7307237200737577,\n",
       "               0.7307444370861983,\n",
       "               0.730757728762127,\n",
       "               0.7307721996596239,\n",
       "               0.730819141228378,\n",
       "               0.7308163319480153,\n",
       "               0.730858637161372,\n",
       "               0.7309214380952317,\n",
       "               0.7309576105367278,\n",
       "               0.7309555591000192,\n",
       "               0.7310600204560647,\n",
       "               0.7311126010349349,\n",
       "               0.7311513349488459,\n",
       "               0.7311522578948287,\n",
       "               0.7311666033444201,\n",
       "               0.7311906530044856,\n",
       "               0.7311962805177886,\n",
       "               0.731225200952652,\n",
       "               0.7312382068438792,\n",
       "               0.73123896512264,\n",
       "               0.7312645605858162,\n",
       "               0.7313053968715478,\n",
       "               0.7313778428608242,\n",
       "               0.7313868689361362,\n",
       "               0.73138385517333,\n",
       "               0.731413633360337,\n",
       "               0.7314657775485622,\n",
       "               0.7315065447931082,\n",
       "               0.7315549412904313,\n",
       "               0.7315649494595401,\n",
       "               0.7315988124879812,\n",
       "               0.7315899894902739,\n",
       "               0.7316053310658013,\n",
       "               0.7316088930163023,\n",
       "               0.7316524659038304,\n",
       "               0.7316803889494352,\n",
       "               0.7317428393601239,\n",
       "               0.7317881071961193,\n",
       "               0.73183217237118,\n",
       "               0.731839387637722,\n",
       "               0.7318846433424223,\n",
       "               0.7319008385644857,\n",
       "               0.7319524317692208,\n",
       "               0.731979553640304,\n",
       "               0.7320429606482557,\n",
       "               0.7320925460399486,\n",
       "               0.7320940822468293,\n",
       "               0.7321237989653564,\n",
       "               0.7321967339568607,\n",
       "               0.7322375765498201,\n",
       "               0.73227287699585,\n",
       "               0.732316531669107,\n",
       "               0.7323389290055908,\n",
       "               0.7323521947714426,\n",
       "               0.7323432747996617,\n",
       "               0.7323756376188643,\n",
       "               0.7323843378313564,\n",
       "               0.7324196121939026,\n",
       "               0.7324324017227087,\n",
       "               0.7324634368541055,\n",
       "               0.7324732327221909,\n",
       "               0.7324902418595665,\n",
       "               0.732516722217991,\n",
       "               0.7325896435333911,\n",
       "               0.7325909934126014,\n",
       "               0.7326194495232942,\n",
       "               0.7326744588302899,\n",
       "               0.7327033660035506,\n",
       "               0.7327150136662837,\n",
       "               0.7327659097237005,\n",
       "               0.7327938509257461,\n",
       "               0.7328161846382183,\n",
       "               0.732891266938333,\n",
       "               0.7329057907484641,\n",
       "               0.7329768485012591,\n",
       "               0.7329955081782774,\n",
       "               0.733015476728998,\n",
       "               0.7330201975875255,\n",
       "               0.7330558523768733,\n",
       "               0.7330916834638729,\n",
       "               0.7331155884403341,\n",
       "               0.7331751216075034,\n",
       "               0.7331881796336743,\n",
       "               0.7332306878263299,\n",
       "               0.7332905629981964,\n",
       "               0.733327982244184,\n",
       "               0.7333758807697105,\n",
       "               0.7334026673154206,\n",
       "               0.7334388953821793,\n",
       "               0.7334599403741308,\n",
       "               0.7335348567127757,\n",
       "               0.7335535949527324,\n",
       "               0.7336031672037435,\n",
       "               0.7336592838000239,\n",
       "               0.7336673259703435,\n",
       "               0.7336756972493799,\n",
       "               0.7337129341476041,\n",
       "               0.7337272202510602,\n",
       "               0.7337723530769885,\n",
       "               0.733814612036159,\n",
       "               0.7338696117162571,\n",
       "               0.7338728971877405,\n",
       "               0.7339339606508536,\n",
       "               0.7339496890295465,\n",
       "               0.7339905988859468,\n",
       "               0.7340094543335107,\n",
       "               0.7340221545020598,\n",
       "               0.7340259239218547,\n",
       "               0.7340648054656577,\n",
       "               0.7340785261074022,\n",
       "               0.7341145964151471,\n",
       "               0.734156354342993,\n",
       "               0.7341766321203178,\n",
       "               0.7342001092131676,\n",
       "               0.734248724014998,\n",
       "               0.7342964473323252,\n",
       "               0.7343151088030231,\n",
       "               0.7343543401292285,\n",
       "               0.7343829818108824,\n",
       "               0.7344046166847137,\n",
       "               0.7344202799486272,\n",
       "               0.7344173809091015,\n",
       "               0.7344176026783834,\n",
       "               0.7345030413122398,\n",
       "               0.7345275887516922,\n",
       "               0.7345753768097539,\n",
       "               0.7345775620303957,\n",
       "               0.7345979396069527,\n",
       "               0.73463866577034,\n",
       "               0.7346467628146774,\n",
       "               0.7346956609291067,\n",
       "               0.7347051262923465,\n",
       "               0.7347296456067897,\n",
       "               0.7347586838237735,\n",
       "               0.7347822363695133,\n",
       "               0.7348175183032435,\n",
       "               0.7348353010645904,\n",
       "               0.7348752906290736,\n",
       "               0.7349347998340118,\n",
       "               0.7349925124032122,\n",
       "               0.7349971662956098,\n",
       "               0.7350006379745382,\n",
       "               0.7350163172344167,\n",
       "               0.735045988244112,\n",
       "               0.7350623422272431,\n",
       "               0.7350720953294989,\n",
       "               0.7350997380067293,\n",
       "               0.7351330412327505,\n",
       "               0.7351504659031176,\n",
       "               0.7351711554786848,\n",
       "               0.735223148204952,\n",
       "               0.7352217303425251,\n",
       "               0.735244361281955,\n",
       "               0.7352824340559412,\n",
       "               0.735318219815017,\n",
       "               0.7353623819597453,\n",
       "               0.7353821130454471,\n",
       "               0.7354262925386239,\n",
       "               0.7354501785669612,\n",
       "               0.7354772974342425,\n",
       "               0.7354939666685402,\n",
       "               0.7355157149021287,\n",
       "               0.7355290753548307,\n",
       "               0.7355220777053029,\n",
       "               0.7355387784587063,\n",
       "               0.7355468510944203,\n",
       "               0.7355850743749814,\n",
       "               0.7356146756593396,\n",
       "               0.7356251852949424,\n",
       "               0.7356498166489591,\n",
       "               0.7356607268875524,\n",
       "               0.735664781923098,\n",
       "               0.7356933049107413,\n",
       "               0.7356947577263938,\n",
       "               0.7357153496596017,\n",
       "               0.73574273195111,\n",
       "               0.7357508225953788,\n",
       "               0.7357662875649359,\n",
       "               0.7357854487229502,\n",
       "               0.7358039670282631,\n",
       "               0.7358413492260809,\n",
       "               0.7358926821214314,\n",
       "               0.7358877025419334,\n",
       "               0.73588130251857,\n",
       "               0.735911395961421,\n",
       "               0.7359184092811986,\n",
       "               0.7359566147670532,\n",
       "               0.7359585640273755,\n",
       "               0.7359883759309647,\n",
       "               0.7360047759663584,\n",
       "               0.7359984103296183,\n",
       "               0.7360199610099224,\n",
       "               0.7360671398992045,\n",
       "               0.7360817562968041,\n",
       "               0.7361079883734009,\n",
       "               0.7361005714536714,\n",
       "               0.7361157922393667,\n",
       "               0.7361413247357816,\n",
       "               0.7361385344402871,\n",
       "               0.7361522817240783,\n",
       "               0.7361427393927363,\n",
       "               0.7361643129625766,\n",
       "               0.7361739790668664,\n",
       "               0.7361925909784146,\n",
       "               0.7361900640244619,\n",
       "               0.7362016545189499,\n",
       "               0.7362272558260077,\n",
       "               0.7362330478185857,\n",
       "               0.7362374209969926,\n",
       "               0.736239870549692,\n",
       "               0.7362269186650698,\n",
       "               0.7362445231562915,\n",
       "               0.7362593219136019,\n",
       "               0.7362757747701152,\n",
       "               0.7362973040014615,\n",
       "               0.7362951349766365,\n",
       "               0.7363168047585282,\n",
       "               0.7363440877974508,\n",
       "               0.7363813653565222,\n",
       "               0.7364160349923323,\n",
       "               0.7364177603688422,\n",
       "               0.736450872036504,\n",
       "               0.7364676503019891,\n",
       "               0.7364831354061192,\n",
       "               0.7364790781657129,\n",
       "               0.7364913301329824,\n",
       "               0.736517611632969,\n",
       "               0.7365703967447019,\n",
       "               0.7365888114519669,\n",
       "               0.736591720950973,\n",
       "               0.7366233229095869,\n",
       "               0.7366497745577608,\n",
       "               0.7366567314089658,\n",
       "               0.7366535960146955,\n",
       "               0.7366638286831035,\n",
       "               0.7366724041276169,\n",
       "               0.7366843428564062,\n",
       "               0.7367080476493266,\n",
       "               0.7366991791782135,\n",
       "               0.7367050668245066,\n",
       "               0.7367126210197659,\n",
       "               0.7367278220762519,\n",
       "               0.7367355730296791,\n",
       "               0.7367591938628656,\n",
       "               0.7367998412888601,\n",
       "               0.736809383165177,\n",
       "               0.7368062821092695,\n",
       "               0.7368255321950109,\n",
       "               0.7368583647203372,\n",
       "               0.7368670120882603,\n",
       "               0.7368679376469283,\n",
       "               0.73688152795937,\n",
       "               0.7368972186319012,\n",
       "               0.7369083495986906,\n",
       "               0.7369172575861062,\n",
       "               0.7369692781669623,\n",
       "               0.7369696886876496,\n",
       "               0.7369903035560789,\n",
       "               0.7369969694905492,\n",
       "               0.7370117683969035,\n",
       "               0.737017431536943,\n",
       "               0.737045593622564,\n",
       "               0.737030970914258,\n",
       "               0.73703950709113,\n",
       "               0.7370404926782583,\n",
       "               0.7370564642214642,\n",
       "               0.7370852091068155,\n",
       "               0.7371003280205528,\n",
       "               0.737102592647621,\n",
       "               0.7371006793646558,\n",
       "               0.7371152949750743,\n",
       "               0.7370980631572823,\n",
       "               0.7371034553194885,\n",
       "               0.7371499033438318,\n",
       "               0.7371656822405649,\n",
       "               0.7371858078455344,\n",
       "               0.7372165828374067,\n",
       "               0.7372207842196046,\n",
       "               0.7372460863415312,\n",
       "               0.7372434627496934,\n",
       "               0.7372554964298953,\n",
       "               0.7372873507802148,\n",
       "               0.7373080403243161,\n",
       "               0.7372919977170669,\n",
       "               0.7373205678522834,\n",
       "               0.7373231573618639,\n",
       "               0.7373255240889747,\n",
       "               0.7373359831331011,\n",
       "               0.7373322928855187,\n",
       "               0.7373609665422827,\n",
       "               0.7374036718001745,\n",
       "               0.7374167288230934,\n",
       "               0.7374249115537485,\n",
       "               0.737453949829341,\n",
       "               0.7374909618667134,\n",
       "               0.7374946321075544,\n",
       "               0.7375163126928901,\n",
       "               0.7375342816862153,\n",
       "               0.7375438215766222,\n",
       "               0.737572528276853,\n",
       "               0.7375910315748497,\n",
       "               0.7375996631230073,\n",
       "               0.7376260613699192,\n",
       "               0.7376369960330156,\n",
       "               0.7376500843159405,\n",
       "               0.7376611672076155,\n",
       "               0.7376776879980417,\n",
       "               0.7376938588210485,\n",
       "               0.7376781463691562,\n",
       "               0.7376847531855807,\n",
       "               0.7377012417062183,\n",
       "               0.7377153293016001,\n",
       "               0.7377211975577694,\n",
       "               0.7377326207060424,\n",
       "               0.737752249273136,\n",
       "               0.7377720303650402,\n",
       "               0.7377920617172109,\n",
       "               0.7378123324330917,\n",
       "               0.7378284105198609,\n",
       "               0.7378534691065998,\n",
       "               0.7378521604879469,\n",
       "               0.7378911499007345,\n",
       "               0.7378923636022605,\n",
       "               0.7379010165332749,\n",
       "               0.7379278691417218,\n",
       "               0.7379688383442708,\n",
       "               0.7379777528133159,\n",
       "               0.7379990932176177,\n",
       "               0.7380163082717399,\n",
       "               0.7380316074381217,\n",
       "               0.7380510091325149,\n",
       "               0.738048513596201,\n",
       "               0.7380451308691789,\n",
       "               0.7380441750815173,\n",
       "               0.7380319262811357,\n",
       "               0.7380365310582062,\n",
       "               0.7380404892419767,\n",
       "               0.7380618204796774,\n",
       "               0.7380990926127448,\n",
       "               0.7381074263717916,\n",
       "               0.7381039871521243,\n",
       "               0.7381101139441787,\n",
       "               0.7381271154462363,\n",
       "               0.7381170032948878,\n",
       "               0.7381133036392884,\n",
       "               0.7381231163521023,\n",
       "               0.7381298261484733,\n",
       "               0.7381422839341826,\n",
       "               0.7381606009193551,\n",
       "               0.7381927558825965,\n",
       "               0.7382248011263111,\n",
       "               0.7382292516881469,\n",
       "               0.738240559337605,\n",
       "               0.7382311262980433,\n",
       "               0.7382257863188666,\n",
       "               0.7382431490680836,\n",
       "               0.7382783479957594,\n",
       "               0.7382887639462453,\n",
       "               0.7383180419852555,\n",
       "               0.7383359611975739,\n",
       "               0.7383526765605535,\n",
       "               0.7383496025447789,\n",
       "               0.7383678407329104,\n",
       "               0.7383776601569003,\n",
       "               0.7383881566713046,\n",
       "               0.7384100145649095,\n",
       "               0.7384280344493662,\n",
       "               0.7384318501549775,\n",
       "               0.7384216703548014,\n",
       "               0.7384450349206367,\n",
       "               0.7384507482154008,\n",
       "               0.7384397746705907,\n",
       "               0.7384644892068651,\n",
       "               0.7384898963526116,\n",
       "               0.7384618836281289,\n",
       "               0.7384701177019786,\n",
       "               0.7384797087589647,\n",
       "               0.7384969608708902,\n",
       "               0.7384962849671328,\n",
       "               0.738494262721739,\n",
       "               0.7385121362281848,\n",
       "               0.7385202032700903,\n",
       "               0.7385241792801354,\n",
       "               0.7385337622579977,\n",
       "               0.7385410410557717,\n",
       "               0.738571129787783,\n",
       "               0.7385635622948843,\n",
       "               0.7385669384904979,\n",
       "               0.7385713050217104,\n",
       "               0.7385725163431249,\n",
       "               0.738584857199705,\n",
       "               0.7386043114203851,\n",
       "               0.7386093872993919,\n",
       "               0.7385976061029834,\n",
       "               0.7386011046880079,\n",
       "               0.7386184287805942,\n",
       "               0.7386081971404834,\n",
       "               0.7386186149328748,\n",
       "               0.7386318468356107,\n",
       "               0.7386408947110518,\n",
       "               0.7386367350775578,\n",
       "               0.7386489423032504,\n",
       "               0.738656884925714,\n",
       "               0.7386744094248223,\n",
       "               0.7386985876449826,\n",
       "               0.7386839658932011,\n",
       "               0.738694540658887,\n",
       "               0.7386952934879321,\n",
       "               0.7387022755580819,\n",
       "               0.7387087341496966,\n",
       "               0.7387094393143798,\n",
       "               0.7387287359123251,\n",
       "               0.7387272103778268,\n",
       "               0.7387133994498153,\n",
       "               0.7387176709191722,\n",
       "               0.7387460635007304,\n",
       "               0.7387425118719045,\n",
       "               0.7387661470869872,\n",
       "               0.738805972094793,\n",
       "               0.7388132755582293,\n",
       "               0.7388303329035452,\n",
       "               0.7388491915686763,\n",
       "               0.7388553358384697,\n",
       "               0.7388557957286577,\n",
       "               0.7388622675766272,\n",
       "               0.7388716907435232,\n",
       "               0.7388752541756669,\n",
       "               0.7388852732588016,\n",
       "               0.7388940054234281,\n",
       "               0.7388839151483951,\n",
       "               0.738876142194739,\n",
       "               0.7388751446930336,\n",
       "               0.7388861319070477,\n",
       "               0.7388934153573781,\n",
       "               0.7389249845119744,\n",
       "               0.7389241404550652,\n",
       "               0.7389280773951602,\n",
       "               0.7389336994451121,\n",
       "               0.7389363769680568,\n",
       "               0.7389493390166495,\n",
       "               0.7389890714270864,\n",
       "               0.7389903497440287,\n",
       "               0.738994010569193,\n",
       "               0.7389988759678106,\n",
       "               0.7390182079518333,\n",
       "               0.7390450707861548,\n",
       "               0.7390649154339178,\n",
       "               0.7390948596179887,\n",
       "               0.7390984680827528,\n",
       "               0.739152561695793,\n",
       "               0.739167732496002,\n",
       "               0.7391797342315116,\n",
       "               0.7391877018538928,\n",
       "               0.7391884089152845,\n",
       "               0.7391903923962584,\n",
       "               0.7391880984888145,\n",
       "               0.739193824791647,\n",
       "               0.739215534786762,\n",
       "               0.7392226234202994,\n",
       "               0.7392421728472552,\n",
       "               0.7392449711910115,\n",
       "               0.7392619938864193,\n",
       "               0.7392840894219753,\n",
       "               0.7392964453455928,\n",
       "               0.7392972505612125,\n",
       "               0.739326300769628,\n",
       "               0.7393466641444235,\n",
       "               0.7393348263380364,\n",
       "               0.7393282317048715,\n",
       "               0.7393551216190701,\n",
       "               0.7393632554865313,\n",
       "               0.7393744080766537,\n",
       "               0.739389563125324,\n",
       "               0.7393854409102819,\n",
       "               0.7393842343181911,\n",
       "               0.7394029539195696,\n",
       "               0.7393958169700464,\n",
       "               0.7394101234825472,\n",
       "               0.7394088398333108,\n",
       "               0.7394102130848507,\n",
       "               0.7394137841611623,\n",
       "               0.7394455414063227,\n",
       "               0.7394430313569436,\n",
       "               0.7394585244269438,\n",
       "               0.7394614832211179,\n",
       "               0.7394532363258554,\n",
       "               0.739453812368174,\n",
       "               0.739487668800811,\n",
       "               0.7394944138532569,\n",
       "               0.7395022408518444,\n",
       "               0.7395110982322295,\n",
       "               0.7395137836902241,\n",
       "               0.7395186010786409,\n",
       "               0.7395334514310888,\n",
       "               0.7395208374956806,\n",
       "               0.7395273198050879,\n",
       "               0.7395290137849202,\n",
       "               0.7395451455071835,\n",
       "               0.7395214539148953,\n",
       "               0.7395485481043821,\n",
       "               0.7395498196868134,\n",
       "               0.7395453824110325,\n",
       "               0.7395588095686174,\n",
       "               0.7395611120813521,\n",
       "               0.7395599493138969,\n",
       "               0.739584063360543,\n",
       "               0.7395985670564544,\n",
       "               0.7395974214342224,\n",
       "               0.7396150110018127,\n",
       "               0.739616928259766,\n",
       "               0.7396454488135866,\n",
       "               0.7396530104380689,\n",
       "               0.7396512385384352,\n",
       "               0.7396625739574366,\n",
       "               0.7396574597811624,\n",
       "               0.7396389804893999,\n",
       "               0.739639326334121,\n",
       "               0.7396502676465635,\n",
       "               0.7396559930805473,\n",
       "               0.73967074268306,\n",
       "               0.7396592108588288,\n",
       "               0.7396599307384224,\n",
       "               0.7396639862998309,\n",
       "               0.7396728737665406,\n",
       "               0.7396879761218323,\n",
       "               0.7396763111401006,\n",
       "               0.7396742978955182,\n",
       "               0.7396673472439866,\n",
       "               0.7396517067777947,\n",
       "               0.7396513229882228,\n",
       "               0.7396613804271542,\n",
       "               0.739666406603083,\n",
       "               0.7396568102631089,\n",
       "               0.7396647409359034,\n",
       "               0.7396605885865772,\n",
       "               0.739671342976826,\n",
       "               0.7396946827565293,\n",
       "               0.7396736985657754,\n",
       "               0.7396806239415245,\n",
       "               0.7396899882422204,\n",
       "               0.7397000821753901,\n",
       "               0.739707424326659,\n",
       "               0.7397141908918642,\n",
       "               0.7397191297243964,\n",
       "               0.7397473948653033,\n",
       "               0.7397627372105996,\n",
       "               0.7397681567260138,\n",
       "               0.7397659502085433,\n",
       "               0.7397868437310955,\n",
       "               0.7397895851620374,\n",
       "               0.7397982543737334,\n",
       "               0.739784088063885,\n",
       "               0.7397827978352809,\n",
       "               0.7397889747339155,\n",
       "               0.7397918878512272,\n",
       "               0.7397875868486616,\n",
       "               0.7397894758099832,\n",
       "               0.7398106506632832,\n",
       "               0.7398181728012325,\n",
       "               0.7398331914428161,\n",
       "               0.7398436713411116,\n",
       "               0.7398410424911617,\n",
       "               0.7398468496716333,\n",
       "               0.7398610073132198,\n",
       "               0.7398741019518591,\n",
       "               0.7398850027063879,\n",
       "               0.7399012459441433,\n",
       "               0.7399166711215428,\n",
       "               0.739919739832739,\n",
       "               0.7399261749320263,\n",
       "               0.7399333105089893,\n",
       "               0.7399344651342676,\n",
       "               0.7399538886993124,\n",
       "               0.7399536049151625,\n",
       "               0.7399715174038574,\n",
       "               0.7400150582561229,\n",
       "               0.7400302748854448,\n",
       "               0.7400281436363604,\n",
       "               0.740024884002805,\n",
       "               0.7400366458696882,\n",
       "               0.7400376683123489,\n",
       "               0.7400536898979795,\n",
       "               0.7400656080787531,\n",
       "               0.7400747696727302,\n",
       "               0.7400737073386794,\n",
       "               0.7400459666198792,\n",
       "               0.7400453521394199,\n",
       "               0.7400732560967396,\n",
       "               0.7401130321117109,\n",
       "               0.7401281430214662,\n",
       "               0.7401286196916059,\n",
       "               0.7401136499179153,\n",
       "               0.7401100708383572,\n",
       "               0.7401009604321296,\n",
       "               0.7401102073999052,\n",
       "               0.7401055487662376,\n",
       "               0.7401102860454595,\n",
       "               0.7401166110463778,\n",
       "               0.7401059253961533,\n",
       "               0.7400941353125722,\n",
       "               0.740099264112603,\n",
       "               0.7400880019978255,\n",
       "               0.7400899572935875,\n",
       "               0.7400912799771553,\n",
       "               0.7401087100094725,\n",
       "               0.7401552676103658,\n",
       "               0.7401774111165255,\n",
       "               0.7401737868237728,\n",
       "               0.7401802382881225,\n",
       "               0.7401465700554519,\n",
       "               0.7401559827001463,\n",
       "               0.7401499717716331,\n",
       "               0.7401496253070865,\n",
       "               0.7401510359347384,\n",
       "               0.7401463384691975,\n",
       "               0.7401741455319417,\n",
       "               0.7401947687532964,\n",
       "               0.7402104638809368,\n",
       "               0.7402285155088453,\n",
       "               0.7402166633026313,\n",
       "               0.7402156846803549,\n",
       "               0.740264502002578,\n",
       "               0.7402619709016257,\n",
       "               0.7402674097699581,\n",
       "               0.7402800546444768,\n",
       "               0.7403147601863321,\n",
       "               0.7403119562674517,\n",
       "               0.7403138459935398,\n",
       "               0.7403234254255592,\n",
       "               0.7403652154537109,\n",
       "               0.7403710100960206,\n",
       "               0.7403709652322096,\n",
       "               0.7403600922962652,\n",
       "               0.7403555880912827,\n",
       "               0.7403680986972645,\n",
       "               0.740344033565328,\n",
       "               0.7403482302532554,\n",
       "               0.7403627274205787,\n",
       "               0.7403552329063873,\n",
       "               0.7403825495938816,\n",
       "               0.7403734495842732,\n",
       "               0.7403892326835176,\n",
       "               0.740380250638467,\n",
       "               0.7404059634307956,\n",
       "               0.7404308928423189,\n",
       "               0.7404275855469366,\n",
       "               0.7403933947721291,\n",
       "               0.7403994743481497,\n",
       "               0.7404010339527521,\n",
       "               0.7403987506540004,\n",
       "               0.7404037684545977,\n",
       "               0.7403971131746244,\n",
       "               0.7403846996158396,\n",
       "               0.7404076152489178,\n",
       "               0.7404451301490995,\n",
       "               0.7404598121059679,\n",
       "               0.7404905909665904,\n",
       "               0.7404899345667072,\n",
       "               0.7404892099168568,\n",
       "               0.7404879134147404,\n",
       "               0.7404211007197389,\n",
       "               0.7404139674693927,\n",
       "               0.740336948759625,\n",
       "               0.7403516480560438,\n",
       "               0.7403592401362131,\n",
       "               0.7404498192632076,\n",
       "               0.740459210621098,\n",
       "               0.7404462719015775,\n",
       "               0.7404671533165763,\n",
       "               0.7404709506332148,\n",
       "               0.7404629344475216,\n",
       "               0.740479084917515,\n",
       "               0.7404796782619167,\n",
       "               0.740512302665736,\n",
       "               0.7405392033277516,\n",
       "               0.7405500910780599,\n",
       "               0.740556671599532,\n",
       "               0.7405716981414712,\n",
       "               0.7405933188300188,\n",
       "               0.7405471524231348,\n",
       "               0.7405432680726434,\n",
       "               0.7405487885143044,\n",
       "               0.7405108887144999,\n",
       "               0.7405126108738825,\n",
       "               0.7405264850262708,\n",
       "               0.7405569634563501,\n",
       "               0.7405578220537329,\n",
       "               0.7405614602258306,\n",
       "               0.7405636998427843,\n",
       "               0.7405788871940779,\n",
       "               0.7405624840033191,\n",
       "               0.7405425343249756,\n",
       "               0.7403579117022692,\n",
       "               0.7403822899667031,\n",
       "               0.7403899232748801,\n",
       "               0.7399987568399594,\n",
       "               0.7398699030092953,\n",
       "               0.7397865603544032,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               0.738538372405945,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               0.7396632194719093,\n",
       "               0.7397528076933177,\n",
       "               0.7397991788025716,\n",
       "               0.7397782418622917,\n",
       "               0.739577828515003,\n",
       "               0.7397503198071932,\n",
       "               0.7397771057995151,\n",
       "               0.7394917480087587,\n",
       "               0.7385674992452599,\n",
       "               0.7385991060328956,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan],\n",
       "              'test-NegativeRMSLEMetric-std': [0.021236375528703905,\n",
       "               0.020834928669660066,\n",
       "               0.022404447378456652,\n",
       "               0.023244451937379867,\n",
       "               0.0208924440298464,\n",
       "               0.01926305251182794,\n",
       "               0.01883308188962952,\n",
       "               0.01665360193554612,\n",
       "               0.016077380077758455,\n",
       "               0.01512935848952673,\n",
       "               0.01491185243968059,\n",
       "               0.014652298069904656,\n",
       "               0.014645679601061485,\n",
       "               0.014745976178571589,\n",
       "               0.013939597344658248,\n",
       "               0.014670776440919216,\n",
       "               0.014692887498793722,\n",
       "               0.014849296586638526,\n",
       "               0.014903513510761341,\n",
       "               0.014862057520640317,\n",
       "               0.014733054791553742,\n",
       "               0.01475720926330012,\n",
       "               0.014926604995574164,\n",
       "               0.014767091496419355,\n",
       "               0.014972719206838223,\n",
       "               0.015016088899427356,\n",
       "               0.015256710455546732,\n",
       "               0.015276291432592133,\n",
       "               0.01495735616376555,\n",
       "               0.014932359509944745,\n",
       "               0.014849627258324042,\n",
       "               0.014807522889367927,\n",
       "               0.014676002833105493,\n",
       "               0.014668449899831668,\n",
       "               0.014648298158418699,\n",
       "               0.014606934528715977,\n",
       "               0.01448736001906396,\n",
       "               0.014298790112911058,\n",
       "               0.014159245395149742,\n",
       "               0.014061292437304493,\n",
       "               0.013986192939712995,\n",
       "               0.013853844667142682,\n",
       "               0.013746469129618166,\n",
       "               0.013587494445767154,\n",
       "               0.013445005941807003,\n",
       "               0.013412342725758392,\n",
       "               0.013372981776921438,\n",
       "               0.013281025372997187,\n",
       "               0.01315957973741078,\n",
       "               0.013118457873682396,\n",
       "               0.013006936493246034,\n",
       "               0.012958191744163369,\n",
       "               0.012893318623203224,\n",
       "               0.012740684028895976,\n",
       "               0.012677366762271658,\n",
       "               0.012640489487664374,\n",
       "               0.012544008227212996,\n",
       "               0.012467443676286358,\n",
       "               0.012380102616191983,\n",
       "               0.01231622222429999,\n",
       "               0.01226471950531272,\n",
       "               0.01220783792786836,\n",
       "               0.012132675630071622,\n",
       "               0.012044901096218625,\n",
       "               0.01196935374202646,\n",
       "               0.011874757863493755,\n",
       "               0.011831625333252951,\n",
       "               0.01178054836575348,\n",
       "               0.0116993161296811,\n",
       "               0.011596471548210375,\n",
       "               0.011520982454530038,\n",
       "               0.011426946001796992,\n",
       "               0.011305315591517505,\n",
       "               0.011213103281626292,\n",
       "               0.011167363763482952,\n",
       "               0.011089354233978657,\n",
       "               0.011052503081759951,\n",
       "               0.01103978676849588,\n",
       "               0.011023213727642155,\n",
       "               0.01087679876122278,\n",
       "               0.01080737742099375,\n",
       "               0.010762741620336363,\n",
       "               0.010790884961941308,\n",
       "               0.01071562621473492,\n",
       "               0.010649176804568602,\n",
       "               0.01064225215489645,\n",
       "               0.010654435197467602,\n",
       "               0.010561789693205944,\n",
       "               0.010494135462632342,\n",
       "               0.01043415683735176,\n",
       "               0.010469428163238382,\n",
       "               0.010428720746991936,\n",
       "               0.01044372402075211,\n",
       "               0.010450110555722878,\n",
       "               0.010393113382903683,\n",
       "               0.01036595495785222,\n",
       "               0.01034146887784766,\n",
       "               0.010276121240669414,\n",
       "               0.010217628574545773,\n",
       "               0.010195629783189783,\n",
       "               0.010156145640246897,\n",
       "               0.010076907036390676,\n",
       "               0.009987624609722693,\n",
       "               0.009943116634705831,\n",
       "               0.00986210497378893,\n",
       "               0.009845951224593114,\n",
       "               0.009809746158171194,\n",
       "               0.00979592121077623,\n",
       "               0.00980706630327967,\n",
       "               0.009793362535631003,\n",
       "               0.00965783378056349,\n",
       "               0.009614583356071875,\n",
       "               0.009442237542539835,\n",
       "               0.009274554988956437,\n",
       "               0.009129908383611527,\n",
       "               0.00896089559732893,\n",
       "               0.008804070197215139,\n",
       "               0.00873262145930952,\n",
       "               0.00858036528335867,\n",
       "               0.008567160437611044,\n",
       "               0.008564602335973757,\n",
       "               0.008643954907388547,\n",
       "               0.008685039436749059,\n",
       "               0.008720016161102698,\n",
       "               0.008715135013359401,\n",
       "               0.008756815001812567,\n",
       "               0.008739347477334845,\n",
       "               0.008823275055381951,\n",
       "               0.008793286386876445,\n",
       "               0.008833957461959632,\n",
       "               0.008791740398995444,\n",
       "               0.008757994406078884,\n",
       "               0.008739722569117817,\n",
       "               0.008776774724571109,\n",
       "               0.008804659150897938,\n",
       "               0.00881358728129759,\n",
       "               0.00884251944764663,\n",
       "               0.008842692465610012,\n",
       "               0.008819430651541596,\n",
       "               0.008922152843795392,\n",
       "               0.009027075703131403,\n",
       "               0.008950037786379466,\n",
       "               0.009011998475030468,\n",
       "               0.008966826158232397,\n",
       "               0.008912428664821998,\n",
       "               0.009048853260666376,\n",
       "               0.009075775538322312,\n",
       "               0.009093300201544407,\n",
       "               0.009070758248028433,\n",
       "               0.009085551270354998,\n",
       "               0.009139373234227838,\n",
       "               0.009184650648407589,\n",
       "               0.009206581190800712,\n",
       "               0.009218200642088456,\n",
       "               0.009293859054625367,\n",
       "               0.009340520015913688,\n",
       "               0.009315839082675933,\n",
       "               0.009361389053253042,\n",
       "               0.009420320961947944,\n",
       "               0.009398810303058716,\n",
       "               0.00938413519535477,\n",
       "               0.009354038505795899,\n",
       "               0.009309365876802084,\n",
       "               0.00935612971998642,\n",
       "               0.009365329348680613,\n",
       "               0.009388566131601078,\n",
       "               0.009402019391329133,\n",
       "               0.009328880215356834,\n",
       "               0.00931455015663314,\n",
       "               0.009309838902669499,\n",
       "               0.009312545866874201,\n",
       "               0.00928292529357191,\n",
       "               0.00925994466168125,\n",
       "               0.009231590877835462,\n",
       "               0.009265457063869753,\n",
       "               0.009345843698687318,\n",
       "               0.009350894555552236,\n",
       "               0.009370679159642318,\n",
       "               0.009335683711162487,\n",
       "               0.009318728726106262,\n",
       "               0.00933951189540004,\n",
       "               0.00937121114425867,\n",
       "               0.009426646093074986,\n",
       "               0.009418744512531633,\n",
       "               0.009422896284952255,\n",
       "               0.009408648678350463,\n",
       "               0.009389277293169147,\n",
       "               0.009414308267571588,\n",
       "               0.009414793744420424,\n",
       "               0.009387862082254395,\n",
       "               0.009424204513500432,\n",
       "               0.009388056335011268,\n",
       "               0.009371597815191408,\n",
       "               0.009396014861793846,\n",
       "               0.009456038972813644,\n",
       "               0.009434204765257033,\n",
       "               0.009462490572917018,\n",
       "               0.009472171805385973,\n",
       "               0.00951035951011131,\n",
       "               0.00953012573917967,\n",
       "               0.009545850113464597,\n",
       "               0.009555303109880522,\n",
       "               0.009531584293308982,\n",
       "               0.009619342358406122,\n",
       "               0.00965601355837528,\n",
       "               0.00966093691176875,\n",
       "               0.009665700312825157,\n",
       "               0.009624566058946755,\n",
       "               0.009639492051057186,\n",
       "               0.009616334244968817,\n",
       "               0.00963226149859992,\n",
       "               0.009594279871037498,\n",
       "               0.009635978124654635,\n",
       "               0.009622704205348085,\n",
       "               0.009593263281063616,\n",
       "               0.009590219392896242,\n",
       "               0.009555355594378822,\n",
       "               0.00954258696613888,\n",
       "               0.009564006689862556,\n",
       "               0.00953536089483418,\n",
       "               0.00953257899568422,\n",
       "               0.009541124076280716,\n",
       "               0.00951131622448398,\n",
       "               0.009505269098062994,\n",
       "               0.009529119981176809,\n",
       "               0.009538090231351104,\n",
       "               0.009507880041317784,\n",
       "               0.009507938951837477,\n",
       "               0.009503665015936994,\n",
       "               0.009494215595803964,\n",
       "               0.009494592292921707,\n",
       "               0.009528449856033263,\n",
       "               0.009555163852223604,\n",
       "               0.009625967083107924,\n",
       "               0.009597309822785945,\n",
       "               0.009630228490358841,\n",
       "               0.009644689379199838,\n",
       "               0.009692104966801453,\n",
       "               0.009701589710844997,\n",
       "               0.009665774099182194,\n",
       "               0.009700851061954603,\n",
       "               0.009751257711025819,\n",
       "               0.00977633000575074,\n",
       "               0.009785170165186333,\n",
       "               0.009735484310053942,\n",
       "               0.009719521342811607,\n",
       "               0.009712696330110064,\n",
       "               0.009745829170043767,\n",
       "               0.009786250476576112,\n",
       "               0.00975903310604308,\n",
       "               0.009744312931221263,\n",
       "               0.009726939349168075,\n",
       "               0.009766981545068535,\n",
       "               0.009775546753105093,\n",
       "               0.009761418959642635,\n",
       "               0.009789192502058489,\n",
       "               0.009758683789349574,\n",
       "               0.009753039850315772,\n",
       "               0.009773967585124176,\n",
       "               0.009785831771337423,\n",
       "               0.009781548945457902,\n",
       "               0.009785850936441909,\n",
       "               0.009763687402229635,\n",
       "               0.009729557743705335,\n",
       "               0.009739412080463453,\n",
       "               0.009750945373669882,\n",
       "               0.009747806520058562,\n",
       "               0.009767999900972466,\n",
       "               0.009751119749749872,\n",
       "               0.009729736430882178,\n",
       "               0.009714320237708985,\n",
       "               0.009689715137642151,\n",
       "               0.009686065338465655,\n",
       "               0.009690306532499533,\n",
       "               0.009706298706170607,\n",
       "               0.009696858435978646,\n",
       "               0.009696884122343584,\n",
       "               0.00973093506303753,\n",
       "               0.00972509952038533,\n",
       "               0.009720918596144322,\n",
       "               0.00975504196361876,\n",
       "               0.009807917512140801,\n",
       "               0.009810248850771995,\n",
       "               0.009817379461268901,\n",
       "               0.009775477500516357,\n",
       "               0.009744451187029981,\n",
       "               0.009706714030222436,\n",
       "               0.009710108376049626,\n",
       "               0.009688169733444727,\n",
       "               0.009701426195825858,\n",
       "               0.00968743973948068,\n",
       "               0.009676518916057364,\n",
       "               0.009684537196101035,\n",
       "               0.009705669849906704,\n",
       "               0.009711109015858532,\n",
       "               0.009687480501754397,\n",
       "               0.009709390588807777,\n",
       "               0.00968599356407986,\n",
       "               0.009633342269207621,\n",
       "               0.009686634625185217,\n",
       "               0.00971428058356154,\n",
       "               0.00971644759259702,\n",
       "               0.009725698907128565,\n",
       "               0.009707296840347247,\n",
       "               0.009714873175074447,\n",
       "               0.009734180224038384,\n",
       "               0.009712105954620015,\n",
       "               0.009689982448321071,\n",
       "               0.009660267696214836,\n",
       "               0.009676940067055545,\n",
       "               0.00970867466463393,\n",
       "               0.00968952561789684,\n",
       "               0.009685259232253463,\n",
       "               0.009685886858328894,\n",
       "               0.009693805110062511,\n",
       "               0.00968353220326136,\n",
       "               0.009683755185910135,\n",
       "               0.009662724460458783,\n",
       "               0.009651937655414358,\n",
       "               0.009665891476143577,\n",
       "               0.00973214362950652,\n",
       "               0.009728957318381248,\n",
       "               0.009787873895954192,\n",
       "               0.009795117165390783,\n",
       "               0.009805487091117567,\n",
       "               0.009823670167746467,\n",
       "               0.009821866901241384,\n",
       "               0.00986835895041255,\n",
       "               0.009852842193941901,\n",
       "               0.009839735467372195,\n",
       "               0.009873976596349783,\n",
       "               0.009870230802128777,\n",
       "               0.009857802036015259,\n",
       "               0.009824023989760143,\n",
       "               0.009768490383995205,\n",
       "               0.009797058059961988,\n",
       "               0.009862772045417236,\n",
       "               0.00985613357009977,\n",
       "               0.009854844498613244,\n",
       "               0.00981996310923454,\n",
       "               0.009853695901993248,\n",
       "               0.009866644218382431,\n",
       "               0.009849048417785894,\n",
       "               0.009870415017894062,\n",
       "               0.00984392160427182,\n",
       "               0.009854509066600625,\n",
       "               0.009860118875937898,\n",
       "               0.009850946751516074,\n",
       "               0.009869391337196283,\n",
       "               0.009831760394776661,\n",
       "               0.009856825353994979,\n",
       "               0.009887963128505718,\n",
       "               0.009913018027892294,\n",
       "               0.009920189919239108,\n",
       "               0.009907818954141955,\n",
       "               0.00991611159456594,\n",
       "               0.009946999007347078,\n",
       "               0.009918578376476878,\n",
       "               0.009909677367189066,\n",
       "               0.00991774340150158,\n",
       "               0.009919344675364684,\n",
       "               0.009919069288226012,\n",
       "               0.009911728363605044,\n",
       "               0.009942756677384774,\n",
       "               0.00991590017315744,\n",
       "               0.009909569323349373,\n",
       "               0.009897104401347093,\n",
       "               0.009904255301369153,\n",
       "               0.009917390207031949,\n",
       "               0.00993203100752446,\n",
       "               0.009932437379969397,\n",
       "               0.009963907594194932,\n",
       "               0.009964534176539292,\n",
       "               0.009949833641253349,\n",
       "               0.00994692385386609,\n",
       "               0.009947906032427836,\n",
       "               0.00994247994208908,\n",
       "               0.009994612299823635,\n",
       "               0.010006584000984267,\n",
       "               0.009992168249867253,\n",
       "               0.010002804731999129,\n",
       "               0.010017891616660632,\n",
       "               0.009995235025375093,\n",
       "               0.009999683670177078,\n",
       "               0.009972673067430455,\n",
       "               0.009997859291175614,\n",
       "               0.010021284748169202,\n",
       "               0.010005190256702729,\n",
       "               0.00998940424875975,\n",
       "               0.010009689268450821,\n",
       "               0.0100342433063048,\n",
       "               0.010054994344200466,\n",
       "               0.010030977268989074,\n",
       "               0.010041017741339218,\n",
       "               0.010022290070526806,\n",
       "               0.010012779633885817,\n",
       "               0.010020921914801934,\n",
       "               0.009983228268126827,\n",
       "               0.009965622042835916,\n",
       "               0.009945971821138534,\n",
       "               0.009947456300790372,\n",
       "               0.009972763618411545,\n",
       "               0.009969729525052207,\n",
       "               0.009957151823462893,\n",
       "               0.009954727887031329,\n",
       "               0.009948998401750305,\n",
       "               0.00992217680312347,\n",
       "               0.009900165014113232,\n",
       "               0.009913093839273242,\n",
       "               0.009899759438712448,\n",
       "               0.009894651331521979,\n",
       "               0.00990556124678295,\n",
       "               0.009917521958410362,\n",
       "               0.009916463451557493,\n",
       "               0.009901543392684738,\n",
       "               0.009913953191424676,\n",
       "               0.009891539983640842,\n",
       "               0.009883322315133704,\n",
       "               0.00988939759812715,\n",
       "               0.009871471291461216,\n",
       "               0.009851333372727216,\n",
       "               0.009835261062114159,\n",
       "               0.00982825872025079,\n",
       "               0.00980850001400094,\n",
       "               0.009784863514263789,\n",
       "               0.009788353342627303,\n",
       "               0.009792494680457038,\n",
       "               0.009775197897230805,\n",
       "               0.00978984524536134,\n",
       "               0.009787335717647463,\n",
       "               0.009780465434887012,\n",
       "               0.009767743966949544,\n",
       "               0.0097538934110026,\n",
       "               0.009749064932921036,\n",
       "               0.00972258474823643,\n",
       "               0.009702724735442302,\n",
       "               0.009721293886175587,\n",
       "               0.009736229378843749,\n",
       "               0.00973478197753624,\n",
       "               0.009731657923985981,\n",
       "               0.009745652003229367,\n",
       "               0.009735763368751576,\n",
       "               0.0097386932677878,\n",
       "               0.009736201534077848,\n",
       "               0.009738989245262506,\n",
       "               0.009717634567281513,\n",
       "               0.009720878716091112,\n",
       "               0.00972346634262232,\n",
       "               0.009721870260586811,\n",
       "               0.009696037815607184,\n",
       "               0.009660095460327728,\n",
       "               0.009639073086744554,\n",
       "               0.009638822525234077,\n",
       "               0.009651085797332903,\n",
       "               0.009687381781950978,\n",
       "               0.009681138047486174,\n",
       "               0.009678350729577637,\n",
       "               0.009656405162397479,\n",
       "               0.009645807565107939,\n",
       "               0.009621999135683878,\n",
       "               0.009621731108499934,\n",
       "               0.009629050407524617,\n",
       "               0.009604247745258686,\n",
       "               0.009594603217480164,\n",
       "               0.009581782133055954,\n",
       "               0.009573726614472129,\n",
       "               0.009554282733246112,\n",
       "               0.009550216458607378,\n",
       "               0.00955186945902791,\n",
       "               0.009557911620460643,\n",
       "               0.009584030564624244,\n",
       "               0.009590059888864553,\n",
       "               0.00957279873782737,\n",
       "               0.009575306289279873,\n",
       "               0.009562837103996349,\n",
       "               0.00959111076515323,\n",
       "               0.00959444786995635,\n",
       "               0.009586176804499878,\n",
       "               0.00958727972483174,\n",
       "               0.009592808895406738,\n",
       "               0.00960093677573632,\n",
       "               0.009621194618436088,\n",
       "               0.0096152878208279,\n",
       "               0.009615656442490543,\n",
       "               0.009619125789491052,\n",
       "               0.009625481622932983,\n",
       "               0.009639391731795042,\n",
       "               0.009623953027180545,\n",
       "               0.009627196480447053,\n",
       "               0.009632932009567878,\n",
       "               0.009652361682299644,\n",
       "               0.009662307661786702,\n",
       "               0.009632531843079769,\n",
       "               0.009607518962148152,\n",
       "               0.009585118036726775,\n",
       "               0.00958117009800648,\n",
       "               0.009572381529039761,\n",
       "               0.009556383943321416,\n",
       "               0.00956205031213688,\n",
       "               0.009551098773009423,\n",
       "               0.009546955981881208,\n",
       "               0.009546825535669906,\n",
       "               0.009543372391059842,\n",
       "               0.0095416937749667,\n",
       "               0.009514216325798578,\n",
       "               0.00950989554919716,\n",
       "               0.009508081395582901,\n",
       "               0.00952970842338513,\n",
       "               0.009536321512099242,\n",
       "               0.009564849858045319,\n",
       "               0.009563415311308323,\n",
       "               0.009569408444345787,\n",
       "               0.009564814925619683,\n",
       "               0.009575739079589479,\n",
       "               0.009574461003435049,\n",
       "               0.009562992827651247,\n",
       "               0.009571105973766026,\n",
       "               0.009573571726262633,\n",
       "               0.009581516468424024,\n",
       "               0.00956508735999609,\n",
       "               0.009563419713127576,\n",
       "               0.009579635915271228,\n",
       "               0.00962608468060364,\n",
       "               0.009620826351078023,\n",
       "               0.009615707688692389,\n",
       "               0.009628176552670394,\n",
       "               0.00963333589869697,\n",
       "               0.009626027508217276,\n",
       "               0.00960951024440694,\n",
       "               0.009611493877724133,\n",
       "               0.009593150770213147,\n",
       "               0.009620693681849412,\n",
       "               0.009610756092572599,\n",
       "               0.009614150013536665,\n",
       "               0.009615915601599271,\n",
       "               0.009635363592027591,\n",
       "               0.009629076643304556,\n",
       "               0.009597411720394428,\n",
       "               0.009564266004199058,\n",
       "               0.009560676962203316,\n",
       "               0.009549873627196795,\n",
       "               0.009575009550541387,\n",
       "               0.009566669998656841,\n",
       "               0.009532309741614712,\n",
       "               0.009547628734501703,\n",
       "               0.009578729285725483,\n",
       "               0.009569662972573207,\n",
       "               0.009568174602941315,\n",
       "               0.009568811207576982,\n",
       "               0.009576408257244028,\n",
       "               0.009562825589432524,\n",
       "               0.00957253574565797,\n",
       "               0.009562375037954807,\n",
       "               0.009553024153199054,\n",
       "               0.009561216542981994,\n",
       "               0.009563423277048325,\n",
       "               0.009564289505332557,\n",
       "               0.0095545279655173,\n",
       "               0.009551154211129526,\n",
       "               0.009555687465030775,\n",
       "               0.009547018420587491,\n",
       "               0.009560991525086509,\n",
       "               0.009550099357507067,\n",
       "               0.009559557620492699,\n",
       "               0.009562129400023066,\n",
       "               0.00958074373348085,\n",
       "               0.009587517616084725,\n",
       "               0.00955187895941371,\n",
       "               0.009563714647484196,\n",
       "               0.009569341771480286,\n",
       "               0.009582199254253534,\n",
       "               0.009568474043603697,\n",
       "               0.009598442559835569,\n",
       "               0.009594474889520047,\n",
       "               0.009597635306202166,\n",
       "               0.009614383478531182,\n",
       "               0.009605449747473863,\n",
       "               0.009611231223489482,\n",
       "               0.009600882305693144,\n",
       "               0.00960096212254771,\n",
       "               0.00961391049525804,\n",
       "               0.009600340163651754,\n",
       "               0.009580074970784837,\n",
       "               0.009576162618580972,\n",
       "               0.009566578726800662,\n",
       "               0.009549352267977578,\n",
       "               0.00955923006536122,\n",
       "               0.009543698395384591,\n",
       "               0.009534771603635684,\n",
       "               0.009539009946055462,\n",
       "               0.009550748630762168,\n",
       "               0.009556242295270512,\n",
       "               0.009564182467131095,\n",
       "               0.009564137472174272,\n",
       "               0.009565627415573178,\n",
       "               0.00956048535175001,\n",
       "               0.009533213632878516,\n",
       "               0.009532677841113713,\n",
       "               0.009531143842637202,\n",
       "               0.009540553990084882,\n",
       "               0.009544451139095246,\n",
       "               0.009537818288147211,\n",
       "               0.009557079462515798,\n",
       "               0.009564144499153002,\n",
       "               0.009544874819920826,\n",
       "               0.009555261348207356,\n",
       "               0.009568750192331414,\n",
       "               0.00955996757171681,\n",
       "               0.009572441128502453,\n",
       "               0.009584469419041258,\n",
       "               0.009583370802701226,\n",
       "               0.00957793177181874,\n",
       "               0.009594311604026542,\n",
       "               0.009612430597201276,\n",
       "               0.009608612799254061,\n",
       "               0.009602864289347623,\n",
       "               0.009598573581603076,\n",
       "               0.009602624634504377,\n",
       "               0.009615702537162358,\n",
       "               0.009590995319862747,\n",
       "               0.00960697698173231,\n",
       "               0.009597681120193727,\n",
       "               0.009585063849755633,\n",
       "               0.009588551129309635,\n",
       "               0.009589451212335532,\n",
       "               0.00958829297139144,\n",
       "               0.009577649159523879,\n",
       "               0.009586557035588273,\n",
       "               0.009592851774352618,\n",
       "               0.009580529660841358,\n",
       "               0.009603268009266103,\n",
       "               0.009594698209276638,\n",
       "               0.009587252431455225,\n",
       "               0.009595749356380679,\n",
       "               0.00960469757702528,\n",
       "               0.009607530963976648,\n",
       "               0.009594121808002813,\n",
       "               0.00961256605603881,\n",
       "               0.009614188265244001,\n",
       "               0.009611073936484464,\n",
       "               0.009618911999668682,\n",
       "               0.009607621633301972,\n",
       "               0.009592973840873372,\n",
       "               0.00958908208900048,\n",
       "               0.009617493872447614,\n",
       "               0.009621418906061567,\n",
       "               0.009638520362575531,\n",
       "               0.009640027421465554,\n",
       "               0.009649350970980054,\n",
       "               0.00965211695006522,\n",
       "               0.00964278911666033,\n",
       "               0.009657609622528368,\n",
       "               0.009657226894190235,\n",
       "               0.009663430710224404,\n",
       "               0.009686265430427131,\n",
       "               0.009692687845880668,\n",
       "               0.009708038856893732,\n",
       "               0.00973492353608078,\n",
       "               0.00975147502269551,\n",
       "               0.009734152691557144,\n",
       "               0.009734307216912846,\n",
       "               0.00975903184099231,\n",
       "               0.00979262651561342,\n",
       "               0.009797826434427151,\n",
       "               0.00977276408283761,\n",
       "               0.00975368301620737,\n",
       "               0.009754011596711654,\n",
       "               0.009767564612163502,\n",
       "               0.009776069137276758,\n",
       "               0.009759626566757093,\n",
       "               0.009746966433213591,\n",
       "               0.009740480056975186,\n",
       "               0.00973658687057581,\n",
       "               0.009737965817538156,\n",
       "               0.009746906144130735,\n",
       "               0.009747341892893509,\n",
       "               0.009748811880365655,\n",
       "               0.009754745505364305,\n",
       "               0.009758446831877269,\n",
       "               0.009757465994681252,\n",
       "               0.009766503880293521,\n",
       "               0.009778494335963887,\n",
       "               0.00979653328185814,\n",
       "               0.009782128486299684,\n",
       "               0.009759559266121834,\n",
       "               0.009782617372888575,\n",
       "               0.009775679596915778,\n",
       "               0.009783790288704354,\n",
       "               0.009803429327171941,\n",
       "               0.009811701651457283,\n",
       "               0.009823478455940308,\n",
       "               0.00982405218568685,\n",
       "               0.009808566131714333,\n",
       "               0.009803212407518237,\n",
       "               0.009789266603822086,\n",
       "               0.009788997206761045,\n",
       "               0.009789820552557293,\n",
       "               0.009778139069453311,\n",
       "               0.009765408439016457,\n",
       "               0.009765181497066181,\n",
       "               0.009744294771563302,\n",
       "               0.00973984628846655,\n",
       "               0.009733698400239854,\n",
       "               0.009729397877427487,\n",
       "               0.00972838980686134,\n",
       "               0.00973202601548839,\n",
       "               0.009718808443218272,\n",
       "               0.009710020996857567,\n",
       "               0.009712831155958914,\n",
       "               0.009704473218306158,\n",
       "               0.009716324933963082,\n",
       "               0.0097133804244167,\n",
       "               0.009698529945776191,\n",
       "               0.009713102739298901,\n",
       "               0.009715284416800377,\n",
       "               0.009718661422481211,\n",
       "               0.009725111366071152,\n",
       "               0.009711230750887234,\n",
       "               0.009706706169303091,\n",
       "               0.009709466576984863,\n",
       "               0.009713346540621066,\n",
       "               0.009716187923094631,\n",
       "               0.009702140002077456,\n",
       "               0.009696277097080428,\n",
       "               0.009707289490462326,\n",
       "               0.009689625111501073,\n",
       "               0.009690098189891968,\n",
       "               0.00969774018094138,\n",
       "               0.00969763034291579,\n",
       "               0.00968980086670333,\n",
       "               0.00967386485101163,\n",
       "               0.009700019216308841,\n",
       "               0.009700267537402444,\n",
       "               0.00969767779435157,\n",
       "               0.009731725595665593,\n",
       "               0.00972705145766479,\n",
       "               0.009715193747885893,\n",
       "               0.00970817177107025,\n",
       "               0.009719722150763898,\n",
       "               0.00970053540402779,\n",
       "               0.009686976562480219,\n",
       "               0.009686431845610279,\n",
       "               0.009685065706279063,\n",
       "               0.009683482801582351,\n",
       "               0.009689005838541095,\n",
       "               0.009688204086653352,\n",
       "               0.00970412562891257,\n",
       "               0.009707042665282448,\n",
       "               0.009692821576868807,\n",
       "               0.009670219339267868,\n",
       "               0.009657461897562178,\n",
       "               0.0096767115257504,\n",
       "               0.009687261087953921,\n",
       "               0.009686992306209112,\n",
       "               0.00969069325611733,\n",
       "               0.009674302395370643,\n",
       "               0.00966507460036602,\n",
       "               0.009655885102968121,\n",
       "               0.009661497082577299,\n",
       "               0.009659423143734644,\n",
       "               0.009674165094092009,\n",
       "               0.009683143468176456,\n",
       "               0.00965902014354397,\n",
       "               0.009622773751114899,\n",
       "               0.009631096479980795,\n",
       "               0.00966341223819857,\n",
       "               0.009657063081313755,\n",
       "               0.009656963684231724,\n",
       "               0.00964872746735088,\n",
       "               0.009649686146606632,\n",
       "               0.009655020568530778,\n",
       "               0.00966411607507424,\n",
       "               0.009704798224555819,\n",
       "               0.009707239947246033,\n",
       "               0.009717675077610455,\n",
       "               0.00970499806515964,\n",
       "               0.009714146400533375,\n",
       "               0.009722230831311312,\n",
       "               0.009728457761073759,\n",
       "               0.009724975184453415,\n",
       "               0.009732022074868207,\n",
       "               0.00972822846279571,\n",
       "               0.009687151468705338,\n",
       "               0.009683555336077895,\n",
       "               0.009693315199661682,\n",
       "               0.009725798106032905,\n",
       "               0.00972295731745379,\n",
       "               0.009731175602017651,\n",
       "               0.009708964820537219,\n",
       "               0.009709431461915101,\n",
       "               0.009682285772447797,\n",
       "               0.009684362956411688,\n",
       "               0.009683479851658567,\n",
       "               0.009694004470423056,\n",
       "               0.009695515339119965,\n",
       "               0.009697530650792097,\n",
       "               0.009670286696684304,\n",
       "               0.009669268874860148,\n",
       "               0.009657571985763055,\n",
       "               0.009671742071039104,\n",
       "               0.009660999889544027,\n",
       "               0.009670885005485094,\n",
       "               0.00974443558951974,\n",
       "               0.009760252432136511,\n",
       "               0.009727392617557569,\n",
       "               0.009741163105720842,\n",
       "               0.009705158462336303,\n",
       "               0.009678939112210082,\n",
       "               0.009645285529360939,\n",
       "               0.00967735758627944,\n",
       "               0.0096843524119234,\n",
       "               0.009693392329046412,\n",
       "               0.009708577974318913,\n",
       "               0.009738694658310454,\n",
       "               0.009745582499625935,\n",
       "               0.009756082023692934,\n",
       "               0.009719532642188106,\n",
       "               0.009726068473545535,\n",
       "               0.009774085743565624,\n",
       "               0.009762795944185235,\n",
       "               0.009778702856665683,\n",
       "               0.009780344683589412,\n",
       "               0.00978045201470665,\n",
       "               0.009773145812125612,\n",
       "               0.00975766411183856,\n",
       "               0.00974296475088399,\n",
       "               0.009798918240826161,\n",
       "               0.00978246571821572,\n",
       "               0.009764415409007805,\n",
       "               0.009756411371247853,\n",
       "               0.009739172901660726,\n",
       "               0.009738071885447853,\n",
       "               0.009716670379057131,\n",
       "               0.00970802198492819,\n",
       "               0.009719593914697988,\n",
       "               0.009731971556356498,\n",
       "               0.00975709177811317,\n",
       "               0.00974796528871028,\n",
       "               0.009729475996308447,\n",
       "               0.009701976568875599,\n",
       "               0.009707165727646443,\n",
       "               0.009705504735179723,\n",
       "               0.009692864447658637,\n",
       "               0.009605469537327884,\n",
       "               0.009601191439456055,\n",
       "               0.009612860317059246,\n",
       "               0.009595944352559775,\n",
       "               0.009577984753670643,\n",
       "               0.009590611167453262,\n",
       "               0.009543963869224176,\n",
       "               0.009539348040863963,\n",
       "               0.009580846352530504,\n",
       "               0.009593458802648175,\n",
       "               0.009641536579975346,\n",
       "               0.009646215362320262,\n",
       "               0.009641682285527487,\n",
       "               0.00964718220293939,\n",
       "               0.009492313639150518,\n",
       "               0.009480057164344356,\n",
       "               0.009360614914158003,\n",
       "               0.009348929025166814,\n",
       "               0.00936023583633475,\n",
       "               0.009510083310961093,\n",
       "               0.009514996150827236,\n",
       "               0.00950122573471273,\n",
       "               0.009495199151652286,\n",
       "               0.009510615715092872,\n",
       "               0.009510857818051072,\n",
       "               0.009538788037179642,\n",
       "               0.009516945867154316,\n",
       "               0.009546871735428654,\n",
       "               0.009577800417001098,\n",
       "               0.009627081970028385,\n",
       "               0.009636365051917176,\n",
       "               0.009626635842236423,\n",
       "               0.009648042714306467,\n",
       "               0.009567470444833984,\n",
       "               0.009550279913967551,\n",
       "               0.009564251570359178,\n",
       "               0.009475583103188127,\n",
       "               0.009473760029187329,\n",
       "               0.009458391007164613,\n",
       "               0.009491357885427367,\n",
       "               0.009479374272707279,\n",
       "               0.009461672832218769,\n",
       "               0.009440961563983106,\n",
       "               0.009445346748236135,\n",
       "               0.009456547930929253,\n",
       "               0.009421893028915485,\n",
       "               0.009136442893330128,\n",
       "               0.009163071815677937,\n",
       "               0.009171506661578491,\n",
       "               0.008732750665198631,\n",
       "               0.008617699642412572,\n",
       "               0.008563341825193216,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               0.008587511249598483,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               0.008491929194402508,\n",
       "               0.008562277743031492,\n",
       "               0.00858167429570899,\n",
       "               0.008562697751652937,\n",
       "               0.008456363648360683,\n",
       "               0.008541368393247603,\n",
       "               0.008547166119409255,\n",
       "               0.008419047038394073,\n",
       "               0.00864169580677276,\n",
       "               0.008613947635996688,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan],\n",
       "              'train-NegativeRMSLEMetric-mean': [-0.5104053985295666,\n",
       "               -0.23483230877331307,\n",
       "               -0.07566281397683194,\n",
       "               0.03690720058493983,\n",
       "               0.12376908955280788,\n",
       "               0.19068337936939656,\n",
       "               0.2471772999725576,\n",
       "               0.2959229147080491,\n",
       "               0.337890704774573,\n",
       "               0.37376742929555035,\n",
       "               0.4050907149560472,\n",
       "               0.4338036183794901,\n",
       "               0.458625187436867,\n",
       "               0.48177896768586576,\n",
       "               0.5029723373988083,\n",
       "               0.521671367625122,\n",
       "               0.5388160419196899,\n",
       "               0.5544634543329406,\n",
       "               0.5690654463573394,\n",
       "               0.5821698300514341,\n",
       "               0.5943389931012265,\n",
       "               0.6053212322267827,\n",
       "               0.6156436536268091,\n",
       "               0.6251084869057346,\n",
       "               0.6342746430965173,\n",
       "               0.6423826470194085,\n",
       "               0.6499563753483002,\n",
       "               0.6568996460461882,\n",
       "               0.6634568826312258,\n",
       "               0.6695440901494333,\n",
       "               0.675236819938332,\n",
       "               0.6805041619405128,\n",
       "               0.685242508020208,\n",
       "               0.6896944147685398,\n",
       "               0.6937522545763868,\n",
       "               0.6976717325340003,\n",
       "               0.7012862209463687,\n",
       "               0.7046546374218126,\n",
       "               0.7077366927489225,\n",
       "               0.7106332057092293,\n",
       "               0.713201476280842,\n",
       "               0.7156148999571164,\n",
       "               0.7179024881739134,\n",
       "               0.719942536412969,\n",
       "               0.7217163537246906,\n",
       "               0.7233926400713844,\n",
       "               0.725020294382288,\n",
       "               0.7264900353723904,\n",
       "               0.7279290880359496,\n",
       "               0.7292351977918488,\n",
       "               0.7303843966911364,\n",
       "               0.7314986883360126,\n",
       "               0.7324685469970865,\n",
       "               0.7333760766166761,\n",
       "               0.7342686888764052,\n",
       "               0.7349937195092948,\n",
       "               0.7355948894658689,\n",
       "               0.7362276914392696,\n",
       "               0.7368170828555222,\n",
       "               0.7373935967003769,\n",
       "               0.7378637651191282,\n",
       "               0.7382447969726331,\n",
       "               0.7386275170266774,\n",
       "               0.7389846227894405,\n",
       "               0.7394115153776322,\n",
       "               0.7396317268865887,\n",
       "               0.7398775496459201,\n",
       "               0.7400725270021481,\n",
       "               0.7402953473230091,\n",
       "               0.740482519219295,\n",
       "               0.7406292827256576,\n",
       "               0.7407606015684982,\n",
       "               0.740892614396202,\n",
       "               0.7410132558161997,\n",
       "               0.7410703637220766,\n",
       "               0.7411366148948151,\n",
       "               0.7412287277400738,\n",
       "               0.7412750087004544,\n",
       "               0.7413544179579625,\n",
       "               0.7413623546504924,\n",
       "               0.7412942301020415,\n",
       "               0.7413232604614766,\n",
       "               0.7413394921759369,\n",
       "               0.7413343662972609,\n",
       "               0.7413614192426385,\n",
       "               0.7413809253778764,\n",
       "               0.7412605529390228,\n",
       "               0.7412451557544266,\n",
       "               0.7412179463795188,\n",
       "               0.7411915870158065,\n",
       "               0.7411441306416522,\n",
       "               0.7410437712249195,\n",
       "               0.7410247658708409,\n",
       "               0.7409935124574334,\n",
       "               0.7409047224922178,\n",
       "               0.7408572351936124,\n",
       "               0.740839053046356,\n",
       "               0.7407484326791896,\n",
       "               0.7407538534943643,\n",
       "               0.7407306072197821,\n",
       "               0.7406354017572212,\n",
       "               0.740599840241745,\n",
       "               0.7405404003132067,\n",
       "               0.7404613794463033,\n",
       "               0.7404329651315498,\n",
       "               0.7404107585779485,\n",
       "               0.7403982765833729,\n",
       "               0.740391763794017,\n",
       "               0.7403769897097813,\n",
       "               0.7403283781765941,\n",
       "               0.7403151857967676,\n",
       "               0.7403237239865547,\n",
       "               0.7403904421129972,\n",
       "               0.7404964960796392,\n",
       "               0.7405473339449383,\n",
       "               0.7406913904993884,\n",
       "               0.740829458343461,\n",
       "               0.7409264726178328,\n",
       "               0.7410552739071747,\n",
       "               0.7411129274338043,\n",
       "               0.7413509539586799,\n",
       "               0.7415306234849999,\n",
       "               0.7417644595508701,\n",
       "               0.7419865222114778,\n",
       "               0.7421977356787752,\n",
       "               0.7424497543642745,\n",
       "               0.742640969808737,\n",
       "               0.7428270063848416,\n",
       "               0.7430015983051498,\n",
       "               0.7432974834936413,\n",
       "               0.7435663652004879,\n",
       "               0.7438231967240149,\n",
       "               0.7440198142634493,\n",
       "               0.7442254262267214,\n",
       "               0.7444585139884489,\n",
       "               0.7447030648975347,\n",
       "               0.7449298780753257,\n",
       "               0.7451010127773754,\n",
       "               0.7453141704466623,\n",
       "               0.7454699034554337,\n",
       "               0.7456429682364185,\n",
       "               0.7457478801822178,\n",
       "               0.7458557620892253,\n",
       "               0.7460441583355102,\n",
       "               0.7461234950898292,\n",
       "               0.746251036019015,\n",
       "               0.7464055007529382,\n",
       "               0.7465126339389933,\n",
       "               0.7466932467128823,\n",
       "               0.7468180442338804,\n",
       "               0.7469073702365422,\n",
       "               0.7469919258984347,\n",
       "               0.7471193412745964,\n",
       "               0.747194152468032,\n",
       "               0.7472949566175283,\n",
       "               0.7473733891838308,\n",
       "               0.7475601104477544,\n",
       "               0.7476311007710443,\n",
       "               0.7477901392949017,\n",
       "               0.7478746093091339,\n",
       "               0.747969419604318,\n",
       "               0.7480314292286354,\n",
       "               0.7481319844066451,\n",
       "               0.7482214412528099,\n",
       "               0.7483263574865793,\n",
       "               0.7484244540017202,\n",
       "               0.7484988898838716,\n",
       "               0.7485663923367298,\n",
       "               0.7486404056329098,\n",
       "               0.7487162457974379,\n",
       "               0.7487649121285975,\n",
       "               0.7488680419251937,\n",
       "               0.7489606988093424,\n",
       "               0.748993298961209,\n",
       "               0.749082823200353,\n",
       "               0.7492237995636267,\n",
       "               0.7492726969457182,\n",
       "               0.7493319276950098,\n",
       "               0.7493938697001447,\n",
       "               0.7494172394984954,\n",
       "               0.749555818179519,\n",
       "               0.7496581814079417,\n",
       "               0.749715541266116,\n",
       "               0.7497877309533206,\n",
       "               0.7498350969980108,\n",
       "               0.7499057436497043,\n",
       "               0.749947316157923,\n",
       "               0.7499956432956043,\n",
       "               0.7500619027067554,\n",
       "               0.7501307683858263,\n",
       "               0.7502404156626061,\n",
       "               0.7502948839800637,\n",
       "               0.7503314725453161,\n",
       "               0.750381200967239,\n",
       "               0.7504547859004322,\n",
       "               0.7505391985950294,\n",
       "               0.7505605288981757,\n",
       "               0.7505851452301778,\n",
       "               0.7506388951664766,\n",
       "               0.7506946484144543,\n",
       "               0.7507432252998931,\n",
       "               0.7508398358606618,\n",
       "               0.7509200570022017,\n",
       "               0.751006800704644,\n",
       "               0.751117782098992,\n",
       "               0.7511813769731692,\n",
       "               0.7512181319674666,\n",
       "               0.7513641205767192,\n",
       "               0.7514120704947908,\n",
       "               0.7515083417567542,\n",
       "               0.7515150704048379,\n",
       "               0.7516136543524913,\n",
       "               0.7516669777503211,\n",
       "               0.7516906039398625,\n",
       "               0.7517575367506931,\n",
       "               0.7517990052304623,\n",
       "               0.7518157121776969,\n",
       "               0.7518639628900666,\n",
       "               0.7519246727148705,\n",
       "               0.7520213344691188,\n",
       "               0.7520321139214328,\n",
       "               0.7520693853419209,\n",
       "               0.7521465318552657,\n",
       "               0.7522314296787879,\n",
       "               0.7523057987060262,\n",
       "               0.7523707851659618,\n",
       "               0.7524239563820982,\n",
       "               0.7525017733038529,\n",
       "               0.7525362983080047,\n",
       "               0.7525968462095944,\n",
       "               0.7526377298642599,\n",
       "               0.7527239918589533,\n",
       "               0.7527926024381865,\n",
       "               0.7528738898316829,\n",
       "               0.7529519592457744,\n",
       "               0.7530400904364558,\n",
       "               0.7530813380584777,\n",
       "               0.7531756805132231,\n",
       "               0.7532264536624605,\n",
       "               0.7533130200661537,\n",
       "               0.7533929573182998,\n",
       "               0.7534880317358562,\n",
       "               0.753582430304798,\n",
       "               0.7535894871071365,\n",
       "               0.753654322479815,\n",
       "               0.7537411485989282,\n",
       "               0.7538187613025464,\n",
       "               0.7538786418426444,\n",
       "               0.7539683692134439,\n",
       "               0.7540570232925842,\n",
       "               0.7541128580128917,\n",
       "               0.7542049491896444,\n",
       "               0.7542413890568401,\n",
       "               0.7542913797379229,\n",
       "               0.7543617779854181,\n",
       "               0.7544301510256787,\n",
       "               0.7545214281590721,\n",
       "               0.7545968000828941,\n",
       "               0.7546558628112028,\n",
       "               0.7546880819540606,\n",
       "               0.7548270999596836,\n",
       "               0.7548677603238391,\n",
       "               0.7549341689207107,\n",
       "               0.7550043658104781,\n",
       "               0.7550699806141521,\n",
       "               0.7551196197606103,\n",
       "               0.7551852726310477,\n",
       "               0.7552357968201324,\n",
       "               0.7552760711514754,\n",
       "               0.7553888928853401,\n",
       "               0.7554664464402254,\n",
       "               0.7556005386208737,\n",
       "               0.7556666495527117,\n",
       "               0.7557477255763333,\n",
       "               0.7558087982353148,\n",
       "               0.7558931520836517,\n",
       "               0.7559856785018771,\n",
       "               0.756038954943751,\n",
       "               0.7561315664081827,\n",
       "               0.7561920374534626,\n",
       "               0.7562887956616972,\n",
       "               0.7563935615650819,\n",
       "               0.7564695041676025,\n",
       "               0.7565512912497733,\n",
       "               0.7566301738209399,\n",
       "               0.7567691423121775,\n",
       "               0.7568646193095157,\n",
       "               0.7569724725175593,\n",
       "               0.7570997902482202,\n",
       "               0.7571972937061597,\n",
       "               0.7573050246391131,\n",
       "               0.757369834187001,\n",
       "               0.7573809293948542,\n",
       "               0.7574443458408802,\n",
       "               0.757509330952337,\n",
       "               0.7575780561962806,\n",
       "               0.7576434691746514,\n",
       "               0.7577192386665331,\n",
       "               0.7577941308197217,\n",
       "               0.7578590660990678,\n",
       "               0.7579265395919738,\n",
       "               0.7580146894986367,\n",
       "               0.7580777660405448,\n",
       "               0.758138470053481,\n",
       "               0.7581804200617388,\n",
       "               0.7582500795497664,\n",
       "               0.7583062197286564,\n",
       "               0.7584096474020529,\n",
       "               0.7584899552030298,\n",
       "               0.7585327865715875,\n",
       "               0.7585986143583492,\n",
       "               0.7586772934700636,\n",
       "               0.7588027629449859,\n",
       "               0.758869394345796,\n",
       "               0.7589438463845845,\n",
       "               0.7590030830635558,\n",
       "               0.7590664399454781,\n",
       "               0.759137395677098,\n",
       "               0.7591815298114022,\n",
       "               0.7592116230361857,\n",
       "               0.7593251584957459,\n",
       "               0.7593855607483194,\n",
       "               0.7594503411122113,\n",
       "               0.759510314554549,\n",
       "               0.7595647660302005,\n",
       "               0.7596387889877741,\n",
       "               0.7596679421709267,\n",
       "               0.7597789673292505,\n",
       "               0.7598309577255342,\n",
       "               0.7598978364597113,\n",
       "               0.7599475705635343,\n",
       "               0.7600085200908555,\n",
       "               0.7600723258365539,\n",
       "               0.7601199277877166,\n",
       "               0.7602313312047968,\n",
       "               0.760329327316472,\n",
       "               0.7604248405941119,\n",
       "               0.760482575461192,\n",
       "               0.7605467639696089,\n",
       "               0.7606274793754251,\n",
       "               0.760696774057106,\n",
       "               0.7607850427063347,\n",
       "               0.7608432320453546,\n",
       "               0.7609141413565352,\n",
       "               0.7609541062442442,\n",
       "               0.7610335959468868,\n",
       "               0.7610982344870809,\n",
       "               0.7611924533314361,\n",
       "               0.7612663992785533,\n",
       "               0.761327543366325,\n",
       "               0.7613838234468853,\n",
       "               0.7615020448236438,\n",
       "               0.7615646291238917,\n",
       "               0.7616193002896943,\n",
       "               0.7616817613391718,\n",
       "               0.7617457866867341,\n",
       "               0.7618229079774264,\n",
       "               0.7618621413573486,\n",
       "               0.7619389933481037,\n",
       "               0.7620079754308898,\n",
       "               0.7620537567572775,\n",
       "               0.7621127551829082,\n",
       "               0.7621554522197931,\n",
       "               0.7622367135743121,\n",
       "               0.7623081777998758,\n",
       "               0.7623535253528544,\n",
       "               0.7624235579384232,\n",
       "               0.7624715237373781,\n",
       "               0.7625179267427397,\n",
       "               0.7625687099874339,\n",
       "               0.7626059905816358,\n",
       "               0.7626848260559288,\n",
       "               0.7627359192809795,\n",
       "               0.762778206977136,\n",
       "               0.762830223617457,\n",
       "               0.7629148613855634,\n",
       "               0.7629536847295878,\n",
       "               0.7630228523502841,\n",
       "               0.7631047838583684,\n",
       "               0.7631357784046877,\n",
       "               0.7632025027555858,\n",
       "               0.7632809912596336,\n",
       "               0.7633439510331009,\n",
       "               0.7634233706060994,\n",
       "               0.7634842251533207,\n",
       "               0.7635368613325886,\n",
       "               0.7636275749510004,\n",
       "               0.7637087260828203,\n",
       "               0.7637615285924646,\n",
       "               0.7638358767359493,\n",
       "               0.7638857727045114,\n",
       "               0.763938682523735,\n",
       "               0.7639916965344268,\n",
       "               0.7640434959039801,\n",
       "               0.7640997220761971,\n",
       "               0.7641489798999646,\n",
       "               0.764210627845074,\n",
       "               0.7642687464485155,\n",
       "               0.7643603481933547,\n",
       "               0.7644256972980521,\n",
       "               0.7644807452665586,\n",
       "               0.764537623295465,\n",
       "               0.7645859017945627,\n",
       "               0.7646447388257307,\n",
       "               0.7646963115878324,\n",
       "               0.7647615927382291,\n",
       "               0.7648231108987622,\n",
       "               0.7648838489995312,\n",
       "               0.7649252198912636,\n",
       "               0.7649506664060499,\n",
       "               0.7649833656888793,\n",
       "               0.7650525416235017,\n",
       "               0.7651071369712414,\n",
       "               0.7651462947081382,\n",
       "               0.7652072853926911,\n",
       "               0.7652862125680755,\n",
       "               0.7653523118045392,\n",
       "               0.7653962828690049,\n",
       "               0.7654731701058259,\n",
       "               0.7655321500456995,\n",
       "               0.7655893889949451,\n",
       "               0.7656609508370174,\n",
       "               0.7656996332481693,\n",
       "               0.7657793685548857,\n",
       "               0.7658575682125134,\n",
       "               0.7659186100240782,\n",
       "               0.7659774783087625,\n",
       "               0.7660495866366321,\n",
       "               0.7661123385227298,\n",
       "               0.7661674315966667,\n",
       "               0.7662136525137144,\n",
       "               0.7662837255063137,\n",
       "               0.7663406071403038,\n",
       "               0.766380643051599,\n",
       "               0.766453556548193,\n",
       "               0.7665314814128112,\n",
       "               0.7665783725975496,\n",
       "               0.766620649257915,\n",
       "               0.7666961464647998,\n",
       "               0.7667489837778827,\n",
       "               0.7668278063674393,\n",
       "               0.7668829372212502,\n",
       "               0.7669257550339604,\n",
       "               0.7669672238371229,\n",
       "               0.7670280909787369,\n",
       "               0.7670733698557141,\n",
       "               0.7671462013292113,\n",
       "               0.7671904234401205,\n",
       "               0.7672719296554711,\n",
       "               0.7673292425430637,\n",
       "               0.7674070179582184,\n",
       "               0.7674713882071706,\n",
       "               0.7675586407247313,\n",
       "               0.7676096830731127,\n",
       "               0.7676744097239465,\n",
       "               0.7677389601423893,\n",
       "               0.7677990110216613,\n",
       "               0.7678660598795549,\n",
       "               0.7679461258923433,\n",
       "               0.7680219260377796,\n",
       "               0.7680913446027351,\n",
       "               0.7681436801220952,\n",
       "               0.7681990826711949,\n",
       "               0.768246021625198,\n",
       "               0.768291728378472,\n",
       "               0.7683115414620632,\n",
       "               0.7683489063365272,\n",
       "               0.7684309264601217,\n",
       "               0.7684641395603442,\n",
       "               0.7684947188134038,\n",
       "               0.7685810480508148,\n",
       "               0.7686460960485966,\n",
       "               0.7686983689676812,\n",
       "               0.7687972484828014,\n",
       "               0.7688351785714856,\n",
       "               0.7688889514286311,\n",
       "               0.7689561009017751,\n",
       "               0.7690069097238463,\n",
       "               0.7690879559687543,\n",
       "               0.7691681932741257,\n",
       "               0.7692158851918982,\n",
       "               0.7692894691127355,\n",
       "               0.7693473205788319,\n",
       "               0.7694113901660066,\n",
       "               0.769460962950075,\n",
       "               0.7695099722390086,\n",
       "               0.7695814418447549,\n",
       "               0.7696594302121331,\n",
       "               0.7697337983702962,\n",
       "               0.7698288385917901,\n",
       "               0.7699142482823629,\n",
       "               0.7700026701257939,\n",
       "               0.7700411288278017,\n",
       "               0.7701010564452989,\n",
       "               0.7701607147727524,\n",
       "               0.7702067601963153,\n",
       "               0.7702732096130939,\n",
       "               0.7703494829259102,\n",
       "               0.7703996654840911,\n",
       "               0.7704682879899845,\n",
       "               0.7705658590091474,\n",
       "               0.7706051162001925,\n",
       "               0.7706640784873997,\n",
       "               0.7706996958942229,\n",
       "               0.77074464214621,\n",
       "               0.7708233773022398,\n",
       "               0.7708719950439125,\n",
       "               0.7709378641355741,\n",
       "               0.7709986583910912,\n",
       "               0.7710481489520524,\n",
       "               0.7711088290282085,\n",
       "               0.7711859653054062,\n",
       "               0.7712503351755514,\n",
       "               0.7713322156413145,\n",
       "               0.7713837647077769,\n",
       "               0.77144027554932,\n",
       "               0.7715115866129826,\n",
       "               0.7715616893098096,\n",
       "               0.771653642503584,\n",
       "               0.7717007085257439,\n",
       "               0.7717619590893579,\n",
       "               0.7718409330724276,\n",
       "               0.7719107414368784,\n",
       "               0.7719536039941688,\n",
       "               0.7720104481324838,\n",
       "               0.7721050037738337,\n",
       "               0.7721757014198645,\n",
       "               0.772252114898339,\n",
       "               0.7723128605814114,\n",
       "               0.7723636498413959,\n",
       "               0.7724289974551894,\n",
       "               0.7724773883277014,\n",
       "               0.7725470142882946,\n",
       "               0.7725850094853086,\n",
       "               0.7726465058615276,\n",
       "               0.7727261904363383,\n",
       "               0.7727778374176723,\n",
       "               0.7728518251947281,\n",
       "               0.7728981578752973,\n",
       "               0.7729436991021611,\n",
       "               0.7730251125314778,\n",
       "               0.7731114020999608,\n",
       "               0.7731668581108455,\n",
       "               0.7732243181490877,\n",
       "               0.7733073661109449,\n",
       "               0.7733817758887762,\n",
       "               0.7734351434722275,\n",
       "               0.773521429766985,\n",
       "               0.7735860548980005,\n",
       "               0.7736618402222966,\n",
       "               0.7737108814848064,\n",
       "               0.7737595595065867,\n",
       "               0.7738158931922727,\n",
       "               0.7739015791342212,\n",
       "               0.7739787274654302,\n",
       "               0.774053802650458,\n",
       "               0.7741155525932034,\n",
       "               0.774177929867234,\n",
       "               0.7742304334569994,\n",
       "               0.7742675652916294,\n",
       "               0.7743102499101344,\n",
       "               0.7744023453179603,\n",
       "               0.7744632065515712,\n",
       "               0.7745253642853024,\n",
       "               0.7745814726327205,\n",
       "               0.7746347741596854,\n",
       "               0.7746921266445173,\n",
       "               0.7747800055327596,\n",
       "               0.7748396175921041,\n",
       "               0.7748898639195533,\n",
       "               0.7749573626450793,\n",
       "               0.7750198743706527,\n",
       "               0.7750717683948063,\n",
       "               0.7751311919625824,\n",
       "               0.7751778215783214,\n",
       "               0.77525048363649,\n",
       "               0.7753258475689043,\n",
       "               0.7753743422438959,\n",
       "               0.775424714811568,\n",
       "               0.7755042341794333,\n",
       "               0.7755489712346236,\n",
       "               0.7756055619604271,\n",
       "               0.7756517285847501,\n",
       "               0.7756906591016669,\n",
       "               0.7757463683503076,\n",
       "               0.7757743236413074,\n",
       "               0.7758163091208135,\n",
       "               0.775870188821485,\n",
       "               0.7759485651094881,\n",
       "               0.7759951870483429,\n",
       "               0.776048896531961,\n",
       "               0.7760972592311057,\n",
       "               0.7761578167251095,\n",
       "               0.7762001452806881,\n",
       "               0.7762725316002428,\n",
       "               0.7763337181493875,\n",
       "               0.7763780426262546,\n",
       "               0.7764488804019201,\n",
       "               0.7764977653543081,\n",
       "               0.7765425503921134,\n",
       "               0.7765862130793981,\n",
       "               0.7766445686664498,\n",
       "               0.7766926774616189,\n",
       "               0.7767329753966906,\n",
       "               0.7767936725526776,\n",
       "               0.7768520367181326,\n",
       "               0.7769181830528151,\n",
       "               0.7769747538125529,\n",
       "               0.7770266657662036,\n",
       "               0.7770721784855789,\n",
       "               0.7771180323234137,\n",
       "               0.7771661937297315,\n",
       "               0.7772357210677021,\n",
       "               0.7772688969459141,\n",
       "               0.7773179177332997,\n",
       "               0.777369248938727,\n",
       "               0.7774120297680559,\n",
       "               0.7774636114438433,\n",
       "               0.777550679559289,\n",
       "               0.7776096432275423,\n",
       "               0.7776521602824145,\n",
       "               0.7776963059582611,\n",
       "               0.777731677231587,\n",
       "               0.7777832318548323,\n",
       "               0.7778530730953301,\n",
       "               0.7779232171286562,\n",
       "               0.7779853040064015,\n",
       "               0.7780238319705319,\n",
       "               0.7780958021430271,\n",
       "               0.7781478570056942,\n",
       "               0.7781976150561607,\n",
       "               0.7782580073555591,\n",
       "               0.778303529872652,\n",
       "               0.778336349595636,\n",
       "               0.7784102205809933,\n",
       "               0.7784720957510632,\n",
       "               0.778535401780866,\n",
       "               0.7786278897035299,\n",
       "               0.7786799489162937,\n",
       "               0.7787416599383356,\n",
       "               0.7787996938261192,\n",
       "               0.7788626790815874,\n",
       "               0.7789386678764684,\n",
       "               0.7789623947006965,\n",
       "               0.7790294797193835,\n",
       "               0.7791058491479093,\n",
       "               0.7791817791786876,\n",
       "               0.7792388337524011,\n",
       "               0.779303012222085,\n",
       "               0.779370289667953,\n",
       "               0.7794137781953481,\n",
       "               0.7794525310520204,\n",
       "               0.7795019861725352,\n",
       "               0.7795423836813116,\n",
       "               0.7796133416462613,\n",
       "               0.7796673751117776,\n",
       "               0.7797251477785944,\n",
       "               0.779791266677184,\n",
       "               0.7798609042367184,\n",
       "               0.7799515218532589,\n",
       "               0.7799973423986264,\n",
       "               0.780062554782947,\n",
       "               0.7801369774840717,\n",
       "               0.7802201524719885,\n",
       "               0.7802780791311223,\n",
       "               0.7803529155584336,\n",
       "               0.7804300259559531,\n",
       "               0.7805184847250225,\n",
       "               0.780571933367995,\n",
       "               0.7806168737378509,\n",
       "               0.7806546278527229,\n",
       "               0.780713660918885,\n",
       "               0.7807738874550391,\n",
       "               0.780820491839082,\n",
       "               0.7808725876779696,\n",
       "               0.780922011043063,\n",
       "               0.78096268995562,\n",
       "               0.7810322277669648,\n",
       "               0.7811058522104735,\n",
       "               0.7811663192507939,\n",
       "               0.7812244662497609,\n",
       "               0.7812727129620246,\n",
       "               0.7813272143852311,\n",
       "               0.7813829450598424,\n",
       "               0.7814394701046006,\n",
       "               0.7815011382124989,\n",
       "               0.7815767360506191,\n",
       "               0.7816304142729638,\n",
       "               0.7816911537442888,\n",
       "               0.7817271549249616,\n",
       "               0.7817849150464427,\n",
       "               0.7818358541259038,\n",
       "               0.7818951331049455,\n",
       "               0.7819450870851197,\n",
       "               0.782010494044141,\n",
       "               0.7820663118150539,\n",
       "               0.7821310043049149,\n",
       "               0.7822054041157513,\n",
       "               0.782274787452976,\n",
       "               0.7823253106894781,\n",
       "               0.7823998623394748,\n",
       "               0.7824753872356183,\n",
       "               0.7825446055925209,\n",
       "               0.782611894657112,\n",
       "               0.7826480136446474,\n",
       "               0.7827069158324236,\n",
       "               0.7827805881207984,\n",
       "               0.7828389014456009,\n",
       "               0.7828797229578397,\n",
       "               0.7829104550258433,\n",
       "               0.7829836842392293,\n",
       "               0.7830278791231382,\n",
       "               0.7830854148231554,\n",
       "               0.7831406212530434,\n",
       "               0.7832138851886825,\n",
       "               0.7832790134511491,\n",
       "               0.7833405470774462,\n",
       "               0.7834007285080864,\n",
       "               0.7834573704707213,\n",
       "               0.7835222033309338,\n",
       "               0.7835937452200014,\n",
       "               0.7836406385097296,\n",
       "               0.7837016268077422,\n",
       "               0.7837455840647417,\n",
       "               0.783791613611973,\n",
       "               0.7838472372682265,\n",
       "               0.7838938756422984,\n",
       "               0.7839365227434719,\n",
       "               0.7839985446764883,\n",
       "               0.7840639621481342,\n",
       "               0.7841102418432951,\n",
       "               0.78415917343711,\n",
       "               0.7841951611270729,\n",
       "               0.7842441265927296,\n",
       "               0.7843089521276273,\n",
       "               0.7843801837681819,\n",
       "               0.7844258785952973,\n",
       "               0.784498066192703,\n",
       "               0.7845610345072708,\n",
       "               0.7846189682691518,\n",
       "               0.7846648683478087,\n",
       "               0.784737436349934,\n",
       "               0.7847932315328185,\n",
       "               0.784844711485735,\n",
       "               0.7849057459814511,\n",
       "               0.7849741623213977,\n",
       "               0.7850295389789678,\n",
       "               0.7850876908753184,\n",
       "               0.7851402956754523,\n",
       "               0.7852089263326156,\n",
       "               0.7852826997493858,\n",
       "               0.7853326613386182,\n",
       "               0.7853851510418008,\n",
       "               0.7854405251667017,\n",
       "               0.7855010471558777,\n",
       "               0.785576832598871,\n",
       "               0.7856315472712778,\n",
       "               0.7856987746910283,\n",
       "               0.7857515020129348,\n",
       "               0.7857966131787336,\n",
       "               0.7858442359739418,\n",
       "               0.7858846253382017,\n",
       "               0.7859509855043475,\n",
       "               0.786005756782786,\n",
       "               0.7860844810984291,\n",
       "               0.7861585612660933,\n",
       "               0.7862192572938167,\n",
       "               0.7862771842350303,\n",
       "               0.7863114207513208,\n",
       "               0.7863586216678093,\n",
       "               0.7864051875695897,\n",
       "               0.7864666891984954,\n",
       "               0.7865326440342513,\n",
       "               0.7865835044372027,\n",
       "               0.7866367263147229,\n",
       "               0.78669869865962,\n",
       "               0.7867650708026142,\n",
       "               0.7868186835222165,\n",
       "               0.7868698471689104,\n",
       "               0.7869232176476384,\n",
       "               0.7869716428413878,\n",
       "               0.7870331943448362,\n",
       "               0.7870916314518566,\n",
       "               0.7871293120551514,\n",
       "               0.7871939970043785,\n",
       "               0.7872640403999294,\n",
       "               0.7873224257114538,\n",
       "               0.7873897466846961,\n",
       "               0.7874454838553584,\n",
       "               0.7874817389040125,\n",
       "               0.7875383204025579,\n",
       "               0.7875669874234601,\n",
       "               0.7876129081796579,\n",
       "               0.7876817115515082,\n",
       "               0.787742405398834,\n",
       "               0.7877848128401446,\n",
       "               0.7878426829110377,\n",
       "               0.7879001282796481,\n",
       "               0.7879415813281134,\n",
       "               0.7879898951513458,\n",
       "               0.7880469673940282,\n",
       "               0.7880938324365089,\n",
       "               0.788165796587592,\n",
       "               0.7882250907015863,\n",
       "               0.7882698906495077,\n",
       "               0.7883246258112011,\n",
       "               0.7883770119266557,\n",
       "               0.788451017074498,\n",
       "               0.7885131432601231,\n",
       "               0.7885554569143185,\n",
       "               0.7886039068444005,\n",
       "               0.7886468135438758,\n",
       "               0.7887037007491401,\n",
       "               0.7887566864600948,\n",
       "               0.7887973930375152,\n",
       "               0.7888329647707994,\n",
       "               0.7888957330786635,\n",
       "               0.7889448944612198,\n",
       "               0.7890075441873161,\n",
       "               0.789050295565938,\n",
       "               0.7891043599965503,\n",
       "               0.7891513958874541,\n",
       "               0.789203325314828,\n",
       "               0.7892503705031404,\n",
       "               0.7892845122813592,\n",
       "               0.7893518121872394,\n",
       "               0.7893923109228214,\n",
       "               0.7894585570739017,\n",
       "               0.7895073385666502,\n",
       "               0.7895574599531144,\n",
       "               0.789591041879483,\n",
       "               0.7896295908456261,\n",
       "               0.7896798496456742,\n",
       "               0.7897363105656503,\n",
       "               0.7897992437468082,\n",
       "               0.7898493841514482,\n",
       "               0.7899088688934585,\n",
       "               0.7899655151830599,\n",
       "               0.7900097687978251,\n",
       "               0.7900551985282704,\n",
       "               0.7901103404377036,\n",
       "               0.7901555184778017,\n",
       "               0.790218257832392,\n",
       "               0.7902833742999709,\n",
       "               0.7903108565895718,\n",
       "               0.79034573802291,\n",
       "               0.790398498123485,\n",
       "               0.7904534742609848,\n",
       "               0.7905172500724967,\n",
       "               0.7905756569343061,\n",
       "               0.790617407349351,\n",
       "               0.7906935202021329,\n",
       "               0.790749007286878,\n",
       "               0.7907775155811559,\n",
       "               0.7908138221875723,\n",
       "               0.7908473332623723,\n",
       "               0.790879612455765,\n",
       "               0.7909467361737358,\n",
       "               0.791001734375566,\n",
       "               0.7910452471690175,\n",
       "               0.7910986268065219,\n",
       "               0.7911529158359073,\n",
       "               0.7912020288003319,\n",
       "               0.7912475491528455,\n",
       "               0.7912937541035372,\n",
       "               0.7913369193738713,\n",
       "               0.7913819748421653,\n",
       "               0.7914275333203643,\n",
       "               0.7914649567908836,\n",
       "               0.7915197155083517,\n",
       "               0.7915834310239962,\n",
       "               0.7916269485529519,\n",
       "               0.791696251906118,\n",
       "               0.7917514881710654,\n",
       "               0.7918094799325979,\n",
       "               0.7918545032217966,\n",
       "               0.7919141383886397,\n",
       "               0.7919549764597166,\n",
       "               0.7920007874022348,\n",
       "               0.7920571903241164,\n",
       "               0.7921054592471264,\n",
       "               0.7921430622927125,\n",
       "               0.7921967175599184,\n",
       "               0.7922418390742683,\n",
       "               0.7922884970626105,\n",
       "               0.7923344202712932,\n",
       "               0.7923759530119674,\n",
       "               0.7924202938142063,\n",
       "               0.7924479485687956,\n",
       "               0.792480014794495,\n",
       "               0.792530894945619,\n",
       "               0.7925934852869319,\n",
       "               0.792631969129861,\n",
       "               0.7926924015964527,\n",
       "               0.7927371210096539,\n",
       "               0.7927972140715221,\n",
       "               0.7928493746959557,\n",
       "               0.7928831730541803,\n",
       "               0.7929374724279281,\n",
       "               0.7929670538603999,\n",
       "               0.7930155091379814,\n",
       "               0.793062453127687,\n",
       "               0.7931223799843907,\n",
       "               0.7931417256932706,\n",
       "               0.7931951936807757,\n",
       "               0.7932560163307418,\n",
       "               0.7932959413017777,\n",
       "               0.7933643535998085,\n",
       "               0.7934234067968137,\n",
       "               0.7934751905305738,\n",
       "               0.7935157219234464,\n",
       "               0.7935711633036,\n",
       "               0.7936233258580172,\n",
       "               0.793674990747994,\n",
       "               0.7937183004457729,\n",
       "               0.7937703276237246,\n",
       "               0.7938233060479805,\n",
       "               0.7938743787984126,\n",
       "               0.7939146597243244,\n",
       "               0.7939546801267949,\n",
       "               0.7939924316331678,\n",
       "               0.7940278247900668,\n",
       "               0.7940605637522867,\n",
       "               0.794018128125226,\n",
       "               0.7941273332621279,\n",
       "               0.7941800893252814,\n",
       "               0.7942412107241899,\n",
       "               0.79428844015581,\n",
       "               0.7943133889591627,\n",
       "               0.794360799459829,\n",
       "               0.794406042422273,\n",
       "               0.7944455108509503,\n",
       "               0.794493041737002,\n",
       "               0.7945186662752729,\n",
       "               0.794579774744076,\n",
       "               0.7946641804648276,\n",
       "               0.7947131517526269,\n",
       "               0.7947502130841588,\n",
       "               0.7948242226890605,\n",
       "               0.7949077270030861,\n",
       "               0.7949358025859505,\n",
       "               0.7949715977259423,\n",
       "               0.7950257339920762,\n",
       "               0.7950442585308225,\n",
       "               0.795114807077083,\n",
       "               0.795154135466368,\n",
       "               0.7951832146784464,\n",
       "               0.7952272559069866,\n",
       "               0.7952324357727205,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               0.7959530988391987,\n",
       "               0.7961062002802722,\n",
       "               0.7961893038635056,\n",
       "               0.7962576669440249,\n",
       "               0.7963108778237238,\n",
       "               0.7963501975912359,\n",
       "               0.7963881510322526,\n",
       "               0.7964475821170863,\n",
       "               0.7964978642639202,\n",
       "               0.796533511410366,\n",
       "               0.7965917175701278,\n",
       "               0.7966465833349383,\n",
       "               0.7967127626399839,\n",
       "               0.7967478499884744,\n",
       "               0.7967982663949744,\n",
       "               0.7968632402881448,\n",
       "               0.7969046587080095,\n",
       "               0.7969483651949888,\n",
       "               0.7970116572093588,\n",
       "               0.797062311488106,\n",
       "               0.7971020135825293,\n",
       "               0.7971524336581016,\n",
       "               0.7971891473837943,\n",
       "               0.7972181514260497,\n",
       "               0.7972302341075663,\n",
       "               0.797259849384752,\n",
       "               0.7973067753169009,\n",
       "               0.7973646242335424,\n",
       "               0.7974261764919175,\n",
       "               0.7974690622269319,\n",
       "               0.7975157296280213,\n",
       "               0.797555441301801,\n",
       "               0.7976004544340205,\n",
       "               0.7976659021086812,\n",
       "               0.7977032634367467,\n",
       "               0.7977462559314397],\n",
       "              'train-NegativeRMSLEMetric-std': [0.016696603038156205,\n",
       "               0.011274795863210365,\n",
       "               0.011294138103594498,\n",
       "               0.011706586137803565,\n",
       "               0.008216744464749738,\n",
       "               0.006690034098723749,\n",
       "               0.0052793296225575,\n",
       "               0.0042749533026791315,\n",
       "               0.003927842668746083,\n",
       "               0.003198656080837617,\n",
       "               0.002632455893277528,\n",
       "               0.002841366193117209,\n",
       "               0.0026440001168787667,\n",
       "               0.002302553529122835,\n",
       "               0.0022064535784540374,\n",
       "               0.002057815920602074,\n",
       "               0.001767563544530287,\n",
       "               0.0016546804726134776,\n",
       "               0.0013900418069015198,\n",
       "               0.0013165100762325714,\n",
       "               0.001253635370243937,\n",
       "               0.0012574042470414116,\n",
       "               0.0013184942582842617,\n",
       "               0.0013127673597031417,\n",
       "               0.0013992430566709402,\n",
       "               0.0014460409615932074,\n",
       "               0.0014777865170730227,\n",
       "               0.0013441758428021993,\n",
       "               0.0013559953030386558,\n",
       "               0.0014020198783828842,\n",
       "               0.0013979315806984275,\n",
       "               0.001395141170756721,\n",
       "               0.001392160715541806,\n",
       "               0.001306959466822262,\n",
       "               0.0012999691930623465,\n",
       "               0.001273950772815806,\n",
       "               0.0011916964734599168,\n",
       "               0.0012018034135194205,\n",
       "               0.0012167796344909115,\n",
       "               0.001254611362892256,\n",
       "               0.0012823074096086574,\n",
       "               0.0012286238468683548,\n",
       "               0.0012535928646419609,\n",
       "               0.0012671897552374155,\n",
       "               0.0013595716527537965,\n",
       "               0.0013572233759552852,\n",
       "               0.001388994320771754,\n",
       "               0.0013615078416568864,\n",
       "               0.001296302144275405,\n",
       "               0.001288752658667169,\n",
       "               0.0013951188608078858,\n",
       "               0.001458146452836701,\n",
       "               0.0014674342227775936,\n",
       "               0.0014514155561879073,\n",
       "               0.0015280781568726223,\n",
       "               0.0015040223320728233,\n",
       "               0.0015963997208875628,\n",
       "               0.0016437496896714754,\n",
       "               0.0016251725833133454,\n",
       "               0.001576279712431383,\n",
       "               0.0015535389977330714,\n",
       "               0.0015917740227996237,\n",
       "               0.0016185673769341978,\n",
       "               0.0016226431245919429,\n",
       "               0.0015684209092903384,\n",
       "               0.0016126111574912188,\n",
       "               0.0016069204381654412,\n",
       "               0.001614540966245396,\n",
       "               0.0015307511223147826,\n",
       "               0.0015560161223837142,\n",
       "               0.001616551185340151,\n",
       "               0.0016729640130056038,\n",
       "               0.0017181484519810518,\n",
       "               0.0017020158863251386,\n",
       "               0.001696306378802602,\n",
       "               0.0016520012050029432,\n",
       "               0.001581835254167488,\n",
       "               0.0016224629527223657,\n",
       "               0.0016043020697708908,\n",
       "               0.001689741642923031,\n",
       "               0.0016692985854889306,\n",
       "               0.0016619066089927736,\n",
       "               0.0016601476753108314,\n",
       "               0.001659474836822018,\n",
       "               0.001698447418957041,\n",
       "               0.0017180945850949498,\n",
       "               0.0017192101196863729,\n",
       "               0.001705900116386692,\n",
       "               0.0016591847868999763,\n",
       "               0.0016984456659731592,\n",
       "               0.001663257683450173,\n",
       "               0.0016344516563017214,\n",
       "               0.001581777190775055,\n",
       "               0.0015845754805268422,\n",
       "               0.001607392415793489,\n",
       "               0.0015926025713161106,\n",
       "               0.0016241406107234825,\n",
       "               0.0016340261741384409,\n",
       "               0.001621169039632824,\n",
       "               0.0016274467578533572,\n",
       "               0.001632854454400508,\n",
       "               0.0016714585146770735,\n",
       "               0.001631505811723162,\n",
       "               0.0016386730194781912,\n",
       "               0.001701052134062704,\n",
       "               0.0016837460981469056,\n",
       "               0.0017596901102461876,\n",
       "               0.0017818286962898366,\n",
       "               0.0017785761848876582,\n",
       "               0.001857804913543841,\n",
       "               0.001916685193176384,\n",
       "               0.00198885325064818,\n",
       "               0.00204960460246122,\n",
       "               0.0021291083293068203,\n",
       "               0.0021653595284785747,\n",
       "               0.0022882095777915275,\n",
       "               0.0024169733197884416,\n",
       "               0.0025134792639821277,\n",
       "               0.0026075282135356115,\n",
       "               0.002684020884119566,\n",
       "               0.0026616053241625874,\n",
       "               0.0026269724104246886,\n",
       "               0.0026390468417013917,\n",
       "               0.0026312544042267714,\n",
       "               0.00259291733094425,\n",
       "               0.0025601338486261767,\n",
       "               0.0025094824422431843,\n",
       "               0.002520722270552136,\n",
       "               0.0025596259746032395,\n",
       "               0.0024766696420474945,\n",
       "               0.002457827153507812,\n",
       "               0.0024364822670199906,\n",
       "               0.0023748931059567146,\n",
       "               0.0023503862092194654,\n",
       "               0.0023023469955429842,\n",
       "               0.0022354921459468544,\n",
       "               0.0022343949681068675,\n",
       "               0.0022213305567442385,\n",
       "               0.002250812104283775,\n",
       "               0.002180469869352496,\n",
       "               0.0021322993540129126,\n",
       "               0.002200965210696485,\n",
       "               0.0021242162063475406,\n",
       "               0.0021533682194787673,\n",
       "               0.002215986070728521,\n",
       "               0.0021389910256054403,\n",
       "               0.0021318429728020734,\n",
       "               0.0021443400431352547,\n",
       "               0.002179827987723085,\n",
       "               0.0021722394905915863,\n",
       "               0.0022461306301958775,\n",
       "               0.0022309484918495636,\n",
       "               0.0022194059062872733,\n",
       "               0.0022024511816643114,\n",
       "               0.002163231447303696,\n",
       "               0.0021466343124904945,\n",
       "               0.0021601047236797337,\n",
       "               0.0021812596494361724,\n",
       "               0.0021207929503542985,\n",
       "               0.002141568913183123,\n",
       "               0.0021064240232395444,\n",
       "               0.0021188879494335525,\n",
       "               0.002166492454145113,\n",
       "               0.0020980900163067363,\n",
       "               0.0020853457359567952,\n",
       "               0.002126891087509922,\n",
       "               0.0021033980886143513,\n",
       "               0.0021238536647710466,\n",
       "               0.002130100809337084,\n",
       "               0.0021526488606397175,\n",
       "               0.002138121875156548,\n",
       "               0.002188872939307313,\n",
       "               0.0022576382775303418,\n",
       "               0.002246671912659704,\n",
       "               0.0022278308673004572,\n",
       "               0.0022314818202239317,\n",
       "               0.002214648991066666,\n",
       "               0.002240167336622758,\n",
       "               0.0022833038079795864,\n",
       "               0.002302166237327236,\n",
       "               0.0023551497909828412,\n",
       "               0.002355815846850674,\n",
       "               0.002311336088502375,\n",
       "               0.0022957448050617375,\n",
       "               0.0022758057441069363,\n",
       "               0.0023111162344264646,\n",
       "               0.0023380166510707126,\n",
       "               0.0023723424238788334,\n",
       "               0.0023997734714082,\n",
       "               0.0024244641050468804,\n",
       "               0.002410587782461367,\n",
       "               0.00243469465792928,\n",
       "               0.002470080581256285,\n",
       "               0.0024716690535364753,\n",
       "               0.002448210948086702,\n",
       "               0.0024902876582507563,\n",
       "               0.002449795309823988,\n",
       "               0.0024422124711165434,\n",
       "               0.0024100235671975947,\n",
       "               0.002442556594824184,\n",
       "               0.002459622285377681,\n",
       "               0.0024564983521612512,\n",
       "               0.0025002630735945094,\n",
       "               0.002473196201386936,\n",
       "               0.002426806610791826,\n",
       "               0.0024613831305747447,\n",
       "               0.0024857342320600785,\n",
       "               0.0025480428657822145,\n",
       "               0.0025461889742673122,\n",
       "               0.0025287678268477286,\n",
       "               0.002546042478265901,\n",
       "               0.00255848780630464,\n",
       "               0.0025090926334527222,\n",
       "               0.0025251869629127916,\n",
       "               0.0025109053023065074,\n",
       "               0.0025316295557454866,\n",
       "               0.002561255320362195,\n",
       "               0.0025886496463946047,\n",
       "               0.002617675836894714,\n",
       "               0.0025925711917029913,\n",
       "               0.0026233756889561073,\n",
       "               0.0026323686462366848,\n",
       "               0.0026650540886072193,\n",
       "               0.002616004586319682,\n",
       "               0.002591972566299377,\n",
       "               0.0026045487164186093,\n",
       "               0.0026553518117688377,\n",
       "               0.002699880504829849,\n",
       "               0.002713452765857221,\n",
       "               0.0027320420777616513,\n",
       "               0.0027541315583410136,\n",
       "               0.0027710911307898753,\n",
       "               0.002816335870103364,\n",
       "               0.002755867123645992,\n",
       "               0.002783691161397198,\n",
       "               0.0027951659526444634,\n",
       "               0.0027924432869085893,\n",
       "               0.0027505234939225387,\n",
       "               0.0027547573776019307,\n",
       "               0.0027474791379414718,\n",
       "               0.0026972022854073346,\n",
       "               0.0026476231446592043,\n",
       "               0.002648999012019287,\n",
       "               0.0026477591070392516,\n",
       "               0.002688148843294788,\n",
       "               0.002665955464316117,\n",
       "               0.0026505577201075357,\n",
       "               0.0026435535127324176,\n",
       "               0.0026033498124188683,\n",
       "               0.002632920945052241,\n",
       "               0.0026812094669508707,\n",
       "               0.002703839766884072,\n",
       "               0.00269014367293675,\n",
       "               0.0027210593596015834,\n",
       "               0.002748919105426792,\n",
       "               0.0027589788535564938,\n",
       "               0.0027589335299422328,\n",
       "               0.0027759010673039522,\n",
       "               0.0028006628067536645,\n",
       "               0.0027620916443947554,\n",
       "               0.0027511386132292545,\n",
       "               0.002782667523752659,\n",
       "               0.002786357289162335,\n",
       "               0.002819298432093814,\n",
       "               0.002827162223393205,\n",
       "               0.0028138962622658143,\n",
       "               0.002819828914618564,\n",
       "               0.0028178903883674996,\n",
       "               0.0028484727428009543,\n",
       "               0.002886076363472942,\n",
       "               0.0028614197212125936,\n",
       "               0.002895301107984339,\n",
       "               0.0029307624234785512,\n",
       "               0.0029751504013484737,\n",
       "               0.0029717838335045106,\n",
       "               0.002960138741230415,\n",
       "               0.0029845328628484587,\n",
       "               0.0029680195482066677,\n",
       "               0.002942625705883944,\n",
       "               0.002944358752917962,\n",
       "               0.00295460210829628,\n",
       "               0.002914399262214699,\n",
       "               0.0029171539411833747,\n",
       "               0.002913685174721226,\n",
       "               0.0029163498564911817,\n",
       "               0.0029428171718918644,\n",
       "               0.002947077911250608,\n",
       "               0.0029266332982573018,\n",
       "               0.0029064610049760052,\n",
       "               0.0028874907650732647,\n",
       "               0.0028816968843995926,\n",
       "               0.0028745775030028366,\n",
       "               0.002872977156256693,\n",
       "               0.002888640016001207,\n",
       "               0.002898145331248327,\n",
       "               0.0028753502118719996,\n",
       "               0.002854882517149433,\n",
       "               0.0029077373039618705,\n",
       "               0.002895267050212201,\n",
       "               0.0028772879758785937,\n",
       "               0.0028146239459293184,\n",
       "               0.0027846966353154,\n",
       "               0.0028048380249144794,\n",
       "               0.002824865115007449,\n",
       "               0.0028363411792599225,\n",
       "               0.002817265425936433,\n",
       "               0.002826431905609706,\n",
       "               0.0028297981430628403,\n",
       "               0.002851066990993879,\n",
       "               0.002853223243973284,\n",
       "               0.0028450347736503423,\n",
       "               0.0028808074405402926,\n",
       "               0.0028307328003923994,\n",
       "               0.0028329992162251768,\n",
       "               0.002828403083103756,\n",
       "               0.0028562269169650546,\n",
       "               0.0028698498456999457,\n",
       "               0.0028504441147706922,\n",
       "               0.002858414671089126,\n",
       "               0.002879010354093288,\n",
       "               0.0028302675382020862,\n",
       "               0.002841183863360254,\n",
       "               0.002795901880739447,\n",
       "               0.0028145425022318613,\n",
       "               0.0028131613007059083,\n",
       "               0.0028042105087621896,\n",
       "               0.0028224042584410343,\n",
       "               0.0027841802814725643,\n",
       "               0.002810187035158342,\n",
       "               0.002785693012757793,\n",
       "               0.0027571456694816063,\n",
       "               0.0027505659123097876,\n",
       "               0.0027740695221793836,\n",
       "               0.0028012789447681794,\n",
       "               0.0028000118359546213,\n",
       "               0.0027819160646548944,\n",
       "               0.0027718543681655677,\n",
       "               0.00278099138287177,\n",
       "               0.0027850774713528976,\n",
       "               0.002801542416750289,\n",
       "               0.0028138020043052144,\n",
       "               0.002784216268664726,\n",
       "               0.0027880400027021,\n",
       "               0.002779020519414899,\n",
       "               0.0027820737956624126,\n",
       "               0.0027671573099496754,\n",
       "               0.0027865109799999725,\n",
       "               0.0028190719591882196,\n",
       "               0.002842037805624334,\n",
       "               0.0028490114360961447,\n",
       "               0.002830810896273642,\n",
       "               0.0028828046209471133,\n",
       "               0.0028492096167157467,\n",
       "               0.0028373896355618666,\n",
       "               0.002834348035962949,\n",
       "               0.002838457951748836,\n",
       "               0.002850647138525882,\n",
       "               0.002867183758974058,\n",
       "               0.0028755355827725783,\n",
       "               0.002872159631212915,\n",
       "               0.00285702706618287,\n",
       "               0.0028904728755815043,\n",
       "               0.0028898368555522187,\n",
       "               0.0028783890928335467,\n",
       "               0.002879779383026134,\n",
       "               0.002892103777139655,\n",
       "               0.002926419518554436,\n",
       "               0.0029093520007653427,\n",
       "               0.0029106182992363963,\n",
       "               0.002914307056935872,\n",
       "               0.0029217741452895406,\n",
       "               0.0029028831168514176,\n",
       "               0.002924157086496995,\n",
       "               0.0029575502063052045,\n",
       "               0.002957668538461828,\n",
       "               0.0029457051155938074,\n",
       "               0.002949409419200712,\n",
       "               0.0029380258330884523,\n",
       "               0.0029330985534350125,\n",
       "               0.002954591287259306,\n",
       "               0.0029500538378809116,\n",
       "               0.0029479343989886526,\n",
       "               0.0029416113567909876,\n",
       "               0.002942030574185898,\n",
       "               0.0029433492415935052,\n",
       "               0.002912471394650213,\n",
       "               0.002950329577871667,\n",
       "               0.00292585578618748,\n",
       "               0.002933777414181563,\n",
       "               0.002917693080213052,\n",
       "               0.0029188865705585924,\n",
       "               0.0029549069008935674,\n",
       "               0.002964839465798213,\n",
       "               0.002978832456664799,\n",
       "               0.002992852281845008,\n",
       "               0.00302187923287117,\n",
       "               0.0029802267197077004,\n",
       "               0.002997196745354623,\n",
       "               0.0030440567860027063,\n",
       "               0.0030419508828653757,\n",
       "               0.003060644314597935,\n",
       "               0.0030473355082674173,\n",
       "               0.003027282864693727,\n",
       "               0.003026521358977404,\n",
       "               0.0030495912223828235,\n",
       "               0.0030576075347924,\n",
       "               0.0030492538237282926,\n",
       "               0.003052733792840801,\n",
       "               0.0030564813568489868,\n",
       "               0.003059338802362506,\n",
       "               0.0030624670343892685,\n",
       "               0.003080245179124574,\n",
       "               0.0031111768766688107,\n",
       "               0.0031140254016269,\n",
       "               0.003117114342509478,\n",
       "               0.0031009658470318824,\n",
       "               0.0031135549041776587,\n",
       "               0.003105145033184739,\n",
       "               0.0030918580917549633,\n",
       "               0.003128447089865455,\n",
       "               0.003158703347630721,\n",
       "               0.003149830807102326,\n",
       "               0.00313692340181581,\n",
       "               0.003153396797663419,\n",
       "               0.0031786528175252985,\n",
       "               0.003158584066301574,\n",
       "               0.003122739028367672,\n",
       "               0.003129615077102212,\n",
       "               0.0031025086084294105,\n",
       "               0.0031262379567153966,\n",
       "               0.0031333409267058476,\n",
       "               0.0031396615464431535,\n",
       "               0.0031226139103468797,\n",
       "               0.003155957011550208,\n",
       "               0.00315460099775324,\n",
       "               0.003174722075969792,\n",
       "               0.0031647174936931186,\n",
       "               0.0031405401335700594,\n",
       "               0.0030976075972420636,\n",
       "               0.0030849406731310878,\n",
       "               0.003058064558341368,\n",
       "               0.0030535724770284125,\n",
       "               0.0030392058959849686,\n",
       "               0.003032545105079066,\n",
       "               0.003028423467538473,\n",
       "               0.0030454412677108785,\n",
       "               0.003040733055312373,\n",
       "               0.003041294788941914,\n",
       "               0.0030775004445133466,\n",
       "               0.0030803504920769724,\n",
       "               0.0030735132933024977,\n",
       "               0.003068492458932356,\n",
       "               0.003085937899773312,\n",
       "               0.0030912778187652413,\n",
       "               0.003052773507907271,\n",
       "               0.003048369887662094,\n",
       "               0.0030430966024004597,\n",
       "               0.0030234992516651187,\n",
       "               0.003056559015727524,\n",
       "               0.003059951413656586,\n",
       "               0.003039658483868891,\n",
       "               0.0030380972850344804,\n",
       "               0.0030527728073059717,\n",
       "               0.0030743223508054906,\n",
       "               0.003074062625811301,\n",
       "               0.0030791388095678172,\n",
       "               0.003078845551400252,\n",
       "               0.0030952267192749577,\n",
       "               0.00308793497821906,\n",
       "               0.0030863196642484333,\n",
       "               0.0030741096482618468,\n",
       "               0.0030697103467329165,\n",
       "               0.003082163427878666,\n",
       "               0.0031001507564566125,\n",
       "               0.0030920470055985595,\n",
       "               0.0030638808999007613,\n",
       "               0.0030784389234406236,\n",
       "               0.0030874446359968007,\n",
       "               0.0030733284828751164,\n",
       "               0.0030629924393185005,\n",
       "               0.0030759897919773187,\n",
       "               0.0030624543940388164,\n",
       "               0.0030479560527429572,\n",
       "               0.00303367476882392,\n",
       "               0.0030138783170552737,\n",
       "               0.003021528337926148,\n",
       "               0.0029960682836926863,\n",
       "               0.003010765312063247,\n",
       "               0.003006826859431005,\n",
       "               0.002999974177775624,\n",
       "               0.0029813470127921238,\n",
       "               0.002973968480460187,\n",
       "               0.0029844526700491328,\n",
       "               0.002981596677600427,\n",
       "               0.002975823585481696,\n",
       "               0.0029955085050613595,\n",
       "               0.003010286842136827,\n",
       "               0.0030250258303650517,\n",
       "               0.003035100829427222,\n",
       "               0.0030427905432371454,\n",
       "               0.003013722045735633,\n",
       "               0.0030216176222419354,\n",
       "               0.0030326271270689335,\n",
       "               0.003030580842709418,\n",
       "               0.0030262851364132115,\n",
       "               0.003056378003049928,\n",
       "               0.0030897396687899893,\n",
       "               0.003074927421120512,\n",
       "               0.0030810259000823314,\n",
       "               0.0030840577405586,\n",
       "               0.0030906549944579873,\n",
       "               0.003089760674335202,\n",
       "               0.0031005812358867357,\n",
       "               0.0030863442864496812,\n",
       "               0.00308909608049088,\n",
       "               0.0031127712641454233,\n",
       "               0.003111118011985365,\n",
       "               0.003083236347826282,\n",
       "               0.003090547137173416,\n",
       "               0.003080933972213518,\n",
       "               0.0030930130876127915,\n",
       "               0.003069477098168557,\n",
       "               0.0030494182562986953,\n",
       "               0.003041361608259816,\n",
       "               0.003039616101366252,\n",
       "               0.0030405210967392824,\n",
       "               0.0030603851072923943,\n",
       "               0.0030646047283453197,\n",
       "               0.003048204788931163,\n",
       "               0.003041202009012641,\n",
       "               0.0030710603292508744,\n",
       "               0.0030522539371236135,\n",
       "               0.0030585183287436665,\n",
       "               0.003081597576661028,\n",
       "               0.003080219436701931,\n",
       "               0.0030608849784329304,\n",
       "               0.0030687838494049768,\n",
       "               0.0030569549908439665,\n",
       "               0.00306303399388406,\n",
       "               0.0030632297761814743,\n",
       "               0.003070141651228362,\n",
       "               0.003085371052473455,\n",
       "               0.003078165189238548,\n",
       "               0.00309341917912515,\n",
       "               0.003074349381722136,\n",
       "               0.003088156954416846,\n",
       "               0.00308115710427799,\n",
       "               0.003095759943391285,\n",
       "               0.003083336738988451,\n",
       "               0.003048666678962978,\n",
       "               0.0030569167078891746,\n",
       "               0.0030485905447029202,\n",
       "               0.0030770129000003235,\n",
       "               0.003108291931624747,\n",
       "               0.0031302977907682744,\n",
       "               0.0031365868946691858,\n",
       "               0.0031620046732711736,\n",
       "               0.0031605371373496,\n",
       "               0.003165799221679882,\n",
       "               0.003173086432017676,\n",
       "               0.0031997784367595794,\n",
       "               0.003203296453105132,\n",
       "               0.003230332887347016,\n",
       "               0.0032441712491965116,\n",
       "               0.003266456612181634,\n",
       "               0.0032973693812284243,\n",
       "               0.0032835651294033895,\n",
       "               0.0032841864769304167,\n",
       "               0.003290746355964788,\n",
       "               0.003290439385180662,\n",
       "               0.0032937024681598203,\n",
       "               0.0032869184752479597,\n",
       "               0.0032796563681744472,\n",
       "               0.0032956538229147988,\n",
       "               0.003294027067295982,\n",
       "               0.003323803953845693,\n",
       "               0.0033359024779939673,\n",
       "               0.003339607508399289,\n",
       "               0.003311093235286073,\n",
       "               0.0032815986081057504,\n",
       "               0.003267890386626899,\n",
       "               0.003269677233481907,\n",
       "               0.003292157517554464,\n",
       "               0.0032849512198502044,\n",
       "               0.0033046805529116786,\n",
       "               0.003309638075950796,\n",
       "               0.0033232114165447973,\n",
       "               0.003314718619511597,\n",
       "               0.0033241513943712167,\n",
       "               0.0033469231296371195,\n",
       "               0.0033578974525681152,\n",
       "               0.003359298940973314,\n",
       "               0.0033460211977318233,\n",
       "               0.0033412661350480212,\n",
       "               0.0033331526299936413,\n",
       "               0.0033228647494790136,\n",
       "               0.0033181645607609786,\n",
       "               0.0033340012202980997,\n",
       "               0.0033196947669306026,\n",
       "               0.0033305894782233417,\n",
       "               0.0033311202023819787,\n",
       "               0.003335128688112573,\n",
       "               0.0033598588067096434,\n",
       "               0.0033519611737147614,\n",
       "               0.0033673892034098425,\n",
       "               0.0033742713268572404,\n",
       "               0.003378782128767015,\n",
       "               0.0033832633479813365,\n",
       "               0.0033676532931179903,\n",
       "               0.0033766359525829854,\n",
       "               0.003373466454383573,\n",
       "               0.0033865258854581088,\n",
       "               0.0033951714743660814,\n",
       "               0.003400176778669038,\n",
       "               0.0034277666550942324,\n",
       "               0.0034198741640836168,\n",
       "               0.0034244277951517212,\n",
       "               0.003409622522825533,\n",
       "               0.0034022729516092628,\n",
       "               0.003390923303997128,\n",
       "               0.0034002389140342674,\n",
       "               0.0034105771778391533,\n",
       "               0.0034239533399835474,\n",
       "               0.003463320826726289,\n",
       "               0.0034739847867808612,\n",
       "               0.0034746161085989348,\n",
       "               0.003508809750621862,\n",
       "               0.0035091429521450323,\n",
       "               0.0035103329139817116,\n",
       "               0.0035289434747978634,\n",
       "               0.003526588163480893,\n",
       "               0.0035148714713504696,\n",
       "               0.0035229175589273428,\n",
       "               0.003556503227224329,\n",
       "               0.0035305219569550625,\n",
       "               0.003536142042210811,\n",
       "               0.003528474790820655,\n",
       "               0.0034838846047918945,\n",
       "               0.003471900938141907,\n",
       "               0.0034960037080548016,\n",
       "               0.00349274465163499,\n",
       "               0.0035012521464356555,\n",
       "               0.003507081472117224,\n",
       "               0.0035070521663978236,\n",
       "               0.0034957328841674394,\n",
       "               0.003498640387788997,\n",
       "               0.003472577789811584,\n",
       "               0.0034654632031478573,\n",
       "               0.0034668038975068744,\n",
       "               0.003449166353447321,\n",
       "               0.0034544281138622237,\n",
       "               0.00345477213574765,\n",
       "               0.0034499286034381875,\n",
       "               0.00344775566096317,\n",
       "               0.0034123896384994014,\n",
       "               0.0034163922604378482,\n",
       "               0.003402310660058543,\n",
       "               0.0034168139178441252,\n",
       "               0.0034050133220564927,\n",
       "               0.003387343856737361,\n",
       "               0.003404139141615504,\n",
       "               0.0033752897399277142,\n",
       "               0.0033520949432228643,\n",
       "               0.0033198938797313463,\n",
       "               0.0033502980467377944,\n",
       "               0.0033827392239840553,\n",
       "               0.0033918829587885366,\n",
       "               0.0033757632289919946,\n",
       "               0.0033583578457369026,\n",
       "               0.0033638645008229073,\n",
       "               0.0033664044098012835,\n",
       "               0.0033586933431504103,\n",
       "               0.0033423516932866216,\n",
       "               0.0033446686670524084,\n",
       "               0.0033549018495655078,\n",
       "               0.003372858512968337,\n",
       "               0.0033981183920258696,\n",
       "               0.0034407204432117234,\n",
       "               0.0034154518559145687,\n",
       "               0.003430268972313246,\n",
       "               0.0034238139652158404,\n",
       "               0.003403517567788666,\n",
       "               0.003382419272660209,\n",
       "               0.0034063924843750816,\n",
       "               0.0034015082681776248,\n",
       "               0.0033958245024012963,\n",
       "               0.0034096327133521956,\n",
       "               0.003413268654998579,\n",
       "               0.003421243335634189,\n",
       "               0.003435912859983507,\n",
       "               0.0034313853879674653,\n",
       "               0.003452393098017179,\n",
       "               0.003476324101246793,\n",
       "               0.003468100867621603,\n",
       "               0.003493964345379944,\n",
       "               0.003483655075779201,\n",
       "               0.0034803334471254334,\n",
       "               0.0034864229487656844,\n",
       "               0.003488703840021054,\n",
       "               0.003503547896518605,\n",
       "               0.0035252801843093273,\n",
       "               0.0035400132311986954,\n",
       "               0.003553661103411191,\n",
       "               0.003562831907236583,\n",
       "               0.0035702639198805365,\n",
       "               0.0035675460361355577,\n",
       "               0.0035658290058425152,\n",
       "               0.0035934188469843088,\n",
       "               0.0036012877913490383,\n",
       "               0.003606149863578242,\n",
       "               0.0036086970607075866,\n",
       "               0.0036223876629540197,\n",
       "               0.003607022435431942,\n",
       "               0.0036187509398688056,\n",
       "               0.003656836057026701,\n",
       "               0.003637933391065613,\n",
       "               0.0036246284598998443,\n",
       "               0.0036320460531348085,\n",
       "               0.003634639456087632,\n",
       "               0.003650720160079414,\n",
       "               0.0036497157138653926,\n",
       "               0.003655235931128822,\n",
       "               0.003645655183095488,\n",
       "               0.003635535556471695,\n",
       "               0.003635175725401912,\n",
       "               0.0036066096936054398,\n",
       "               0.0036278414905875625,\n",
       "               0.0036327307689503104,\n",
       "               0.003637023846636183,\n",
       "               0.003638633216072985,\n",
       "               0.0036317813522317565,\n",
       "               0.0036280065472291917,\n",
       "               0.0036242085764988954,\n",
       "               0.0036315419010208324,\n",
       "               0.0036575853388870266,\n",
       "               0.0036591287779943037,\n",
       "               0.0036651377807118005,\n",
       "               0.0036393303106813926,\n",
       "               0.003656580484141994,\n",
       "               0.0036433478711269856,\n",
       "               0.0036284872197817467,\n",
       "               0.003613719871403506,\n",
       "               0.003614170241206588,\n",
       "               0.003619777969959082,\n",
       "               0.0036173866891330998,\n",
       "               0.0036153993179040503,\n",
       "               0.003615377279499337,\n",
       "               0.0036243448621337587,\n",
       "               0.003624676727483264,\n",
       "               0.0036016765054636946,\n",
       "               0.0036046478284413076,\n",
       "               0.0036030683194672223,\n",
       "               0.0036180994292196117,\n",
       "               0.0036331944335311157,\n",
       "               0.0036372255840202805,\n",
       "               0.003648605091781727,\n",
       "               0.0036332995135268083,\n",
       "               0.0036331281572394543,\n",
       "               0.0036193782455608515,\n",
       "               0.003618173695744888,\n",
       "               0.003624457342011361,\n",
       "               0.003623803772444433,\n",
       "               0.003620923214534801,\n",
       "               0.003621219851191378,\n",
       "               0.003628264084041712,\n",
       "               0.0036253795328127954,\n",
       "               0.0036194060659586655,\n",
       "               0.0036162276112628815,\n",
       "               0.003620760771552016,\n",
       "               0.0036090252654261443,\n",
       "               0.0036298098724910233,\n",
       "               0.003651211748986293,\n",
       "               0.0036353319942519485,\n",
       "               0.0036425034896928475,\n",
       "               0.003639688692926966,\n",
       "               0.0036333116278233143,\n",
       "               0.0036242024746470725,\n",
       "               0.00362792722200973,\n",
       "               0.003625269340514429,\n",
       "               0.003637767781061446,\n",
       "               0.0036298233775388707,\n",
       "               0.0036239847297057375,\n",
       "               0.003617440547803744,\n",
       "               0.003607069044396886,\n",
       "               0.003595820781139578,\n",
       "               0.0035985804121375603,\n",
       "               0.0036118632748779584,\n",
       "               0.0035827782326416685,\n",
       "               0.0035730052114643445,\n",
       "               0.003575816295112722,\n",
       "               0.003565634510576954,\n",
       "               0.003565142108020921,\n",
       "               0.003559990295156381,\n",
       "               0.003583552365743793,\n",
       "               0.00360206944397036,\n",
       "               0.0036045181866357252,\n",
       "               0.0036024975773281606,\n",
       "               0.003580032226064598,\n",
       "               0.0035785693322619365,\n",
       "               0.003573058560729058,\n",
       "               0.003584773196606779,\n",
       "               0.003576908705649547,\n",
       "               0.0035525948291775535,\n",
       "               0.0035270072386067246,\n",
       "               0.0035107922032473126,\n",
       "               0.003510265479551778,\n",
       "               0.0035164765903287184,\n",
       "               0.003507016941216717,\n",
       "               0.003501898308066345,\n",
       "               0.0035074049391352153,\n",
       "               0.00349402926045208,\n",
       "               0.0034876506097835914,\n",
       "               0.0034665787989654265,\n",
       "               0.0034579324884923815,\n",
       "               0.003458129968675449,\n",
       "               0.003444371637298243,\n",
       "               0.00343406730553555,\n",
       "               0.003432929273755068,\n",
       "               0.003440828257861169,\n",
       "               0.0034272749005820743,\n",
       "               0.003426383732047255,\n",
       "               0.0034172387211712,\n",
       "               0.0034325688210815374,\n",
       "               0.0034300030704338866,\n",
       "               0.0034315844039468847,\n",
       "               0.003432119742448633,\n",
       "               0.0034189923653002376,\n",
       "               0.00344335487016569,\n",
       "               0.003439423857598228,\n",
       "               0.0034309360067885995,\n",
       "               0.0034408569650309366,\n",
       "               0.0034488196689478165,\n",
       "               0.0034693724145446036,\n",
       "               0.00346451774754454,\n",
       "               0.0034596718072835188,\n",
       "               0.0034613410965835772,\n",
       "               0.00344880450224188,\n",
       "               0.0034538150742013107,\n",
       "               0.003459846999681718,\n",
       "               0.003457407902582274,\n",
       "               0.0034516389982193084,\n",
       "               0.0034575698771479525,\n",
       "               0.0034372813068094333,\n",
       "               0.0034528052342970967,\n",
       "               0.003454107518603382,\n",
       "               0.003451715134338051,\n",
       "               0.003438089007417476,\n",
       "               0.003444105818990264,\n",
       "               0.0034386988586751653,\n",
       "               0.0034184885633157624,\n",
       "               0.003427041066763177,\n",
       "               0.003415699115761301,\n",
       "               0.0034281479817669194,\n",
       "               0.0034296878174477487,\n",
       "               0.0034314679599524224,\n",
       "               0.0034305087913708006,\n",
       "               0.003424295323664861,\n",
       "               0.00340933700778609,\n",
       "               0.0034209505652858995,\n",
       "               0.003417694352642564,\n",
       "               0.0034122521751431168,\n",
       "               0.003425576299444763,\n",
       "               0.0034023886302500723,\n",
       "               0.0033921495482013937,\n",
       "               0.0033932309598748846,\n",
       "               0.0033992986288238713,\n",
       "               0.0034082005762044927,\n",
       "               0.0034072988929240867,\n",
       "               0.0034006703132307367,\n",
       "               0.003423946915675269,\n",
       "               0.003425978897981792,\n",
       "               0.003421191674953314,\n",
       "               0.003422720005530605,\n",
       "               0.0034004049905225374,\n",
       "               0.003395909171437316,\n",
       "               0.0033903593270460082,\n",
       "               0.003378457452230899,\n",
       "               0.003382005053545694,\n",
       "               0.003368576301668041,\n",
       "               0.0033774750013129476,\n",
       "               0.0033946979744735243,\n",
       "               0.0034051570841071794,\n",
       "               0.0034020940738563295,\n",
       "               0.0033789760457980608,\n",
       "               0.0033611456842167903,\n",
       "               0.003374812163432593,\n",
       "               0.0033860974947161884,\n",
       "               0.00340424646758814,\n",
       "               0.0034025386008888973,\n",
       "               0.0033981054618656757,\n",
       "               0.003392326358400624,\n",
       "               0.003387829955788923,\n",
       "               0.003367499348296136,\n",
       "               0.003345824339875594,\n",
       "               0.0033396585049426915,\n",
       "               0.003313635362439695,\n",
       "               0.00329362752839167,\n",
       "               0.0032749411975135606,\n",
       "               0.003278564099889782,\n",
       "               0.0032766554829013095,\n",
       "               0.0032988214051419143,\n",
       "               0.0032797427822251857,\n",
       "               0.0032862776426399527,\n",
       "               0.0032997452063783434,\n",
       "               0.0033012746685314545,\n",
       "               0.0033016725416379843,\n",
       "               0.0033233559893761904,\n",
       "               0.0033228290917196104,\n",
       "               0.0033100558273596007,\n",
       "               0.003298147556759261,\n",
       "               0.0032821869068503087,\n",
       "               0.0032773233763743383,\n",
       "               0.0032933288375327067,\n",
       "               0.003279110028400089,\n",
       "               0.003267579087299468,\n",
       "               0.0032681231624838024,\n",
       "               0.0032574179162744837,\n",
       "               0.00324907576378248,\n",
       "               0.003246957394914013,\n",
       "               0.003262861632355449,\n",
       "               0.0032642794733498894,\n",
       "               0.0032683620684499023,\n",
       "               0.0032724379057601223,\n",
       "               0.0033212404815209195,\n",
       "               0.003269528792620315,\n",
       "               0.00327336148851199,\n",
       "               0.003293247064223695,\n",
       "               0.00331722554324575,\n",
       "               0.003314871278916994,\n",
       "               0.003310707744912961,\n",
       "               0.003305346216058592,\n",
       "               0.00331036680580225,\n",
       "               0.00332037623603921,\n",
       "               0.003344796098082711,\n",
       "               0.003355780018828604,\n",
       "               0.003304108860329176,\n",
       "               0.003306066373453264,\n",
       "               0.003302393319715375,\n",
       "               0.0032828939440268947,\n",
       "               0.003271900860017794,\n",
       "               0.0032857956807641284,\n",
       "               0.003284338465585304,\n",
       "               0.003300466310876642,\n",
       "               0.0033222800773903896,\n",
       "               0.003321627911658631,\n",
       "               0.00331656027588767,\n",
       "               0.0033196930627996256,\n",
       "               0.0033258122302204894,\n",
       "               0.003346507457114088,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               nan,\n",
       "               0.0033618297714760512,\n",
       "               0.003311943722723641,\n",
       "               0.00333992803215663,\n",
       "               0.0033428025985287164,\n",
       "               0.0033409842182935876,\n",
       "               0.003351946540071436,\n",
       "               0.00334754124870683,\n",
       "               0.0033353487477481695,\n",
       "               0.0033285682734131884,\n",
       "               0.003331489353594509,\n",
       "               0.0033282912582093273,\n",
       "               0.0033145270048675767,\n",
       "               0.0032947324994591844,\n",
       "               0.003296015809199488,\n",
       "               0.0032867883569106726,\n",
       "               0.0032815132903646397,\n",
       "               0.003290723892796316,\n",
       "               0.003269122372227506,\n",
       "               0.0032768442738767617,\n",
       "               0.003293946945697891,\n",
       "               0.003305236108468989,\n",
       "               0.003323759295200243,\n",
       "               0.003310918975065404,\n",
       "               0.0033191813369845076,\n",
       "               0.0033384721335786064,\n",
       "               0.0033438355418309758,\n",
       "               0.0033296185455867814,\n",
       "               0.003327131173558848,\n",
       "               0.0032958967883790365,\n",
       "               0.003274273999589175,\n",
       "               0.003295508790195153,\n",
       "               0.0032976923330064792,\n",
       "               0.0033087485786752035,\n",
       "               0.003287081391122892,\n",
       "               0.003286884920309408,\n",
       "               0.0032839752572390384],\n",
       "              'test-RMSE-mean': [871.1173798238797,\n",
       "               861.5386318835672,\n",
       "               852.1916427248952,\n",
       "               843.2243093163537,\n",
       "               834.3431096458177,\n",
       "               826.5120689562048,\n",
       "               818.33218214271,\n",
       "               810.3154565211473,\n",
       "               802.4982932480515,\n",
       "               795.2664223562437,\n",
       "               788.4105076232387,\n",
       "               781.8232597732085,\n",
       "               775.580138197037,\n",
       "               769.3501267085664,\n",
       "               763.0636697603,\n",
       "               757.2238276331054,\n",
       "               751.735432906725,\n",
       "               746.3906123020731,\n",
       "               740.921027026473,\n",
       "               735.9058250213154,\n",
       "               730.9521109846015,\n",
       "               726.421574274951,\n",
       "               721.8591685219203,\n",
       "               717.5593047823321,\n",
       "               713.2148366589445,\n",
       "               709.0982864130842,\n",
       "               705.218835307661,\n",
       "               701.5883192677641,\n",
       "               698.0735225562157,\n",
       "               694.7356988391747,\n",
       "               691.3751561392407,\n",
       "               688.1163753426388,\n",
       "               685.2117065490876,\n",
       "               682.2517092540546,\n",
       "               679.4171099171128,\n",
       "               676.26704188972,\n",
       "               673.5315625933465,\n",
       "               670.7810711637351,\n",
       "               668.2657200190959,\n",
       "               665.7332286160622,\n",
       "               663.2975958852136,\n",
       "               660.9925808529109,\n",
       "               658.6938996591737,\n",
       "               656.5147074318318,\n",
       "               654.3160314386256,\n",
       "               652.2763693638277,\n",
       "               650.316092104639,\n",
       "               648.5407685250742,\n",
       "               646.6775006813366,\n",
       "               644.8065667340812,\n",
       "               643.0231518994912,\n",
       "               641.5387056247384,\n",
       "               640.0648260519229,\n",
       "               638.4653143340105,\n",
       "               637.005017853143,\n",
       "               635.4647658480679,\n",
       "               634.1488482642724,\n",
       "               632.8937281874494,\n",
       "               631.6418914718196,\n",
       "               630.3906668379135,\n",
       "               629.0693488425015,\n",
       "               627.946038174451,\n",
       "               626.7828505503657,\n",
       "               625.5772820871175,\n",
       "               624.3366083647281,\n",
       "               623.3296865252918,\n",
       "               622.3629081815175,\n",
       "               621.3863623475896,\n",
       "               620.3587826529323,\n",
       "               619.4814185208527,\n",
       "               618.5606172047451,\n",
       "               617.7542581616574,\n",
       "               617.0340162858324,\n",
       "               616.2080018259763,\n",
       "               615.415979089065,\n",
       "               614.7621757845254,\n",
       "               614.0739079279509,\n",
       "               613.4017094354007,\n",
       "               612.7549040819815,\n",
       "               612.2147121158637,\n",
       "               611.5247151456007,\n",
       "               610.8873173091221,\n",
       "               610.2545279510836,\n",
       "               609.6085101953694,\n",
       "               608.9335858236734,\n",
       "               608.3587960967119,\n",
       "               607.809870931535,\n",
       "               607.2053553853182,\n",
       "               606.7064675226627,\n",
       "               606.2439124705817,\n",
       "               605.7393861835814,\n",
       "               605.2235110494305,\n",
       "               604.7994468721837,\n",
       "               604.3604766760378,\n",
       "               603.9164898400608,\n",
       "               603.5481917232823,\n",
       "               603.1261671651231,\n",
       "               602.670190920622,\n",
       "               602.1841678949243,\n",
       "               601.8730473658361,\n",
       "               601.5656966128525,\n",
       "               601.2173655255572,\n",
       "               600.875657651471,\n",
       "               600.5507708729465,\n",
       "               600.2448364399872,\n",
       "               599.9115874064244,\n",
       "               599.6662000040305,\n",
       "               599.3664171237408,\n",
       "               599.0505045565782,\n",
       "               598.6700074132607,\n",
       "               598.3022664264705,\n",
       "               597.9783184616506,\n",
       "               597.6629937261895,\n",
       "               597.1772970542418,\n",
       "               596.7752145298925,\n",
       "               596.3677698588282,\n",
       "               596.021604579996,\n",
       "               595.7096749599524,\n",
       "               595.3814661326007,\n",
       "               595.015805718597,\n",
       "               594.5755744390059,\n",
       "               594.1781568481646,\n",
       "               593.7846932822407,\n",
       "               593.4035463835128,\n",
       "               592.991705430125,\n",
       "               592.5425218185579,\n",
       "               592.1104736057798,\n",
       "               591.8406953859686,\n",
       "               591.3842463131523,\n",
       "               590.8919629088332,\n",
       "               590.5004755408428,\n",
       "               590.1368919358817,\n",
       "               589.765574350941,\n",
       "               589.4156335648927,\n",
       "               589.1673192045449,\n",
       "               588.7466694778886,\n",
       "               588.3255620180813,\n",
       "               588.0623929464865,\n",
       "               587.7883610772085,\n",
       "               587.4932962069827,\n",
       "               587.1963038271483,\n",
       "               586.9146485822047,\n",
       "               586.6953058904058,\n",
       "               586.4647170747239,\n",
       "               586.1568343141527,\n",
       "               585.8850280685358,\n",
       "               585.6293478827456,\n",
       "               585.4409602825118,\n",
       "               585.090927057299,\n",
       "               584.8020874567906,\n",
       "               584.6636339708532,\n",
       "               584.5775903043412,\n",
       "               584.331158857272,\n",
       "               584.1829999698497,\n",
       "               583.9892502890302,\n",
       "               583.8162126895429,\n",
       "               583.4835591025281,\n",
       "               583.3538574293848,\n",
       "               583.0069728118771,\n",
       "               582.8628667779286,\n",
       "               582.6639937740504,\n",
       "               582.5610132066919,\n",
       "               582.3070246226154,\n",
       "               582.2487726738271,\n",
       "               582.0389959700991,\n",
       "               581.8448121297329,\n",
       "               581.7392274509014,\n",
       "               581.7047595830787,\n",
       "               581.5755378164498,\n",
       "               581.4961833843129,\n",
       "               581.3337849882238,\n",
       "               581.1789692587356,\n",
       "               581.053499052734,\n",
       "               580.9946489793529,\n",
       "               580.8340337868397,\n",
       "               580.6577702081217,\n",
       "               580.5402863089656,\n",
       "               580.4769934722353,\n",
       "               580.4390078245539,\n",
       "               580.3778994645487,\n",
       "               580.2579801904782,\n",
       "               580.1292947697036,\n",
       "               580.0439147833565,\n",
       "               579.9731734579144,\n",
       "               579.8909285835714,\n",
       "               579.7275257319129,\n",
       "               579.6496544924889,\n",
       "               579.5783398686913,\n",
       "               579.4833767394482,\n",
       "               579.4134754868244,\n",
       "               579.3385391727455,\n",
       "               579.2653035738034,\n",
       "               579.2527801922506,\n",
       "               579.1601605708222,\n",
       "               579.028105638012,\n",
       "               578.9324817742295,\n",
       "               578.8071518695735,\n",
       "               578.7369071436834,\n",
       "               578.7172651013609,\n",
       "               578.6686342997114,\n",
       "               578.5947288729622,\n",
       "               578.5330966479098,\n",
       "               578.4847483132592,\n",
       "               578.3867854269663,\n",
       "               578.3431068754637,\n",
       "               578.2928830278062,\n",
       "               578.2359695563038,\n",
       "               578.1267762143112,\n",
       "               577.9755191551707,\n",
       "               577.9356623147667,\n",
       "               577.8756103416047,\n",
       "               577.8247759056676,\n",
       "               577.8062958983625,\n",
       "               577.7547217133434,\n",
       "               577.738945401464,\n",
       "               577.6604857404806,\n",
       "               577.6263619815169,\n",
       "               577.5381535048429,\n",
       "               577.5140149869295,\n",
       "               577.354336470161,\n",
       "               577.3268543987476,\n",
       "               577.3115562404616,\n",
       "               577.2655303969266,\n",
       "               577.1864926232777,\n",
       "               577.1259569281348,\n",
       "               576.9985889933442,\n",
       "               576.941404045864,\n",
       "               576.88165388641,\n",
       "               576.846363164105,\n",
       "               576.7810097944521,\n",
       "               576.7357257183222,\n",
       "               576.6357508266432,\n",
       "               576.5676613671918,\n",
       "               576.4891841460258,\n",
       "               576.4336138167938,\n",
       "               576.3438221197445,\n",
       "               576.3141690142245,\n",
       "               576.2065882540364,\n",
       "               576.1467496855196,\n",
       "               576.0819205484522,\n",
       "               576.0460706752216,\n",
       "               575.9810533752795,\n",
       "               575.95255249771,\n",
       "               575.9275782617347,\n",
       "               575.8712407250252,\n",
       "               575.7870666334107,\n",
       "               575.7444111703726,\n",
       "               575.6932911011465,\n",
       "               575.6063383740384,\n",
       "               575.5665424198271,\n",
       "               575.5356011711026,\n",
       "               575.5179943520169,\n",
       "               575.4845713897099,\n",
       "               575.4434884401287,\n",
       "               575.3715816314295,\n",
       "               575.3527248869481,\n",
       "               575.3256845685534,\n",
       "               575.3246765352778,\n",
       "               575.2804053608963,\n",
       "               575.2475160277667,\n",
       "               575.1627357311161,\n",
       "               575.1577120121844,\n",
       "               575.1231176562703,\n",
       "               575.0711065155455,\n",
       "               574.987683328538,\n",
       "               574.9621405904642,\n",
       "               574.867748141042,\n",
       "               574.8426351364833,\n",
       "               574.787737758377,\n",
       "               574.707676436844,\n",
       "               574.6604554870853,\n",
       "               574.5969688261472,\n",
       "               574.5870043389474,\n",
       "               574.5717125089332,\n",
       "               574.5347522022466,\n",
       "               574.4690974758635,\n",
       "               574.4477663372851,\n",
       "               574.4158141966922,\n",
       "               574.2993426822376,\n",
       "               574.2905640016186,\n",
       "               574.2477770666729,\n",
       "               574.1480106302504,\n",
       "               574.0987389581176,\n",
       "               574.0542037807661,\n",
       "               574.0128128353066,\n",
       "               573.9656013419169,\n",
       "               573.9238736623288,\n",
       "               573.8350710500222,\n",
       "               573.7789729730199,\n",
       "               573.7547376079472,\n",
       "               573.6760217795488,\n",
       "               573.6170230395536,\n",
       "               573.6021332444195,\n",
       "               573.5323522222881,\n",
       "               573.5240046047384,\n",
       "               573.457427366944,\n",
       "               573.4193443282065,\n",
       "               573.3969458595918,\n",
       "               573.3361731222817,\n",
       "               573.3109870526887,\n",
       "               573.2828695127562,\n",
       "               573.2403927865397,\n",
       "               573.216145368709,\n",
       "               573.1636552582918,\n",
       "               573.1454663000325,\n",
       "               573.0649702751383,\n",
       "               573.0009119006288,\n",
       "               572.9758916365402,\n",
       "               572.934269120925,\n",
       "               572.8790018591474,\n",
       "               572.8446789070615,\n",
       "               572.8068289801935,\n",
       "               572.767594719376,\n",
       "               572.7707285637026,\n",
       "               572.6886035836835,\n",
       "               572.6420703732426,\n",
       "               572.6136356635199,\n",
       "               572.5912478360804,\n",
       "               572.559997480302,\n",
       "               572.535639514096,\n",
       "               572.3853313482253,\n",
       "               572.3595132629056,\n",
       "               572.318878918997,\n",
       "               572.2642993256328,\n",
       "               572.2114198586859,\n",
       "               572.1841596714446,\n",
       "               572.1691962518573,\n",
       "               572.1242286512434,\n",
       "               572.0484259382085,\n",
       "               572.0418211055375,\n",
       "               572.020010650492,\n",
       "               572.0047807826065,\n",
       "               571.9905783479169,\n",
       "               571.945182159757,\n",
       "               571.8850657884493,\n",
       "               571.8165249486849,\n",
       "               571.7793518688708,\n",
       "               571.7464433839074,\n",
       "               571.7559653480944,\n",
       "               571.7486116343744,\n",
       "               571.7129715749631,\n",
       "               571.6068146621111,\n",
       "               571.5836533146431,\n",
       "               571.5157779743693,\n",
       "               571.4741662578178,\n",
       "               571.4278383151204,\n",
       "               571.4273254752109,\n",
       "               571.3500562524172,\n",
       "               571.2898950576175,\n",
       "               571.2494441292507,\n",
       "               571.1932950201668,\n",
       "               571.1868395291053,\n",
       "               571.1463741509114,\n",
       "               571.1332816550615,\n",
       "               571.1027849809212,\n",
       "               571.0802609625558,\n",
       "               571.0266352288165,\n",
       "               571.0116504153394,\n",
       "               570.9398541807037,\n",
       "               570.8668401920388,\n",
       "               570.8322686179711,\n",
       "               570.7789574591955,\n",
       "               570.7690787396928,\n",
       "               570.7590280239974,\n",
       "               570.7257593908188,\n",
       "               570.736411007837,\n",
       "               570.6714916921642,\n",
       "               570.6690375920945,\n",
       "               570.6436898966283,\n",
       "               570.629448921464,\n",
       "               570.6169595991602,\n",
       "               570.6162061156643,\n",
       "               570.6106868896325,\n",
       "               570.573005339802,\n",
       "               570.5638620958418,\n",
       "               570.5401516130235,\n",
       "               570.5088766091503,\n",
       "               570.4938221681608,\n",
       "               570.3935249714602,\n",
       "               570.3471133213301,\n",
       "               570.3048383074721,\n",
       "               570.2919337372402,\n",
       "               570.2266816651053,\n",
       "               570.2016682207927,\n",
       "               570.1675401406073,\n",
       "               570.1202337511754,\n",
       "               570.0768934735504,\n",
       "               570.0607343065392,\n",
       "               569.9973533506092,\n",
       "               569.9279404466849,\n",
       "               569.8808168200678,\n",
       "               569.8219930624142,\n",
       "               569.8178730792531,\n",
       "               569.772517314121,\n",
       "               569.737939698704,\n",
       "               569.7255138785764,\n",
       "               569.7254607308797,\n",
       "               569.7620553279894,\n",
       "               569.6799994581332,\n",
       "               569.6435676513606,\n",
       "               569.6397462861571,\n",
       "               569.6320884334899,\n",
       "               569.6188892842856,\n",
       "               569.5946110760898,\n",
       "               569.5650799424412,\n",
       "               569.51096103959,\n",
       "               569.4937720791697,\n",
       "               569.4935008511171,\n",
       "               569.4709787382154,\n",
       "               569.442699006784,\n",
       "               569.4210872736326,\n",
       "               569.3793670210043,\n",
       "               569.3830437631747,\n",
       "               569.3340280179784,\n",
       "               569.3110503671885,\n",
       "               569.2635969928145,\n",
       "               569.2499814876554,\n",
       "               569.257976125677,\n",
       "               569.225498403647,\n",
       "               569.1933680124108,\n",
       "               569.1728588561227,\n",
       "               569.1332746050132,\n",
       "               569.0914069477644,\n",
       "               569.0519123114557,\n",
       "               568.9922617342226,\n",
       "               568.9516732952793,\n",
       "               568.938439342362,\n",
       "               568.9086301495267,\n",
       "               568.8750770807679,\n",
       "               568.8621247529874,\n",
       "               568.8594830286869,\n",
       "               568.8159333572637,\n",
       "               568.727567200866,\n",
       "               568.7155129846054,\n",
       "               568.6604022947022,\n",
       "               568.6862575409007,\n",
       "               568.6469839699478,\n",
       "               568.6385637469748,\n",
       "               568.6141046797682,\n",
       "               568.5910209170153,\n",
       "               568.5451129429092,\n",
       "               568.5116136780991,\n",
       "               568.4937568445766,\n",
       "               568.5098907785147,\n",
       "               568.4832662937581,\n",
       "               568.4407552559857,\n",
       "               568.3927722041597,\n",
       "               568.3815909287573,\n",
       "               568.3683913288197,\n",
       "               568.3616138082573,\n",
       "               568.3565062549843,\n",
       "               568.3293975890681,\n",
       "               568.2600050993129,\n",
       "               568.2491993392615,\n",
       "               568.1986326557505,\n",
       "               568.1976168360658,\n",
       "               568.1607288324362,\n",
       "               568.1493597145826,\n",
       "               568.0945481658072,\n",
       "               567.9967049390127,\n",
       "               567.9651281389373,\n",
       "               567.9571919518936,\n",
       "               567.9063492465227,\n",
       "               567.82929382304,\n",
       "               567.8229964082278,\n",
       "               567.7976410099815,\n",
       "               567.7969191113592,\n",
       "               567.7893649574482,\n",
       "               567.7782331488887,\n",
       "               567.7666158861514,\n",
       "               567.7295166178662,\n",
       "               567.692862451143,\n",
       "               567.6641406448445,\n",
       "               567.5545292559449,\n",
       "               567.5590007897341,\n",
       "               567.5231594001095,\n",
       "               567.52062710133,\n",
       "               567.510945795166,\n",
       "               567.4737507387383,\n",
       "               567.454900066685,\n",
       "               567.4509313931801,\n",
       "               567.4227017390562,\n",
       "               567.4082587729933,\n",
       "               567.3939966303353,\n",
       "               567.3915577276346,\n",
       "               567.3993448325151,\n",
       "               567.3655535494124,\n",
       "               567.3003920313556,\n",
       "               567.2548403427361,\n",
       "               567.253824245636,\n",
       "               567.2125157565868,\n",
       "               567.1594479379861,\n",
       "               567.1404257942798,\n",
       "               567.1229458639439,\n",
       "               567.0776525404992,\n",
       "               567.0524925337717,\n",
       "               567.0271876478879,\n",
       "               567.00216065366,\n",
       "               566.9954499642542,\n",
       "               566.9761039650746,\n",
       "               567.0013361525832,\n",
       "               566.9641136843544,\n",
       "               566.9394054466946,\n",
       "               566.9360931929588,\n",
       "               566.9276784355336,\n",
       "               566.9348882971316,\n",
       "               566.9237992757268,\n",
       "               566.9158890258198,\n",
       "               566.9079778570251,\n",
       "               566.878683564724,\n",
       "               566.8758718270398,\n",
       "               566.8750251918988,\n",
       "               566.8580475371384,\n",
       "               566.8384552912866,\n",
       "               566.7949596150379,\n",
       "               566.7313135571616,\n",
       "               566.6771271771788,\n",
       "               566.6508657038696,\n",
       "               566.649594573208,\n",
       "               566.6566591917546,\n",
       "               566.6533626592011,\n",
       "               566.64074002248,\n",
       "               566.5903437893973,\n",
       "               566.5726891418092,\n",
       "               566.530563110685,\n",
       "               566.4329815249695,\n",
       "               566.4160950617811,\n",
       "               566.3829048556333,\n",
       "               566.3856085835889,\n",
       "               566.3604091929786,\n",
       "               566.3465160001549,\n",
       "               566.350338821964,\n",
       "               566.3383294293283,\n",
       "               566.3366410280969,\n",
       "               566.3029671747865,\n",
       "               566.2918081961474,\n",
       "               566.2749740771674,\n",
       "               566.2867349482019,\n",
       "               566.2754816097961,\n",
       "               566.242221033278,\n",
       "               566.2168845606107,\n",
       "               566.2217182799146,\n",
       "               566.2121703952282,\n",
       "               566.1719521449224,\n",
       "               566.1591710140804,\n",
       "               566.1315369186945,\n",
       "               566.0892170104305,\n",
       "               566.0602604511566,\n",
       "               566.0575960111486,\n",
       "               566.0389101675745,\n",
       "               566.0407824512798,\n",
       "               566.0398512539762,\n",
       "               566.0170808371456,\n",
       "               565.987234549917,\n",
       "               565.9692774089274,\n",
       "               565.89787767329,\n",
       "               565.8755936816445,\n",
       "               565.8223936309047,\n",
       "               565.8084400644699,\n",
       "               565.7887513365916,\n",
       "               565.7976302473331,\n",
       "               565.7664516326729,\n",
       "               565.7050553063308,\n",
       "               565.7053350479194,\n",
       "               565.7098426274064,\n",
       "               565.7100004285952,\n",
       "               565.7015340208243,\n",
       "               565.6862542599343,\n",
       "               565.6646791289305,\n",
       "               565.6627146325776,\n",
       "               565.6358293209087,\n",
       "               565.5979750554244,\n",
       "               565.5924369618093,\n",
       "               565.5639013429078,\n",
       "               565.5508291979448,\n",
       "               565.5382481464026,\n",
       "               565.5146893843647,\n",
       "               565.5154414645922,\n",
       "               565.4811126911961,\n",
       "               565.481419738817,\n",
       "               565.4776486977585,\n",
       "               565.461538909636,\n",
       "               565.4272235632036,\n",
       "               565.4166635617574,\n",
       "               565.4233170089722,\n",
       "               565.392781215072,\n",
       "               565.3567652708239,\n",
       "               565.3152777930102,\n",
       "               565.3008835813754,\n",
       "               565.2693456398557,\n",
       "               565.232984424368,\n",
       "               565.228676869188,\n",
       "               565.2002217633316,\n",
       "               565.1989926870717,\n",
       "               565.1721767100488,\n",
       "               565.1349467976819,\n",
       "               565.1171315181443,\n",
       "               565.0878732375451,\n",
       "               565.0624600273628,\n",
       "               565.040356387599,\n",
       "               565.027978017081,\n",
       "               565.0266929625511,\n",
       "               565.0488527165382,\n",
       "               565.0284691437922,\n",
       "               565.0644808450097,\n",
       "               565.0110915191016,\n",
       "               565.0135846530636,\n",
       "               564.9880667629756,\n",
       "               564.9803975965863,\n",
       "               564.9739619182034,\n",
       "               564.9815124911087,\n",
       "               564.9549038180004,\n",
       "               564.938516859187,\n",
       "               564.9545988352157,\n",
       "               564.9103422856977,\n",
       "               564.8285626956774,\n",
       "               564.8281190687468,\n",
       "               564.7863203897272,\n",
       "               564.7653520369086,\n",
       "               564.7370999255398,\n",
       "               564.7364704314152,\n",
       "               564.6735451260938,\n",
       "               564.6562291209133,\n",
       "               564.6528633446469,\n",
       "               564.6421209615665,\n",
       "               564.618874593669,\n",
       "               564.6362964313417,\n",
       "               564.6427311885446,\n",
       "               564.6507655074287,\n",
       "               564.6127182271714,\n",
       "               564.5980209511999,\n",
       "               564.5693772647305,\n",
       "               564.5561694215005,\n",
       "               564.5485203517958,\n",
       "               564.5372886751477,\n",
       "               564.5387734501597,\n",
       "               564.4969526477149,\n",
       "               564.5114723879677,\n",
       "               564.4898863132355,\n",
       "               564.4885392486674,\n",
       "               564.4862691886732,\n",
       "               564.4446665105354,\n",
       "               564.4180350801549,\n",
       "               564.389675056838,\n",
       "               564.3596242711874,\n",
       "               564.3084074910992,\n",
       "               564.2576047137042,\n",
       "               564.2410474012577,\n",
       "               564.2075175673398,\n",
       "               564.2048981866385,\n",
       "               564.1892576623129,\n",
       "               564.1824038185105,\n",
       "               564.1849951083898,\n",
       "               564.1672529842131,\n",
       "               564.1100001961956,\n",
       "               564.0607176577169,\n",
       "               564.0456763457466,\n",
       "               564.0216555782924,\n",
       "               563.9733076710979,\n",
       "               563.9578189406095,\n",
       "               563.9503644615102,\n",
       "               563.9340906809858,\n",
       "               563.8909630694513,\n",
       "               563.8255318734009,\n",
       "               563.804492570657,\n",
       "               563.7680111572978,\n",
       "               563.756119985273,\n",
       "               563.7435894102202,\n",
       "               563.7133591892631,\n",
       "               563.6561799511886,\n",
       "               563.6586225989089,\n",
       "               563.6632739510092,\n",
       "               563.6238582697345,\n",
       "               563.6105981731168,\n",
       "               563.5815721663796,\n",
       "               563.556796775889,\n",
       "               563.5466856670531,\n",
       "               563.5206985164361,\n",
       "               563.4741216455152,\n",
       "               563.4533509259105,\n",
       "               563.4550531215584,\n",
       "               563.4585761490669,\n",
       "               563.4924052859025,\n",
       "               563.4823745476316,\n",
       "               563.4541740188647,\n",
       "               563.4207609219636,\n",
       "               563.4295396277237,\n",
       "               563.4214706948596,\n",
       "               563.3833516891173,\n",
       "               563.3836714664936,\n",
       "               563.372177575216,\n",
       "               563.3815051913168,\n",
       "               563.3807624967897,\n",
       "               563.3562159833444,\n",
       "               563.3425217602885,\n",
       "               563.345518458191,\n",
       "               563.3204132962966,\n",
       "               563.2859927021373,\n",
       "               563.2678468859896,\n",
       "               563.2532049505485,\n",
       "               563.2179717142327,\n",
       "               563.190923228443,\n",
       "               563.1632982072176,\n",
       "               563.133128584591,\n",
       "               563.1358560493134,\n",
       "               563.0740113159752,\n",
       "               563.0380557801475,\n",
       "               563.0346304349607,\n",
       "               563.0120304851686,\n",
       "               562.986831277863,\n",
       "               562.9889970855618,\n",
       "               562.9936951328393,\n",
       "               562.9934512362813,\n",
       "               563.0064741680759,\n",
       "               562.9732705877379,\n",
       "               562.9362618791322,\n",
       "               562.9106596605529,\n",
       "               562.9160356884664,\n",
       "               562.9045609574775,\n",
       "               562.9076283030352,\n",
       "               562.8827130521242,\n",
       "               562.872912727743,\n",
       "               562.8707220879539,\n",
       "               562.8825455547925,\n",
       "               562.8916471247082,\n",
       "               562.9060096001303,\n",
       "               562.9378899987184,\n",
       "               562.9483943301759,\n",
       "               562.9461169416967,\n",
       "               562.923754410456,\n",
       "               562.8855853298982,\n",
       "               562.9019411803926,\n",
       "               562.8768827069373,\n",
       "               562.8040055861838,\n",
       "               562.7921700330797,\n",
       "               562.7512284688438,\n",
       "               562.7498458836944,\n",
       "               562.7319127652596,\n",
       "               562.7349732045457,\n",
       "               562.7211699096905,\n",
       "               562.6932515778483,\n",
       "               562.6861339880102,\n",
       "               562.6387047116177,\n",
       "               562.6324095791636,\n",
       "               562.614672479157,\n",
       "               562.5812465540619,\n",
       "               562.5653200050111,\n",
       "               562.5492580971033,\n",
       "               562.5260819001909,\n",
       "               562.5384221779939,\n",
       "               562.5503202439727,\n",
       "               562.533207360383,\n",
       "               562.5418538543915,\n",
       "               562.5181743764558,\n",
       "               562.5094179375104,\n",
       "               562.4999779455571,\n",
       "               562.4900335741651,\n",
       "               562.4566477860437,\n",
       "               562.4448099283635,\n",
       "               562.4034823883896,\n",
       "               562.3796377310331,\n",
       "               562.36434251411,\n",
       "               562.3407835261062,\n",
       "               562.3446222252769,\n",
       "               562.3318680838366,\n",
       "               562.3422435201591,\n",
       "               562.3187213398244,\n",
       "               562.3193825026149,\n",
       "               562.3043934561967,\n",
       "               562.2934698646387,\n",
       "               562.2793912322708,\n",
       "               562.2703639811058,\n",
       "               562.2650302801682,\n",
       "               562.2658430076762,\n",
       "               562.2316603751212,\n",
       "               562.2252394556928,\n",
       "               562.2016563096345,\n",
       "               562.2186749182868,\n",
       "               562.1873877431475,\n",
       "               562.1634790796099,\n",
       "               562.1102380816403,\n",
       "               562.1247567815409,\n",
       "               562.1330527430745,\n",
       "               562.1638207689718,\n",
       "               562.1609061728352,\n",
       "               562.1422107520295,\n",
       "               562.1469743926609,\n",
       "               562.132021969365,\n",
       "               562.1284922083377,\n",
       "               562.128988683945,\n",
       "               562.1188002832116,\n",
       "               562.1092496334984,\n",
       "               562.1031071562883,\n",
       "               562.0811172768841,\n",
       "               562.0750354372722,\n",
       "               562.0807893050531,\n",
       "               562.0694553620755,\n",
       "               562.0653175394363,\n",
       "               562.0467266310453,\n",
       "               562.0393200555006,\n",
       "               562.0150155499289,\n",
       "               562.0042805192376,\n",
       "               561.9951715679723,\n",
       "               561.9832081592763,\n",
       "               561.9662788505419,\n",
       "               561.947838901386,\n",
       "               561.9280504236108,\n",
       "               561.921972868489,\n",
       "               561.9154928286196,\n",
       "               561.9143266486636,\n",
       "               561.923462340088,\n",
       "               561.8855406902193,\n",
       "               561.8805269685071,\n",
       "               561.8708839607164,\n",
       "               561.845375723412,\n",
       "               561.8203829639203,\n",
       "               561.7883764333395,\n",
       "               561.8022027739768,\n",
       "               561.7398612330724,\n",
       "               561.7516584025523,\n",
       "               561.7316984183567,\n",
       "               561.7091214864959,\n",
       "               561.6890760347362,\n",
       "               561.698664056833,\n",
       "               561.6497555977837,\n",
       "               561.6342014288339,\n",
       "               561.6416873243071,\n",
       "               561.5926702183415,\n",
       "               561.5929111510591,\n",
       "               561.6242243316937,\n",
       "               561.6052180557674,\n",
       "               561.6034898932616,\n",
       "               561.5930258248427,\n",
       "               561.5824784625302,\n",
       "               561.5514868642055,\n",
       "               561.5437939261838,\n",
       "               561.5360492288121,\n",
       "               561.5430128012956,\n",
       "               561.5096843593043,\n",
       "               561.5055587779552,\n",
       "               561.4743881835219,\n",
       "               561.4620065891465,\n",
       "               561.4632672836788,\n",
       "               561.4323961817206,\n",
       "               561.4124806731168,\n",
       "               561.4090369568423,\n",
       "               561.3908806079951,\n",
       "               561.3979605224646,\n",
       "               561.3908779652726,\n",
       "               561.3871999240421,\n",
       "               561.3571340667261,\n",
       "               561.3502295152533,\n",
       "               561.3576392403355,\n",
       "               561.3236644847732,\n",
       "               561.3050977701939,\n",
       "               561.3011092922836,\n",
       "               561.2905682174977,\n",
       "               561.2865649184625,\n",
       "               561.2692594046115,\n",
       "               561.2628494145417,\n",
       "               561.2068060242052,\n",
       "               561.2041654861241,\n",
       "               561.1810622810272,\n",
       "               561.1813034102568,\n",
       "               561.1776875273898,\n",
       "               561.1506000003744,\n",
       "               561.1275549753766,\n",
       "               561.1107208172051,\n",
       "               561.0697784725269,\n",
       "               561.0518830626063,\n",
       "               561.0352050481637,\n",
       "               561.0155521345263,\n",
       "               561.0322075088962,\n",
       "               561.0262008006289,\n",
       "               560.9971765902822,\n",
       "               560.9710052856306,\n",
       "               560.9489271661625,\n",
       "               560.9440410086675,\n",
       "               560.9524284632216,\n",
       "               560.9298520240592,\n",
       "               560.934485263325,\n",
       "               560.9015548185804,\n",
       "               560.8862771024358,\n",
       "               560.8667199557161,\n",
       "               560.8497475861,\n",
       "               560.8450749075462,\n",
       "               560.8063642153796,\n",
       "               560.8088422485031,\n",
       "               560.8244854928737,\n",
       "               560.8012198842141,\n",
       "               560.770161029822,\n",
       "               560.7447000274396,\n",
       "               560.7370631553401,\n",
       "               560.7398142820763,\n",
       "               560.7662835623745,\n",
       "               560.749521237057,\n",
       "               560.7371355308866,\n",
       "               560.7384072177435,\n",
       "               560.771251667766,\n",
       "               560.7352866056383,\n",
       "               560.7223057054576,\n",
       "               560.6763373275119,\n",
       "               560.6732948579847,\n",
       "               560.6607832149518,\n",
       "               560.6639121624222,\n",
       "               560.6541636829277,\n",
       "               560.60360565418,\n",
       "               560.5946406718679,\n",
       "               560.5861490882035,\n",
       "               560.5542289762369,\n",
       "               560.5498930822855,\n",
       "               560.5342658131689,\n",
       "               560.5141402175991,\n",
       "               560.5048434335815,\n",
       "               560.4771118031338,\n",
       "               560.5081176613404,\n",
       "               560.4959375808255,\n",
       "               560.4881316058561,\n",
       "               560.4771123971299,\n",
       "               560.487089389732,\n",
       "               560.4683313539912,\n",
       "               560.4770686370474,\n",
       "               560.4804053925889,\n",
       "               560.4772735604314,\n",
       "               560.45172504331,\n",
       "               560.448186286943,\n",
       "               560.4394513307309,\n",
       "               560.4242503640559,\n",
       "               560.4065674220765,\n",
       "               560.4002669625525,\n",
       "               560.3938758282478,\n",
       "               560.374280174186,\n",
       "               560.366205601954,\n",
       "               560.3531521446382,\n",
       "               560.3531606468039,\n",
       "               560.3606436429241,\n",
       "               560.2890010530798,\n",
       "               560.2747043285837,\n",
       "               560.2919000888687,\n",
       "               560.3033196074011,\n",
       "               560.3333363095492,\n",
       "               560.3238693435114,\n",
       "               560.3414987470692,\n",
       "               560.3430332466639,\n",
       "               560.3384818639003,\n",
       "               560.3327654640514,\n",
       "               560.3216892449639,\n",
       "               560.3268837725751,\n",
       "               560.3051965877669,\n",
       "               560.2910563319684,\n",
       "               560.2828703537364,\n",
       "               560.2768778581503,\n",
       "               560.2732806639788,\n",
       "               560.2779737525202,\n",
       "               560.27674655739,\n",
       "               560.267683723543,\n",
       "               560.2733782624115,\n",
       "               560.2411350294063,\n",
       "               560.2289801213216,\n",
       "               560.2174839835969,\n",
       "               560.1997027648061,\n",
       "               560.1932655760536,\n",
       "               560.2108099014852,\n",
       "               560.2073460925347,\n",
       "               560.202400975884,\n",
       "               560.1935117175713,\n",
       "               560.1688335851577,\n",
       "               560.1485098858335,\n",
       "               560.1557768894402,\n",
       "               560.1517638431643,\n",
       "               560.135110768806,\n",
       "               560.152813898126,\n",
       "               560.1153394923848,\n",
       "               560.1027613198942,\n",
       "               560.1392966957666,\n",
       "               560.1167156010586,\n",
       "               560.1392523243065,\n",
       "               560.1386979351698,\n",
       "               560.1410631644126,\n",
       "               560.1319469254015,\n",
       "               560.1386804932052,\n",
       "               560.1274410296879,\n",
       "               560.1197856953337,\n",
       "               560.0887568284124,\n",
       "               560.0838254603485,\n",
       "               560.084228619912,\n",
       "               560.0628495365196,\n",
       "               560.0508280893664,\n",
       "               560.0327127839445,\n",
       "               560.0403455760118,\n",
       "               560.028141141309,\n",
       "               560.0452627437418,\n",
       "               560.0294181668365,\n",
       "               560.0251184632754,\n",
       "               560.0141074597409,\n",
       "               560.0118368884707,\n",
       "               560.015241801196,\n",
       "               560.0067198215736,\n",
       "               559.9974772532553,\n",
       "               559.9845533601326],\n",
       "              'test-RMSE-std': [124.84854165949133,\n",
       "               125.80095811964412,\n",
       "               126.83702291237961,\n",
       "               127.92645323484814,\n",
       "               128.83197871773447,\n",
       "               129.60946527579887,\n",
       "               130.24697936476892,\n",
       "               130.82816103508242,\n",
       "               131.5267063244995,\n",
       "               131.9654693689604,\n",
       "               132.5372269775732,\n",
       "               133.19535354000976,\n",
       "               133.71625370690384,\n",
       "               134.40635996487592,\n",
       "               134.82975865715096,\n",
       "               135.65708000224274,\n",
       "               136.30275611619183,\n",
       "               136.91255351798944,\n",
       "               137.6302105087908,\n",
       "               138.33296031699285,\n",
       "               138.97607527566558,\n",
       "               139.53645101254378,\n",
       "               140.08368122223087,\n",
       "               140.57423134605202,\n",
       "               140.9553982823442,\n",
       "               141.6041431145393,\n",
       "               142.19689164270343,\n",
       "               142.756241500836,\n",
       "               143.11863661735612,\n",
       "               143.68086221922422,\n",
       "               143.9701068869304,\n",
       "               144.44466647564423,\n",
       "               144.82419100056958,\n",
       "               145.340967300568,\n",
       "               145.64881880692442,\n",
       "               145.9606782184915,\n",
       "               146.32344599612588,\n",
       "               146.5197825457338,\n",
       "               146.80380000190584,\n",
       "               147.20442837719852,\n",
       "               147.43165339291753,\n",
       "               147.6397716814362,\n",
       "               147.8168860296049,\n",
       "               148.06179512244267,\n",
       "               148.32131860697615,\n",
       "               148.52431246520325,\n",
       "               148.81912269435938,\n",
       "               148.97117914963158,\n",
       "               149.1396455172147,\n",
       "               149.29923765572562,\n",
       "               149.3783239433431,\n",
       "               149.48545045368434,\n",
       "               149.54926531672317,\n",
       "               149.6246350881279,\n",
       "               149.79262611235183,\n",
       "               150.04492914134258,\n",
       "               150.15791110703725,\n",
       "               150.22423732262533,\n",
       "               150.4035839161679,\n",
       "               150.40519611186824,\n",
       "               150.41894036706,\n",
       "               150.57572349575017,\n",
       "               150.65773727952305,\n",
       "               150.67195777185836,\n",
       "               150.75884403064202,\n",
       "               150.80016417356174,\n",
       "               150.9458502485344,\n",
       "               151.04766314164155,\n",
       "               151.19517277208692,\n",
       "               151.1610602574322,\n",
       "               151.1602524983499,\n",
       "               151.19038333730816,\n",
       "               151.1130499590279,\n",
       "               151.1337120441669,\n",
       "               151.2674448323248,\n",
       "               151.27382455675885,\n",
       "               151.2426677354463,\n",
       "               151.2721976399283,\n",
       "               151.29691095195057,\n",
       "               151.2065988227369,\n",
       "               151.26424968334055,\n",
       "               151.23847214469873,\n",
       "               151.32047167680147,\n",
       "               151.22528520623575,\n",
       "               151.1722959020495,\n",
       "               151.14543243146247,\n",
       "               151.2810203625983,\n",
       "               151.35868757846862,\n",
       "               151.25980154123113,\n",
       "               151.19663525109456,\n",
       "               151.2777333459655,\n",
       "               151.2837621961534,\n",
       "               151.28092950569624,\n",
       "               151.3127443026438,\n",
       "               151.23461596289732,\n",
       "               151.16833336349183,\n",
       "               151.13516752815437,\n",
       "               151.2091755848077,\n",
       "               151.20180383863558,\n",
       "               151.07645071219204,\n",
       "               151.06358689507204,\n",
       "               150.974362987415,\n",
       "               150.8563467590687,\n",
       "               150.8494725279091,\n",
       "               150.83333329358186,\n",
       "               150.93159242025186,\n",
       "               150.89452425913942,\n",
       "               150.83748233508095,\n",
       "               150.82446692296114,\n",
       "               150.8070871979381,\n",
       "               150.61979904993504,\n",
       "               150.49783762634965,\n",
       "               150.36843957622943,\n",
       "               150.36502237677297,\n",
       "               150.3392337060461,\n",
       "               150.27099091702337,\n",
       "               150.29813898270095,\n",
       "               150.3290202701869,\n",
       "               150.22058481351235,\n",
       "               150.18784463909506,\n",
       "               150.25079151036002,\n",
       "               150.22431083183236,\n",
       "               150.2462125975096,\n",
       "               150.25771989288984,\n",
       "               150.342715148845,\n",
       "               150.41889397528095,\n",
       "               150.4945974681643,\n",
       "               150.53839669522054,\n",
       "               150.568351488202,\n",
       "               150.59505732382237,\n",
       "               150.57549742667902,\n",
       "               150.53100374207324,\n",
       "               150.58958417544937,\n",
       "               150.57731620321968,\n",
       "               150.59576397513905,\n",
       "               150.69800383807333,\n",
       "               150.7597125523623,\n",
       "               150.72717251947415,\n",
       "               150.67642552573648,\n",
       "               150.76724680865888,\n",
       "               150.7870909143218,\n",
       "               150.73360735622862,\n",
       "               150.75993499498037,\n",
       "               150.7311154032674,\n",
       "               150.7019685061848,\n",
       "               150.8362675110024,\n",
       "               150.86393843964132,\n",
       "               150.85819989114256,\n",
       "               150.75599403892758,\n",
       "               150.7516605248806,\n",
       "               150.7728371514365,\n",
       "               150.7576391525268,\n",
       "               150.69373110218362,\n",
       "               150.76439092316522,\n",
       "               150.85047196114226,\n",
       "               150.8835678292588,\n",
       "               150.88284421120804,\n",
       "               150.85558311355445,\n",
       "               150.8377187585083,\n",
       "               150.83157407063945,\n",
       "               150.89161269510086,\n",
       "               150.85837123232474,\n",
       "               150.76913041759525,\n",
       "               150.73826173384523,\n",
       "               150.82265616380022,\n",
       "               150.84798092087482,\n",
       "               150.89035056787284,\n",
       "               150.87337232433708,\n",
       "               150.86034961867756,\n",
       "               150.850197606019,\n",
       "               150.91741233419313,\n",
       "               150.9003474413437,\n",
       "               150.92225336197933,\n",
       "               150.9147157226858,\n",
       "               150.85683217348554,\n",
       "               150.9624225914849,\n",
       "               150.91294294191795,\n",
       "               150.93013834701082,\n",
       "               150.96376086423913,\n",
       "               150.95441563214797,\n",
       "               150.94380409260384,\n",
       "               150.98312209554294,\n",
       "               151.0297826492503,\n",
       "               151.0151753501681,\n",
       "               151.02507867664963,\n",
       "               150.89417188450628,\n",
       "               150.9121966771227,\n",
       "               150.9106469930586,\n",
       "               150.91425450693538,\n",
       "               150.89114085750083,\n",
       "               150.89826803330004,\n",
       "               150.80186425402655,\n",
       "               150.79460868895245,\n",
       "               150.83114360221595,\n",
       "               150.8477500216414,\n",
       "               150.8501960748378,\n",
       "               150.91208681268952,\n",
       "               150.8911523483038,\n",
       "               150.8230047326854,\n",
       "               150.8415065604744,\n",
       "               150.84232492180945,\n",
       "               150.84706670290265,\n",
       "               150.81333174959477,\n",
       "               150.8938061090534,\n",
       "               150.91187724689715,\n",
       "               150.90239545859436,\n",
       "               150.91067935524535,\n",
       "               150.91736723723648,\n",
       "               150.90993752237594,\n",
       "               150.88254551691801,\n",
       "               150.90441449995456,\n",
       "               150.9159313594549,\n",
       "               150.89466227132945,\n",
       "               150.88418648622266,\n",
       "               150.89125777339453,\n",
       "               150.8822792651143,\n",
       "               150.82527351064226,\n",
       "               150.8235371472123,\n",
       "               150.84690238500832,\n",
       "               150.86091623521807,\n",
       "               150.81546497888866,\n",
       "               150.82673439702864,\n",
       "               150.80398637588033,\n",
       "               150.81943905190687,\n",
       "               150.81797037832652,\n",
       "               150.77033013470785,\n",
       "               150.79200412390918,\n",
       "               150.81095403183983,\n",
       "               150.80240764490065,\n",
       "               150.74299381324465,\n",
       "               150.74821846961555,\n",
       "               150.79510159155336,\n",
       "               150.8486390745105,\n",
       "               150.8590189728156,\n",
       "               150.86873731113522,\n",
       "               150.8560782533722,\n",
       "               150.85943040161638,\n",
       "               150.918155012399,\n",
       "               150.89815148574294,\n",
       "               150.90902439449425,\n",
       "               150.89725676679532,\n",
       "               150.92448944919786,\n",
       "               150.95356486164937,\n",
       "               150.95172531399297,\n",
       "               150.9248521760037,\n",
       "               150.94270851155318,\n",
       "               150.94727873034304,\n",
       "               150.93519471632965,\n",
       "               150.944879300386,\n",
       "               150.87545377579067,\n",
       "               150.8770686135826,\n",
       "               150.89021655454815,\n",
       "               150.91212198193944,\n",
       "               150.8726551248246,\n",
       "               150.8381067398754,\n",
       "               150.84920057238912,\n",
       "               150.79434545414995,\n",
       "               150.76990572139562,\n",
       "               150.8186719819467,\n",
       "               150.81838894256174,\n",
       "               150.79987924981822,\n",
       "               150.8326992932389,\n",
       "               150.8229431329516,\n",
       "               150.80789883513518,\n",
       "               150.84729346749876,\n",
       "               150.84456630874837,\n",
       "               150.8182144106664,\n",
       "               150.84169713737583,\n",
       "               150.8458975634697,\n",
       "               150.7803245075302,\n",
       "               150.7490636215376,\n",
       "               150.7533723997556,\n",
       "               150.75151514167018,\n",
       "               150.76154485300725,\n",
       "               150.79009011767118,\n",
       "               150.79604095124137,\n",
       "               150.78660300099352,\n",
       "               150.80927805745026,\n",
       "               150.88281625747257,\n",
       "               150.89789203559005,\n",
       "               150.93954882353785,\n",
       "               150.99900697940953,\n",
       "               151.00192317146983,\n",
       "               151.0147857835803,\n",
       "               150.92094503359397,\n",
       "               150.89332462774271,\n",
       "               150.79865370015372,\n",
       "               150.7412089809522,\n",
       "               150.72900499844096,\n",
       "               150.7408604296933,\n",
       "               150.7237878385364,\n",
       "               150.70382170945584,\n",
       "               150.69338641719182,\n",
       "               150.71832347363423,\n",
       "               150.718862529087,\n",
       "               150.72480188239345,\n",
       "               150.74636226304366,\n",
       "               150.73336566611133,\n",
       "               150.73493912432957,\n",
       "               150.7712418067713,\n",
       "               150.78821086520628,\n",
       "               150.81918647485574,\n",
       "               150.81820253081864,\n",
       "               150.83046064358317,\n",
       "               150.83565477136568,\n",
       "               150.8810541170456,\n",
       "               150.86814640248116,\n",
       "               150.81421384223933,\n",
       "               150.79335893977154,\n",
       "               150.83102586737374,\n",
       "               150.85450050326187,\n",
       "               150.83129046470924,\n",
       "               150.80666916514573,\n",
       "               150.80569875730885,\n",
       "               150.84019760705263,\n",
       "               150.8279704124564,\n",
       "               150.81975972771366,\n",
       "               150.80456072499172,\n",
       "               150.78736230896172,\n",
       "               150.76825711402697,\n",
       "               150.83688963671372,\n",
       "               150.80619125792796,\n",
       "               150.82058851429105,\n",
       "               150.81864010017242,\n",
       "               150.84615894863214,\n",
       "               150.85243474173032,\n",
       "               150.85045108105652,\n",
       "               150.8744988942732,\n",
       "               150.80091795152427,\n",
       "               150.82138847895874,\n",
       "               150.81408318199692,\n",
       "               150.82739590191824,\n",
       "               150.82199881278035,\n",
       "               150.76914037083213,\n",
       "               150.69089003407504,\n",
       "               150.72074365818986,\n",
       "               150.73678555944113,\n",
       "               150.7115222209836,\n",
       "               150.7028481139022,\n",
       "               150.6560780794361,\n",
       "               150.67347239952335,\n",
       "               150.6555476251237,\n",
       "               150.64559974219594,\n",
       "               150.66885847349113,\n",
       "               150.66880611258136,\n",
       "               150.6817015613271,\n",
       "               150.67672451026726,\n",
       "               150.65950522705083,\n",
       "               150.69286485683477,\n",
       "               150.68747512130278,\n",
       "               150.7244998194876,\n",
       "               150.75516134282782,\n",
       "               150.75284277832213,\n",
       "               150.7760200140276,\n",
       "               150.74779407383284,\n",
       "               150.74704559737953,\n",
       "               150.765195846812,\n",
       "               150.7763484566167,\n",
       "               150.75045562001952,\n",
       "               150.74413447274546,\n",
       "               150.75420301580976,\n",
       "               150.74321657961121,\n",
       "               150.77353383407015,\n",
       "               150.77606572195575,\n",
       "               150.77930120977896,\n",
       "               150.8052075854592,\n",
       "               150.80630766489642,\n",
       "               150.7961874968055,\n",
       "               150.81373989875667,\n",
       "               150.81693007472134,\n",
       "               150.8234465226578,\n",
       "               150.81581958276556,\n",
       "               150.81569507370259,\n",
       "               150.83276562243873,\n",
       "               150.82428142108066,\n",
       "               150.827263310822,\n",
       "               150.82164507388256,\n",
       "               150.85951874843124,\n",
       "               150.81076344648426,\n",
       "               150.76972786335898,\n",
       "               150.80459050298307,\n",
       "               150.8187223110689,\n",
       "               150.78451168926406,\n",
       "               150.7886313390867,\n",
       "               150.7659417115204,\n",
       "               150.81905528718045,\n",
       "               150.8339939813445,\n",
       "               150.8275904385902,\n",
       "               150.80959637530444,\n",
       "               150.8215931934633,\n",
       "               150.86288059719172,\n",
       "               150.8663039691605,\n",
       "               150.83597165301606,\n",
       "               150.85496090473194,\n",
       "               150.84521531816114,\n",
       "               150.82157012220395,\n",
       "               150.76847750537124,\n",
       "               150.7582528050983,\n",
       "               150.69339889207362,\n",
       "               150.67973039858433,\n",
       "               150.66288743788786,\n",
       "               150.66932920345164,\n",
       "               150.68400134576822,\n",
       "               150.66043370900107,\n",
       "               150.64849218361664,\n",
       "               150.67210307300513,\n",
       "               150.6528956693084,\n",
       "               150.65360200182806,\n",
       "               150.6595755810639,\n",
       "               150.66761299316937,\n",
       "               150.67244685400482,\n",
       "               150.71186411521484,\n",
       "               150.70448549725268,\n",
       "               150.70452387455845,\n",
       "               150.6931090182682,\n",
       "               150.6985704647346,\n",
       "               150.63383207786777,\n",
       "               150.6046543494671,\n",
       "               150.61358436138758,\n",
       "               150.59491597351618,\n",
       "               150.57743799856462,\n",
       "               150.5934608158162,\n",
       "               150.5714894815618,\n",
       "               150.54762152476434,\n",
       "               150.5164999650607,\n",
       "               150.54490388984732,\n",
       "               150.53692098025255,\n",
       "               150.46271395553015,\n",
       "               150.4673909262438,\n",
       "               150.4724278607985,\n",
       "               150.46334916166424,\n",
       "               150.4634035498759,\n",
       "               150.4210884890285,\n",
       "               150.43678817162504,\n",
       "               150.43377082703876,\n",
       "               150.40849658777458,\n",
       "               150.42617858456984,\n",
       "               150.4284219071429,\n",
       "               150.46414134293906,\n",
       "               150.47886218945544,\n",
       "               150.48875322501306,\n",
       "               150.47005030553112,\n",
       "               150.48227238141757,\n",
       "               150.47794320654484,\n",
       "               150.4372605286616,\n",
       "               150.4580543727767,\n",
       "               150.4952555841473,\n",
       "               150.50135959167616,\n",
       "               150.50191073357695,\n",
       "               150.46961800157626,\n",
       "               150.44192950817296,\n",
       "               150.44221151907834,\n",
       "               150.44795487768783,\n",
       "               150.44768671472187,\n",
       "               150.47622837179932,\n",
       "               150.47208616734645,\n",
       "               150.4822492171145,\n",
       "               150.42894559897022,\n",
       "               150.41493121917583,\n",
       "               150.3599574348403,\n",
       "               150.36413983589281,\n",
       "               150.36813319062017,\n",
       "               150.32495364666178,\n",
       "               150.32690889701004,\n",
       "               150.3146398133671,\n",
       "               150.31312836425275,\n",
       "               150.28323635032154,\n",
       "               150.28005085746,\n",
       "               150.26229198496182,\n",
       "               150.26295153896808,\n",
       "               150.2820204830812,\n",
       "               150.23373334990063,\n",
       "               150.1518493017262,\n",
       "               150.13316966936216,\n",
       "               150.14348947859733,\n",
       "               150.18736524438296,\n",
       "               150.15453103078505,\n",
       "               150.16482411329753,\n",
       "               150.14527187008503,\n",
       "               150.17139797329057,\n",
       "               150.20107987168825,\n",
       "               150.23375041274144,\n",
       "               150.2460886817245,\n",
       "               150.25593100229435,\n",
       "               150.25809417650558,\n",
       "               150.2593605038215,\n",
       "               150.27801539407926,\n",
       "               150.28112594149607,\n",
       "               150.2806102879113,\n",
       "               150.28185957146164,\n",
       "               150.26093938833284,\n",
       "               150.26999527605935,\n",
       "               150.26072930076248,\n",
       "               150.23348014975207,\n",
       "               150.16728083552167,\n",
       "               150.1666292853312,\n",
       "               150.1718601892928,\n",
       "               150.17068379582577,\n",
       "               150.1654096695596,\n",
       "               150.15940277046644,\n",
       "               150.13498261296488,\n",
       "               150.11970631688806,\n",
       "               150.13596548840357,\n",
       "               150.1357827519631,\n",
       "               150.11634090036475,\n",
       "               150.1096139736881,\n",
       "               150.1120586327673,\n",
       "               150.12337580358502,\n",
       "               150.11734475321285,\n",
       "               150.1413338018688,\n",
       "               150.1180686527509,\n",
       "               150.1063120097453,\n",
       "               150.10023541939432,\n",
       "               150.10105367812795,\n",
       "               150.05586675893946,\n",
       "               150.03502626993938,\n",
       "               150.07104204650815,\n",
       "               150.11194816126388,\n",
       "               150.08096639583462,\n",
       "               150.07269245246908,\n",
       "               150.06542221134742,\n",
       "               150.0668801746731,\n",
       "               150.07379443102116,\n",
       "               150.0537113050816,\n",
       "               150.05672629546916,\n",
       "               150.0934979136607,\n",
       "               150.1173573354968,\n",
       "               150.13869028343845,\n",
       "               150.10597946349475,\n",
       "               150.09993757407995,\n",
       "               150.12372699392935,\n",
       "               150.1328606646502,\n",
       "               150.1332053718043,\n",
       "               150.14261898663227,\n",
       "               150.12267135689007,\n",
       "               150.11791830854503,\n",
       "               150.07461997866375,\n",
       "               150.05064080090983,\n",
       "               150.06119545163605,\n",
       "               150.08052172030685,\n",
       "               150.0496110745609,\n",
       "               150.05330004167794,\n",
       "               150.0675071138812,\n",
       "               150.0886309126192,\n",
       "               150.12549769123547,\n",
       "               150.17533654173235,\n",
       "               150.16075518929043,\n",
       "               150.1716304566598,\n",
       "               150.1424674815964,\n",
       "               150.126676736329,\n",
       "               150.11988287689567,\n",
       "               150.1129262277063,\n",
       "               150.100916770948,\n",
       "               150.08504329200434,\n",
       "               150.12332581453254,\n",
       "               150.11320626039895,\n",
       "               150.11312685840906,\n",
       "               150.12397411595845,\n",
       "               150.12056942019296,\n",
       "               150.1230735149492,\n",
       "               150.11236514597988,\n",
       "               150.13616326155574,\n",
       "               150.12139554803335,\n",
       "               150.12394647802336,\n",
       "               150.11155437180346,\n",
       "               150.11136858404814,\n",
       "               150.09943532270557,\n",
       "               150.0668879771871,\n",
       "               150.0895963657619,\n",
       "               150.09369266410891,\n",
       "               150.0805865965879,\n",
       "               150.10966266742702,\n",
       "               150.14204953922805,\n",
       "               150.1311755687876,\n",
       "               150.14716075924247,\n",
       "               150.13951005031709,\n",
       "               150.12371396821996,\n",
       "               150.11304852829699,\n",
       "               150.12701948778346,\n",
       "               150.12556695220968,\n",
       "               150.1288932939998,\n",
       "               150.1178325346828,\n",
       "               150.12186936391237,\n",
       "               150.10572057575058,\n",
       "               150.1042281466444,\n",
       "               150.07307895117884,\n",
       "               150.08005685668417,\n",
       "               150.1166155648586,\n",
       "               150.1120879523546,\n",
       "               150.1130726172292,\n",
       "               150.1461572556841,\n",
       "               150.1120978880112,\n",
       "               150.1282988757079,\n",
       "               150.11781992070462,\n",
       "               150.11662585037206,\n",
       "               150.13009869962315,\n",
       "               150.11032296491865,\n",
       "               150.10472271282376,\n",
       "               150.10928058375652,\n",
       "               150.13709295772702,\n",
       "               150.13311167589615,\n",
       "               150.12597853603395,\n",
       "               150.1178646498381,\n",
       "               150.1513445524305,\n",
       "               150.15379249799113,\n",
       "               150.15201512754854,\n",
       "               150.15517736649207,\n",
       "               150.16731889191823,\n",
       "               150.1753549033829,\n",
       "               150.18576820692752,\n",
       "               150.18773167093917,\n",
       "               150.16013021600335,\n",
       "               150.14739438690495,\n",
       "               150.12813721871615,\n",
       "               150.13042334549607,\n",
       "               150.11149347490888,\n",
       "               150.09666088800566,\n",
       "               150.0726709884668,\n",
       "               150.0752878399949,\n",
       "               150.05466497101543,\n",
       "               150.03334211202198,\n",
       "               150.06069826096368,\n",
       "               150.05532453496383,\n",
       "               150.0477234523801,\n",
       "               150.08055269169583,\n",
       "               150.08268486865362,\n",
       "               150.09220526493664,\n",
       "               150.0747521772239,\n",
       "               150.06944615247292,\n",
       "               150.08218550876802,\n",
       "               150.10645775847834,\n",
       "               150.10770209621398,\n",
       "               150.09867840933546,\n",
       "               150.0934301352973,\n",
       "               150.10407429776674,\n",
       "               150.11127763551244,\n",
       "               150.10407291488227,\n",
       "               150.10197010990524,\n",
       "               150.09443248089605,\n",
       "               150.08607157567215,\n",
       "               150.09595951452928,\n",
       "               150.11250533541468,\n",
       "               150.09897723806995,\n",
       "               150.09211556094203,\n",
       "               150.12881382694968,\n",
       "               150.11850627644108,\n",
       "               150.14923069228945,\n",
       "               150.16966222592,\n",
       "               150.19268007198744,\n",
       "               150.19764988091268,\n",
       "               150.16898485453416,\n",
       "               150.1827214724926,\n",
       "               150.16549370830623,\n",
       "               150.1774987039021,\n",
       "               150.18905605086542,\n",
       "               150.16232025423443,\n",
       "               150.17794628334076,\n",
       "               150.2070557691859,\n",
       "               150.24902950869324,\n",
       "               150.2544103576371,\n",
       "               150.24096476076866,\n",
       "               150.2331452926328,\n",
       "               150.23359731197147,\n",
       "               150.28918377578668,\n",
       "               150.26146897366132,\n",
       "               150.22201454116689,\n",
       "               150.2257209738638,\n",
       "               150.2407353794199,\n",
       "               150.23997568322696,\n",
       "               150.23765589697783,\n",
       "               150.2448773397887,\n",
       "               150.28032711341973,\n",
       "               150.26128719536885,\n",
       "               150.2748406183405,\n",
       "               150.2683361769768,\n",
       "               150.28982160188988,\n",
       "               150.2908455468973,\n",
       "               150.29409716298895,\n",
       "               150.27983989399297,\n",
       "               150.25607900706783,\n",
       "               150.26284321759655,\n",
       "               150.26638328828042,\n",
       "               150.25456878947705,\n",
       "               150.21879640631357,\n",
       "               150.22719026167803,\n",
       "               150.21241449793678,\n",
       "               150.23767813819126,\n",
       "               150.24632619162057,\n",
       "               150.24253169404693,\n",
       "               150.24466059478914,\n",
       "               150.26116424414778,\n",
       "               150.2633553156955,\n",
       "               150.23664191836812,\n",
       "               150.23614091427316,\n",
       "               150.20728531067297,\n",
       "               150.19715606688004,\n",
       "               150.20028290761203,\n",
       "               150.16302315294772,\n",
       "               150.17416096784353,\n",
       "               150.1746777487422,\n",
       "               150.19127903798648,\n",
       "               150.1685257408205,\n",
       "               150.16950366052703,\n",
       "               150.17925588279,\n",
       "               150.15762812758007,\n",
       "               150.1493614996036,\n",
       "               150.18093191999245,\n",
       "               150.1675001001116,\n",
       "               150.15063500576508,\n",
       "               150.1426662751386,\n",
       "               150.16245358317636,\n",
       "               150.16989593236198,\n",
       "               150.15247443239784,\n",
       "               150.15617509744294,\n",
       "               150.14748887222598,\n",
       "               150.17389365940957,\n",
       "               150.17737886621305,\n",
       "               150.15036996036886,\n",
       "               150.16560525941216,\n",
       "               150.16131023368746,\n",
       "               150.16347429447572,\n",
       "               150.17120840825893,\n",
       "               150.16286451940212,\n",
       "               150.15433007763457,\n",
       "               150.15257735805275,\n",
       "               150.14079403042047,\n",
       "               150.1716156539557,\n",
       "               150.1784338424167,\n",
       "               150.18342790514689,\n",
       "               150.19207188139885,\n",
       "               150.19853657284784,\n",
       "               150.20637492377207,\n",
       "               150.22164302253194,\n",
       "               150.22169086342515,\n",
       "               150.21314483215986,\n",
       "               150.22335993678482,\n",
       "               150.20432875262946,\n",
       "               150.15370734502804,\n",
       "               150.14630569449955,\n",
       "               150.13832773951216,\n",
       "               150.16685217356437,\n",
       "               150.17867476147376,\n",
       "               150.1891392131735,\n",
       "               150.18388919201695,\n",
       "               150.18090090591352,\n",
       "               150.13285976900883,\n",
       "               150.16887121999494,\n",
       "               150.1960646940456,\n",
       "               150.1808066566628,\n",
       "               150.17416072756438,\n",
       "               150.20095492026033,\n",
       "               150.1614154048086,\n",
       "               150.1938397893177,\n",
       "               150.2147115904765,\n",
       "               150.22142025548865,\n",
       "               150.21797154256592,\n",
       "               150.21804097092178,\n",
       "               150.24117407936328,\n",
       "               150.24887873818494,\n",
       "               150.2729245942536,\n",
       "               150.28884919543438,\n",
       "               150.293927183835,\n",
       "               150.2566785286668,\n",
       "               150.21599347746704,\n",
       "               150.2216832809445,\n",
       "               150.26642394765182,\n",
       "               150.29718543494818,\n",
       "               150.30004370238558,\n",
       "               150.28495865697482,\n",
       "               150.2880838952321,\n",
       "               150.29168947253018,\n",
       "               150.2923640080159,\n",
       "               150.30903882541602,\n",
       "               150.2942176442846,\n",
       "               150.31112331450808,\n",
       "               150.32050408774236,\n",
       "               150.3290735062604,\n",
       "               150.34772571411142,\n",
       "               150.34372076829823,\n",
       "               150.3136768237876,\n",
       "               150.26518286842867,\n",
       "               150.27322211053973,\n",
       "               150.26246051723618,\n",
       "               150.2468176966465,\n",
       "               150.2468550051495,\n",
       "               150.2545056799865,\n",
       "               150.26080388362814,\n",
       "               150.24096213798848,\n",
       "               150.24379329849117,\n",
       "               150.2410951023363,\n",
       "               150.2480612402581,\n",
       "               150.2524464009385,\n",
       "               150.24978716549094,\n",
       "               150.25580098890615,\n",
       "               150.24305050842324,\n",
       "               150.24086398195928,\n",
       "               150.24294269366578,\n",
       "               150.23910895875653,\n",
       "               150.23911661626508,\n",
       "               150.23613073617662,\n",
       "               150.22610927366946,\n",
       "               150.23746878941702,\n",
       "               150.26168550774045,\n",
       "               150.29951129750933,\n",
       "               150.344418522451,\n",
       "               150.3836486247247,\n",
       "               150.3458330722876,\n",
       "               150.3133163057402,\n",
       "               150.31901730992945,\n",
       "               150.3247380149285,\n",
       "               150.32395793164258,\n",
       "               150.33428221274337,\n",
       "               150.31187944802568,\n",
       "               150.28609650619467,\n",
       "               150.27263775348956,\n",
       "               150.26289826879395,\n",
       "               150.29554634946322,\n",
       "               150.29628607097263,\n",
       "               150.27858016546807,\n",
       "               150.25991122663757,\n",
       "               150.27663109897335,\n",
       "               150.29844142284654,\n",
       "               150.30213914687326,\n",
       "               150.29435436997463,\n",
       "               150.25586022714995,\n",
       "               150.25025295574662,\n",
       "               150.21686149902698,\n",
       "               150.21135124197548,\n",
       "               150.19873719123535,\n",
       "               150.1978785320943,\n",
       "               150.17756192571048,\n",
       "               150.1767040847264,\n",
       "               150.1925332355408,\n",
       "               150.1874580716046,\n",
       "               150.1960582877474,\n",
       "               150.20395001121943,\n",
       "               150.20936105233636,\n",
       "               150.19628423504457,\n",
       "               150.20328586822836,\n",
       "               150.1899171013685,\n",
       "               150.16576312301342,\n",
       "               150.17333246366806,\n",
       "               150.16374249215,\n",
       "               150.1757755829591,\n",
       "               150.16599618440773,\n",
       "               150.17837376345526,\n",
       "               150.14603455832437,\n",
       "               150.13735564308715,\n",
       "               150.1405379758721,\n",
       "               150.1213996127115,\n",
       "               150.11995349726047,\n",
       "               150.09040659637776,\n",
       "               150.0819730073353,\n",
       "               150.0981762024332,\n",
       "               150.12640459049544,\n",
       "               150.11414620750705,\n",
       "               150.12879504066404,\n",
       "               150.11956675232204,\n",
       "               150.12448046412032,\n",
       "               150.11851649311183,\n",
       "               150.1625828779893,\n",
       "               150.12020871660732,\n",
       "               150.13383786467267,\n",
       "               150.14970943103143,\n",
       "               150.15194618313333,\n",
       "               150.1357845965266,\n",
       "               150.1314274153337,\n",
       "               150.14402227577816,\n",
       "               150.17744367694615,\n",
       "               150.15690232129612,\n",
       "               150.1767607565865,\n",
       "               150.19530899114312,\n",
       "               150.19890439747815,\n",
       "               150.21726368589697,\n",
       "               150.21424195749492,\n",
       "               150.24911890156767,\n",
       "               150.23668563641507,\n",
       "               150.22899522167893,\n",
       "               150.22552738702302,\n",
       "               150.2345059182651,\n",
       "               150.22701542423576,\n",
       "               150.24822730841302,\n",
       "               150.22938121135022,\n",
       "               150.23238754931535,\n",
       "               150.2414241008357,\n",
       "               150.2316597387518,\n",
       "               150.24483436397804,\n",
       "               150.2540976853754,\n",
       "               150.2709813998482,\n",
       "               150.27606070653937,\n",
       "               150.27587822241907,\n",
       "               150.28832041055946,\n",
       "               150.29242104877588,\n",
       "               150.29431443102857,\n",
       "               150.2783777549422,\n",
       "               150.32132199774438,\n",
       "               150.3279053174766,\n",
       "               150.33293655387976,\n",
       "               150.31828940977715,\n",
       "               150.340071400926,\n",
       "               150.3587673328989,\n",
       "               150.36502918485263,\n",
       "               150.35850461498538,\n",
       "               150.3770652498424,\n",
       "               150.37208326832263,\n",
       "               150.3562896958347,\n",
       "               150.35470930923466,\n",
       "               150.34210553220527,\n",
       "               150.34074412522122,\n",
       "               150.32186154889737,\n",
       "               150.3271622387796,\n",
       "               150.31819339822778,\n",
       "               150.30059182994395,\n",
       "               150.2995644056128,\n",
       "               150.30189460542124,\n",
       "               150.3115079197607,\n",
       "               150.2829930961173,\n",
       "               150.26188266136083,\n",
       "               150.26442684451183,\n",
       "               150.23512355675507,\n",
       "               150.22127926865969,\n",
       "               150.20651192582667,\n",
       "               150.21710525078782,\n",
       "               150.2054973784784,\n",
       "               150.22312904525026,\n",
       "               150.22161089934824,\n",
       "               150.23847661873057,\n",
       "               150.24411017636,\n",
       "               150.24238488742003,\n",
       "               150.24985130839332,\n",
       "               150.2677148787118,\n",
       "               150.2621908985059,\n",
       "               150.26826922021905,\n",
       "               150.272417185867,\n",
       "               150.2710550036734,\n",
       "               150.25732438936225,\n",
       "               150.24757076421244,\n",
       "               150.25500292009468,\n",
       "               150.27310645968063,\n",
       "               150.26960332815915,\n",
       "               150.26188047044337,\n",
       "               150.24150221004385,\n",
       "               150.2638771561446,\n",
       "               150.26377735543363,\n",
       "               150.23378226496305,\n",
       "               150.24500988707868,\n",
       "               150.2394501206626,\n",
       "               150.2326055454218,\n",
       "               150.22869454486317,\n",
       "               150.2271613916837,\n",
       "               150.22489981169727,\n",
       "               150.22042381534058,\n",
       "               150.2193094796434,\n",
       "               150.21466961450662,\n",
       "               150.23575388868989,\n",
       "               150.24866842995613,\n",
       "               150.2165917136832,\n",
       "               150.2714148733406,\n",
       "               150.2830349277501,\n",
       "               150.25115406592874,\n",
       "               150.24940036281984,\n",
       "               150.26281400490112,\n",
       "               150.27273458993187,\n",
       "               150.24387980891387,\n",
       "               150.2478906972705,\n",
       "               150.26116383494823,\n",
       "               150.2274974284795,\n",
       "               150.2304519450776,\n",
       "               150.23137951202455,\n",
       "               150.23223499437734,\n",
       "               150.22984257648085,\n",
       "               150.23458300878377,\n",
       "               150.24831889613364,\n",
       "               150.24973133288805,\n",
       "               150.2681119498198,\n",
       "               150.2420326630095,\n",
       "               150.22059822594517,\n",
       "               150.18184058459016,\n",
       "               150.16784906850222,\n",
       "               150.1829023698047,\n",
       "               150.20839280965723,\n",
       "               150.19763653644452,\n",
       "               150.19772269580693,\n",
       "               150.20102053019693,\n",
       "               150.2084717427414,\n",
       "               150.21669956751705,\n",
       "               150.2032488505082,\n",
       "               150.18534939498738,\n",
       "               150.19606608425522,\n",
       "               150.1958597836929,\n",
       "               150.19854933116153,\n",
       "               150.18200301583812,\n",
       "               150.19799972097104,\n",
       "               150.20121933360866,\n",
       "               150.2208867920333,\n",
       "               150.22082146507637,\n",
       "               150.1897393883987,\n",
       "               150.20232754763774,\n",
       "               150.19296395277618,\n",
       "               150.20717811571967],\n",
       "              'train-RMSE-mean': [878.815905418207,\n",
       "               868.7792962412395,\n",
       "               859.1905402976827,\n",
       "               849.9706747430382,\n",
       "               840.8350520402297,\n",
       "               832.1053169726262,\n",
       "               823.6418156555914,\n",
       "               815.4321594866441,\n",
       "               807.4056194061804,\n",
       "               799.6563864853963,\n",
       "               792.0618835066418,\n",
       "               784.8615515658996,\n",
       "               777.8942926680846,\n",
       "               771.0944342911107,\n",
       "               764.422484763007,\n",
       "               757.9671261501957,\n",
       "               751.7518589580941,\n",
       "               745.5712710629557,\n",
       "               739.7392596213527,\n",
       "               734.0102829318599,\n",
       "               728.525580749383,\n",
       "               723.1289046639989,\n",
       "               717.9882157833946,\n",
       "               712.8488754597316,\n",
       "               708.096719174316,\n",
       "               703.3781104020336,\n",
       "               698.8197823122572,\n",
       "               694.2254423863683,\n",
       "               689.8430138802427,\n",
       "               685.5874105641401,\n",
       "               681.3334888588759,\n",
       "               677.3152963544017,\n",
       "               673.3807270175632,\n",
       "               669.4595779060822,\n",
       "               665.8241533580275,\n",
       "               662.0607952448225,\n",
       "               658.3190740569737,\n",
       "               654.7673366200191,\n",
       "               651.4481248946078,\n",
       "               647.9884134512043,\n",
       "               644.7821849231457,\n",
       "               641.5471786333012,\n",
       "               638.2566804574262,\n",
       "               635.2891294357227,\n",
       "               632.5859081830456,\n",
       "               629.9514825287994,\n",
       "               627.1068852291905,\n",
       "               624.421245541767,\n",
       "               621.5952556392201,\n",
       "               618.9029749314512,\n",
       "               616.3994383497403,\n",
       "               613.7543898139968,\n",
       "               611.2816616800444,\n",
       "               608.8324547310783,\n",
       "               606.356242515437,\n",
       "               604.1254080333067,\n",
       "               601.9645031273346,\n",
       "               599.8106691058648,\n",
       "               597.5437613406367,\n",
       "               595.3600808418948,\n",
       "               593.2488639008667,\n",
       "               591.2522110653365,\n",
       "               589.2150530646861,\n",
       "               587.2125338833528,\n",
       "               585.1430561486914,\n",
       "               583.3711543294814,\n",
       "               581.3866172448564,\n",
       "               579.5630175684771,\n",
       "               577.7042920840408,\n",
       "               575.8800874075557,\n",
       "               574.1284775390789,\n",
       "               572.4252657503633,\n",
       "               570.6261497864165,\n",
       "               568.9318219608413,\n",
       "               567.4441795211167,\n",
       "               565.8908521504122,\n",
       "               564.1736854243809,\n",
       "               562.6363904266788,\n",
       "               561.0529131767125,\n",
       "               559.5557947302977,\n",
       "               558.141300784321,\n",
       "               556.6875937495047,\n",
       "               555.2125191952551,\n",
       "               553.8407432370601,\n",
       "               552.3845791773548,\n",
       "               550.9585027190817,\n",
       "               549.8003858682185,\n",
       "               548.5509167094945,\n",
       "               547.2840097045706,\n",
       "               545.9199761600278,\n",
       "               544.8017036848673,\n",
       "               543.7164731761817,\n",
       "               542.5392691374257,\n",
       "               541.2598145494478,\n",
       "               540.1630820270447,\n",
       "               539.0141900784995,\n",
       "               537.8603644091766,\n",
       "               536.7928470723899,\n",
       "               535.6502362056491,\n",
       "               534.5137416186352,\n",
       "               533.5135894757486,\n",
       "               532.442665845441,\n",
       "               531.444260128744,\n",
       "               530.4611251601096,\n",
       "               529.451996157937,\n",
       "               528.3878630634124,\n",
       "               527.376701522886,\n",
       "               526.4725881545311,\n",
       "               525.5105699804155,\n",
       "               524.6092212504434,\n",
       "               523.7120208375579,\n",
       "               522.8188859493115,\n",
       "               521.9175224152294,\n",
       "               520.9658181805856,\n",
       "               520.153888862641,\n",
       "               519.1656767975395,\n",
       "               518.2273384768132,\n",
       "               517.257430041039,\n",
       "               516.3838766194282,\n",
       "               515.605182734396,\n",
       "               514.6322014799514,\n",
       "               513.7477051219602,\n",
       "               512.7352707870563,\n",
       "               511.77461653435495,\n",
       "               510.8533596010332,\n",
       "               509.84792456526395,\n",
       "               509.0845327699334,\n",
       "               508.3205547111039,\n",
       "               507.59119626854016,\n",
       "               506.6396462507893,\n",
       "               505.80598714491,\n",
       "               504.93756662183205,\n",
       "               504.02406412771717,\n",
       "               503.1876932619489,\n",
       "               502.26775216063123,\n",
       "               501.43052779843964,\n",
       "               500.62257814809857,\n",
       "               499.9829014535879,\n",
       "               499.1784976861983,\n",
       "               498.47348854653563,\n",
       "               497.6983994025183,\n",
       "               497.0916252414173,\n",
       "               496.52888229084175,\n",
       "               495.8094860616819,\n",
       "               495.2061210668218,\n",
       "               494.463678012408,\n",
       "               493.7972045001782,\n",
       "               493.250889246536,\n",
       "               492.5080477839614,\n",
       "               491.9315481154416,\n",
       "               491.387458571811,\n",
       "               490.91679752853963,\n",
       "               490.3619932014193,\n",
       "               489.8214645410486,\n",
       "               489.19859305105564,\n",
       "               488.65337427358526,\n",
       "               487.9013266506246,\n",
       "               487.35411073601233,\n",
       "               486.7154836655133,\n",
       "               486.1559978519228,\n",
       "               485.57987078941494,\n",
       "               485.157802239525,\n",
       "               484.59754568558867,\n",
       "               484.0747594155261,\n",
       "               483.5617314354139,\n",
       "               483.05140592471145,\n",
       "               482.60119424802554,\n",
       "               482.0359650660474,\n",
       "               481.5370739177175,\n",
       "               481.1066903747318,\n",
       "               480.69402049713517,\n",
       "               480.22024541256286,\n",
       "               479.7126302367234,\n",
       "               479.3193761106665,\n",
       "               478.76979728979705,\n",
       "               478.17601209271515,\n",
       "               477.7913007993363,\n",
       "               477.3653947901897,\n",
       "               476.8983374142361,\n",
       "               476.5514989341069,\n",
       "               475.895280073701,\n",
       "               475.425002492983,\n",
       "               475.07289842635464,\n",
       "               474.64237807591763,\n",
       "               474.26893632923077,\n",
       "               473.8193359913803,\n",
       "               473.46386805065623,\n",
       "               473.10145701831595,\n",
       "               472.7665824471072,\n",
       "               472.41216492408614,\n",
       "               471.9349349245478,\n",
       "               471.59115483660906,\n",
       "               471.2530960411759,\n",
       "               470.92722731015874,\n",
       "               470.57968867666216,\n",
       "               470.1965113293839,\n",
       "               469.9102286367391,\n",
       "               469.63513365199503,\n",
       "               469.3437458838183,\n",
       "               468.991145383426,\n",
       "               468.6599001686321,\n",
       "               468.2073339230842,\n",
       "               467.8449103886177,\n",
       "               467.48541108613233,\n",
       "               467.0451176864789,\n",
       "               466.74193758732565,\n",
       "               466.4626132662337,\n",
       "               465.9799509259889,\n",
       "               465.68434506481253,\n",
       "               465.29597754297475,\n",
       "               465.1080701156361,\n",
       "               464.67140510586194,\n",
       "               464.37749545620215,\n",
       "               464.0738012011049,\n",
       "               463.8055167942116,\n",
       "               463.4868127853136,\n",
       "               463.2732189638672,\n",
       "               462.9856143158589,\n",
       "               462.74552748764444,\n",
       "               462.411071749849,\n",
       "               462.2351643081546,\n",
       "               461.99531940605294,\n",
       "               461.6107307531097,\n",
       "               461.284925866766,\n",
       "               461.0195620191154,\n",
       "               460.72622781867994,\n",
       "               460.455027664538,\n",
       "               460.1722638190555,\n",
       "               459.96201844895523,\n",
       "               459.7313047900652,\n",
       "               459.48665850511,\n",
       "               459.1268020181402,\n",
       "               458.8998527787965,\n",
       "               458.60088631285026,\n",
       "               458.3253595356074,\n",
       "               457.93808091657695,\n",
       "               457.72481191580727,\n",
       "               457.3875862442228,\n",
       "               457.1031406959305,\n",
       "               456.81506204548515,\n",
       "               456.4622764013099,\n",
       "               456.1112825373583,\n",
       "               455.7824456445166,\n",
       "               455.6428199929367,\n",
       "               455.4002157423656,\n",
       "               455.11150169673493,\n",
       "               454.8223233286888,\n",
       "               454.60039305710114,\n",
       "               454.2365351279306,\n",
       "               453.88126210096846,\n",
       "               453.6128339953375,\n",
       "               453.3138300923113,\n",
       "               453.10579239883145,\n",
       "               452.8672847782947,\n",
       "               452.5828608837959,\n",
       "               452.3038330082844,\n",
       "               452.0119529566253,\n",
       "               451.7283755534762,\n",
       "               451.4596749717178,\n",
       "               451.2982177926865,\n",
       "               450.84347915525257,\n",
       "               450.68115231641207,\n",
       "               450.442817133379,\n",
       "               450.2034089160351,\n",
       "               449.9371784936799,\n",
       "               449.71611334624515,\n",
       "               449.42348665816064,\n",
       "               449.18014274745167,\n",
       "               448.99802775438945,\n",
       "               448.69467215919974,\n",
       "               448.3990513787934,\n",
       "               447.9788073164262,\n",
       "               447.7229908357731,\n",
       "               447.45204369977773,\n",
       "               447.2080469568535,\n",
       "               446.94914378961874,\n",
       "               446.5988892353388,\n",
       "               446.38421162808993,\n",
       "               446.07007360661225,\n",
       "               445.85680942580865,\n",
       "               445.53605715087997,\n",
       "               445.1851107364847,\n",
       "               444.89087839932773,\n",
       "               444.5681981792853,\n",
       "               444.30343393809636,\n",
       "               443.8180981157364,\n",
       "               443.4744788853086,\n",
       "               443.1128816481888,\n",
       "               442.7189660665496,\n",
       "               442.4141198831468,\n",
       "               442.0909248329309,\n",
       "               441.7671960804581,\n",
       "               441.6592371003512,\n",
       "               441.42980762921036,\n",
       "               441.19659819540357,\n",
       "               440.9718059509064,\n",
       "               440.7549230099331,\n",
       "               440.48176057880863,\n",
       "               440.16963214060735,\n",
       "               439.948990756063,\n",
       "               439.681812737984,\n",
       "               439.42508645675497,\n",
       "               439.2023516191683,\n",
       "               438.9791189786236,\n",
       "               438.80096545013976,\n",
       "               438.54241866719275,\n",
       "               438.32068959734426,\n",
       "               438.0177646862838,\n",
       "               437.76752572064726,\n",
       "               437.5387851485395,\n",
       "               437.29616197120913,\n",
       "               437.0069774642219,\n",
       "               436.61628988338015,\n",
       "               436.3601581660672,\n",
       "               436.10952841113414,\n",
       "               435.8722889828923,\n",
       "               435.6228330114428,\n",
       "               435.35061939557437,\n",
       "               435.13096753864176,\n",
       "               434.9892896528225,\n",
       "               434.6064430291896,\n",
       "               434.3817026635553,\n",
       "               434.2270055201519,\n",
       "               434.01694557826795,\n",
       "               433.8222018415912,\n",
       "               433.5984862180685,\n",
       "               433.47664107437623,\n",
       "               433.18638264913886,\n",
       "               432.9914128478569,\n",
       "               432.74395142091834,\n",
       "               432.57065743670717,\n",
       "               432.34278123274305,\n",
       "               432.1171492280611,\n",
       "               431.91097703467733,\n",
       "               431.59064250733826,\n",
       "               431.3130428663887,\n",
       "               431.0473768989188,\n",
       "               430.8278199426571,\n",
       "               430.62428533547217,\n",
       "               430.3311431677504,\n",
       "               430.0924999507045,\n",
       "               429.80303223395214,\n",
       "               429.5902674757088,\n",
       "               429.36084927762124,\n",
       "               429.176327693703,\n",
       "               428.8848191034908,\n",
       "               428.654578388136,\n",
       "               428.3690356134747,\n",
       "               428.0498747033672,\n",
       "               427.83410998313445,\n",
       "               427.65748500859974,\n",
       "               427.2684992244732,\n",
       "               427.057132074529,\n",
       "               426.88382407174885,\n",
       "               426.67809968183053,\n",
       "               426.50404374284096,\n",
       "               426.21030548882834,\n",
       "               426.05516568189216,\n",
       "               425.7615618589267,\n",
       "               425.49896793395476,\n",
       "               425.3105557373712,\n",
       "               425.1027670998957,\n",
       "               424.9493055423595,\n",
       "               424.66253964398356,\n",
       "               424.38572634430136,\n",
       "               424.2004523166817,\n",
       "               423.9447947277473,\n",
       "               423.7486403095278,\n",
       "               423.56885049639004,\n",
       "               423.36428807812246,\n",
       "               423.19796200199073,\n",
       "               422.94612627498253,\n",
       "               422.78033979394576,\n",
       "               422.5969731185569,\n",
       "               422.4207503520959,\n",
       "               422.1534051534253,\n",
       "               422.0004983364587,\n",
       "               421.7916632257678,\n",
       "               421.4895949066213,\n",
       "               421.33131390543133,\n",
       "               421.0739558478237,\n",
       "               420.8206671042044,\n",
       "               420.586136084739,\n",
       "               420.33177043171673,\n",
       "               420.0967472275421,\n",
       "               419.91946462913756,\n",
       "               419.5862148987444,\n",
       "               419.2881535822602,\n",
       "               419.0294012823506,\n",
       "               418.76490072407057,\n",
       "               418.59728927945423,\n",
       "               418.40708611735164,\n",
       "               418.23976811936075,\n",
       "               418.0375024860302,\n",
       "               417.8253298870956,\n",
       "               417.59788025612653,\n",
       "               417.32675293840975,\n",
       "               417.131196619592,\n",
       "               416.7832071075906,\n",
       "               416.5643444518655,\n",
       "               416.34421244028215,\n",
       "               416.10729409441075,\n",
       "               415.93823471996495,\n",
       "               415.71496656465496,\n",
       "               415.4782276329994,\n",
       "               415.22134753683457,\n",
       "               415.00225328926507,\n",
       "               414.79927718393947,\n",
       "               414.65848274077507,\n",
       "               414.5371294734538,\n",
       "               414.3831666401855,\n",
       "               414.155995377456,\n",
       "               413.9410763896629,\n",
       "               413.7911674888517,\n",
       "               413.5649387886686,\n",
       "               413.30784881890185,\n",
       "               413.0296505553255,\n",
       "               412.85035523032974,\n",
       "               412.6176531106806,\n",
       "               412.3868172260949,\n",
       "               412.1717524435024,\n",
       "               411.9388651677518,\n",
       "               411.7854698258776,\n",
       "               411.560687343978,\n",
       "               411.34445686370617,\n",
       "               411.1527414966846,\n",
       "               410.9088755806288,\n",
       "               410.67524490374706,\n",
       "               410.484547088569,\n",
       "               410.31155255818993,\n",
       "               410.14385534939885,\n",
       "               409.9032653466692,\n",
       "               409.68146502679446,\n",
       "               409.50126691392313,\n",
       "               409.25539381094245,\n",
       "               408.9748322473968,\n",
       "               408.76347229503375,\n",
       "               408.6220929274114,\n",
       "               408.38735107777865,\n",
       "               408.2289820897919,\n",
       "               407.9384840243867,\n",
       "               407.7398512146129,\n",
       "               407.56947211172155,\n",
       "               407.38766538504956,\n",
       "               407.1393064473961,\n",
       "               406.9489027600336,\n",
       "               406.68399130807,\n",
       "               406.50337213568366,\n",
       "               406.244853313052,\n",
       "               406.0304841524809,\n",
       "               405.75792650489944,\n",
       "               405.49956923261857,\n",
       "               405.2138907238392,\n",
       "               404.987738475857,\n",
       "               404.7966328478912,\n",
       "               404.54935457498385,\n",
       "               404.30257239612706,\n",
       "               404.1093303564528,\n",
       "               403.846303070058,\n",
       "               403.6170224484914,\n",
       "               403.3825681630844,\n",
       "               403.1842511912293,\n",
       "               402.9867132848246,\n",
       "               402.79478125290177,\n",
       "               402.64127564052006,\n",
       "               402.5345686343201,\n",
       "               402.3602528921475,\n",
       "               402.08190824159976,\n",
       "               401.94321851150573,\n",
       "               401.8098481308899,\n",
       "               401.56382270562085,\n",
       "               401.3048101117353,\n",
       "               401.10595041965905,\n",
       "               400.76627466470296,\n",
       "               400.61097584821033,\n",
       "               400.42292500789296,\n",
       "               400.1503799879395,\n",
       "               399.9491554168645,\n",
       "               399.70868526396396,\n",
       "               399.46572019604116,\n",
       "               399.2440610991734,\n",
       "               398.96726976553,\n",
       "               398.75427013680866,\n",
       "               398.5454406358009,\n",
       "               398.37467353820097,\n",
       "               398.20554173730295,\n",
       "               397.96559908950485,\n",
       "               397.7355142438452,\n",
       "               397.4503227352358,\n",
       "               397.15747583563245,\n",
       "               396.8929752136266,\n",
       "               396.6169579859606,\n",
       "               396.4801992501964,\n",
       "               396.30533728664653,\n",
       "               396.1029323508481,\n",
       "               395.91561755990847,\n",
       "               395.72895808573145,\n",
       "               395.44276411075936,\n",
       "               395.2673652459408,\n",
       "               395.05756390681984,\n",
       "               394.7362528407606,\n",
       "               394.5602542139786,\n",
       "               394.33328295333445,\n",
       "               394.2035826174214,\n",
       "               394.05673497733625,\n",
       "               393.79967040858764,\n",
       "               393.61636816133716,\n",
       "               393.3799055203207,\n",
       "               393.1715852676539,\n",
       "               392.9688535751598,\n",
       "               392.7593940638933,\n",
       "               392.5007761740705,\n",
       "               392.2753152041513,\n",
       "               392.0175936724825,\n",
       "               391.8424928204504,\n",
       "               391.6175402094415,\n",
       "               391.38369380436933,\n",
       "               391.18741887592057,\n",
       "               390.9069732776487,\n",
       "               390.7430230708924,\n",
       "               390.52705232665346,\n",
       "               390.281685371544,\n",
       "               390.0398944521367,\n",
       "               389.88098570766175,\n",
       "               389.726873406401,\n",
       "               389.42796260690193,\n",
       "               389.21913230549274,\n",
       "               388.9859121740842,\n",
       "               388.7577254935553,\n",
       "               388.56660719067315,\n",
       "               388.36868591645185,\n",
       "               388.20110139416846,\n",
       "               387.99550886298755,\n",
       "               387.83380915685893,\n",
       "               387.6423517964283,\n",
       "               387.3669578294741,\n",
       "               387.1674895711253,\n",
       "               386.93045036826226,\n",
       "               386.7596857515019,\n",
       "               386.6038133571625,\n",
       "               386.35874354054596,\n",
       "               386.0953039776242,\n",
       "               385.8995631858632,\n",
       "               385.68447708564594,\n",
       "               385.37747901795143,\n",
       "               385.08635466294743,\n",
       "               384.90498834520173,\n",
       "               384.64002794116385,\n",
       "               384.43016310508557,\n",
       "               384.2023111764405,\n",
       "               384.01440745974844,\n",
       "               383.81818385679594,\n",
       "               383.6009585542794,\n",
       "               383.33144178469547,\n",
       "               383.0559886081448,\n",
       "               382.8242298547114,\n",
       "               382.58368331546234,\n",
       "               382.37058274555045,\n",
       "               382.2057027457594,\n",
       "               382.0384539309788,\n",
       "               381.9004466674181,\n",
       "               381.65293373230577,\n",
       "               381.4478530372022,\n",
       "               381.23446029549973,\n",
       "               381.00685908065395,\n",
       "               380.8329629115801,\n",
       "               380.6366681755718,\n",
       "               380.3720038514211,\n",
       "               380.15190863068716,\n",
       "               379.925218343225,\n",
       "               379.71243621372594,\n",
       "               379.5343232292859,\n",
       "               379.35702399848765,\n",
       "               379.18007563020876,\n",
       "               379.01811137272466,\n",
       "               378.75938476028915,\n",
       "               378.48973212807414,\n",
       "               378.2945228241977,\n",
       "               378.1305554019356,\n",
       "               377.8910965313974,\n",
       "               377.71099220381086,\n",
       "               377.51827097468686,\n",
       "               377.336743491767,\n",
       "               377.1768616572033,\n",
       "               376.9846124671763,\n",
       "               376.8936685765974,\n",
       "               376.750106675153,\n",
       "               376.5767770492215,\n",
       "               376.3084343617939,\n",
       "               376.12598283564375,\n",
       "               375.9570460187488,\n",
       "               375.7744300270882,\n",
       "               375.55372238595044,\n",
       "               375.4051625747102,\n",
       "               375.15872633299585,\n",
       "               374.9220650421699,\n",
       "               374.7434906164091,\n",
       "               374.55316605885116,\n",
       "               374.38003481818697,\n",
       "               374.19669283636404,\n",
       "               374.0026859219534,\n",
       "               373.7979916200488,\n",
       "               373.5884064866615,\n",
       "               373.44461058904767,\n",
       "               373.1976576618923,\n",
       "               372.99460405350766,\n",
       "               372.7148389256072,\n",
       "               372.5264140813641,\n",
       "               372.3379899284508,\n",
       "               372.1790564950591,\n",
       "               372.0065629008682,\n",
       "               371.85257626956775,\n",
       "               371.58876803171813,\n",
       "               371.4337808279944,\n",
       "               371.27068145288195,\n",
       "               371.08810267009005,\n",
       "               370.8967870939649,\n",
       "               370.711999815869,\n",
       "               370.4192039274134,\n",
       "               370.2472614973784,\n",
       "               370.0871347493361,\n",
       "               369.9076553259844,\n",
       "               369.7247065344868,\n",
       "               369.5289834541994,\n",
       "               369.3098604780911,\n",
       "               369.06269557486763,\n",
       "               368.853182417467,\n",
       "               368.6654313982672,\n",
       "               368.42130637743503,\n",
       "               368.2489906232382,\n",
       "               368.08040209029286,\n",
       "               367.8834170553517,\n",
       "               367.721309202359,\n",
       "               367.62677647716976,\n",
       "               367.38273999427054,\n",
       "               367.17531424863216,\n",
       "               366.9543953944059,\n",
       "               366.6129805070023,\n",
       "               366.44916615336194,\n",
       "               366.2236216079921,\n",
       "               366.03841801403263,\n",
       "               365.8173944123119,\n",
       "               365.5720757835256,\n",
       "               365.4855551339875,\n",
       "               365.2483963496327,\n",
       "               364.9926732655366,\n",
       "               364.7024963869591,\n",
       "               364.5231044937758,\n",
       "               364.29803079153214,\n",
       "               364.0827402422718,\n",
       "               363.93151986962255,\n",
       "               363.81795835651576,\n",
       "               363.6483840922333,\n",
       "               363.52442827314184,\n",
       "               363.30452520673265,\n",
       "               363.10425272929547,\n",
       "               362.87088213882964,\n",
       "               362.57486483893297,\n",
       "               362.31986756054795,\n",
       "               361.9822718302913,\n",
       "               361.8233864256621,\n",
       "               361.650559370666,\n",
       "               361.45196627603195,\n",
       "               361.1763619947828,\n",
       "               360.9760296029244,\n",
       "               360.71372521525296,\n",
       "               360.51979898264165,\n",
       "               360.2460971249548,\n",
       "               360.083428595303,\n",
       "               359.92991325939596,\n",
       "               359.80660322083213,\n",
       "               359.57370125052887,\n",
       "               359.3782820829972,\n",
       "               359.2033202952061,\n",
       "               359.0030858188954,\n",
       "               358.8058192972973,\n",
       "               358.6585924856028,\n",
       "               358.4180135062487,\n",
       "               358.1560210280047,\n",
       "               357.956065696984,\n",
       "               357.76596776903904,\n",
       "               357.60308639983833,\n",
       "               357.4035227834209,\n",
       "               357.2236156008101,\n",
       "               357.02808797536795,\n",
       "               356.78705571621856,\n",
       "               356.53906769944524,\n",
       "               356.3772507365296,\n",
       "               356.15537123820525,\n",
       "               356.0203792407318,\n",
       "               355.8293542921225,\n",
       "               355.6857881775854,\n",
       "               355.49831618724886,\n",
       "               355.32518311184356,\n",
       "               355.0820224204519,\n",
       "               354.9076302070756,\n",
       "               354.7023940896527,\n",
       "               354.45645596261704,\n",
       "               354.2027266040744,\n",
       "               354.0424045273471,\n",
       "               353.8263227953075,\n",
       "               353.58891819916033,\n",
       "               353.3710902444424,\n",
       "               353.17690037321273,\n",
       "               353.0521406859627,\n",
       "               352.830416414612,\n",
       "               352.58510935598713,\n",
       "               352.39596266868733,\n",
       "               352.24993973456355,\n",
       "               352.09327256882005,\n",
       "               351.8862006211367,\n",
       "               351.7551471406793,\n",
       "               351.5725688387071,\n",
       "               351.3905839391098,\n",
       "               351.13183347135225,\n",
       "               350.8901731324771,\n",
       "               350.65968403590756,\n",
       "               350.46641470986776,\n",
       "               350.28226089969826,\n",
       "               350.08708916530463,\n",
       "               349.85460989644423,\n",
       "               349.6963747422486,\n",
       "               349.4977888489046,\n",
       "               349.34110570795934,\n",
       "               349.1695177522041,\n",
       "               348.98649032349124,\n",
       "               348.8522271604762,\n",
       "               348.7143572034836,\n",
       "               348.5353174430109,\n",
       "               348.31418933360845,\n",
       "               348.15835882117517,\n",
       "               347.96356444294645,\n",
       "               347.8271151685669,\n",
       "               347.6541497480076,\n",
       "               347.41323767815254,\n",
       "               347.21480039366344,\n",
       "               347.0296561971635,\n",
       "               346.8080122154571,\n",
       "               346.62301933935635,\n",
       "               346.4130212860344,\n",
       "               346.2401060419623,\n",
       "               346.01400840030976,\n",
       "               345.8399984511484,\n",
       "               345.6626512890813,\n",
       "               345.50789488942075,\n",
       "               345.28493327528497,\n",
       "               345.1308151005367,\n",
       "               344.9332228345248,\n",
       "               344.76478193588684,\n",
       "               344.5641630262109,\n",
       "               344.3203313435196,\n",
       "               344.14401894558625,\n",
       "               343.9576933610256,\n",
       "               343.7581026622029,\n",
       "               343.55154311784923,\n",
       "               343.34518076121867,\n",
       "               343.19061795531553,\n",
       "               342.9668465514965,\n",
       "               342.7927618569955,\n",
       "               342.65207503583343,\n",
       "               342.5036643462105,\n",
       "               342.33570866775733,\n",
       "               342.1220066281425,\n",
       "               341.92056800992816,\n",
       "               341.6914774260725,\n",
       "               341.46878691301265,\n",
       "               341.2671869970088,\n",
       "               341.0896191349845,\n",
       "               340.97002520215193,\n",
       "               340.8089970600673,\n",
       "               340.62215308689713,\n",
       "               340.40796133340586,\n",
       "               340.22479426502747,\n",
       "               340.050683511233,\n",
       "               339.8675511810284,\n",
       "               339.65156993580695,\n",
       "               339.4467410569981,\n",
       "               339.2391649932378,\n",
       "               339.103366161187,\n",
       "               338.94834996182055,\n",
       "               338.74563941211954,\n",
       "               338.54554714253953,\n",
       "               338.378188491026,\n",
       "               338.1891803356992,\n",
       "               337.98499275971517,\n",
       "               337.74151269076117,\n",
       "               337.57307326752016,\n",
       "               337.39137852579205,\n",
       "               337.2326709553534,\n",
       "               337.0927602081711,\n",
       "               336.90481998649443,\n",
       "               336.7660472026695,\n",
       "               336.6077045314518,\n",
       "               336.42503128901996,\n",
       "               336.23130344542926,\n",
       "               336.0844695400382,\n",
       "               335.9080367548236,\n",
       "               335.7105412339762,\n",
       "               335.5551739745892,\n",
       "               335.4115131955258,\n",
       "               335.2229467731254,\n",
       "               335.0443111341129,\n",
       "               334.7831086941311,\n",
       "               334.57286296032606,\n",
       "               334.4352780427422,\n",
       "               334.2710232488068,\n",
       "               334.0866716287019,\n",
       "               333.8419839085653,\n",
       "               333.657836802852,\n",
       "               333.5063681073732,\n",
       "               333.3314590755169,\n",
       "               333.16012647500656,\n",
       "               332.93989310253016,\n",
       "               332.73880596067806,\n",
       "               332.58398732758286,\n",
       "               332.42865371871613,\n",
       "               332.22242036217636,\n",
       "               332.04705126737974,\n",
       "               331.85465475439037,\n",
       "               331.70411649006945,\n",
       "               331.5336815135959,\n",
       "               331.3801673616926,\n",
       "               331.1992335859874,\n",
       "               331.03389429522383,\n",
       "               330.87994228297794,\n",
       "               330.69448357912995,\n",
       "               330.5110269288719,\n",
       "               330.2771664849446,\n",
       "               330.13405096852665,\n",
       "               329.9641115352526,\n",
       "               329.8500103322436,\n",
       "               329.7014563154365,\n",
       "               329.524832900634,\n",
       "               329.33612086398983,\n",
       "               329.1310036522505,\n",
       "               328.9164419276229,\n",
       "               328.7255486333097,\n",
       "               328.5334016882361,\n",
       "               328.4023490952963,\n",
       "               328.2424342022126,\n",
       "               328.0501057460076,\n",
       "               327.9198643522791,\n",
       "               327.6875453107897,\n",
       "               327.4671785883588,\n",
       "               327.32304082141843,\n",
       "               327.2001207996523,\n",
       "               327.0543220862347,\n",
       "               326.85784377070615,\n",
       "               326.67407339594337,\n",
       "               326.4753763740453,\n",
       "               326.3341181130674,\n",
       "               326.10867081577464,\n",
       "               325.94418494230405,\n",
       "               325.8226342685954,\n",
       "               325.6798447234558,\n",
       "               325.5638915228458,\n",
       "               325.4472419601831,\n",
       "               325.2289185252987,\n",
       "               325.03719609111715,\n",
       "               324.9007710080374,\n",
       "               324.72386081828296,\n",
       "               324.5480753022074,\n",
       "               324.3909658580739,\n",
       "               324.2651427062573,\n",
       "               324.1160456571093,\n",
       "               323.9583616747642,\n",
       "               323.80434249675284,\n",
       "               323.6307487182395,\n",
       "               323.5051355198325,\n",
       "               323.34515373673514,\n",
       "               323.15325296404865,\n",
       "               323.00494147586403,\n",
       "               322.81270036017025,\n",
       "               322.6120134103957,\n",
       "               322.4160134295215,\n",
       "               322.23330373886154,\n",
       "               322.03977212372325,\n",
       "               321.8831727844353,\n",
       "               321.7418493080387,\n",
       "               321.5892364703149,\n",
       "               321.44290079536376,\n",
       "               321.3070550400353,\n",
       "               321.135225743835,\n",
       "               320.98552356707114,\n",
       "               320.8262169495375,\n",
       "               320.6798780320463,\n",
       "               320.5204885749779,\n",
       "               320.35099349615973,\n",
       "               320.2364101046031,\n",
       "               320.10173267635406,\n",
       "               319.9509234647168,\n",
       "               319.74903157964553,\n",
       "               319.6408499731236,\n",
       "               319.4378263290181,\n",
       "               319.30411755849667,\n",
       "               319.1385631854607,\n",
       "               318.94952094530794,\n",
       "               318.8067587080377,\n",
       "               318.6531691324407,\n",
       "               318.53467252494414,\n",
       "               318.3780976766155,\n",
       "               318.217375243569,\n",
       "               318.02154264474586,\n",
       "               317.86610240554586,\n",
       "               317.70689675923353,\n",
       "               317.508829389942,\n",
       "               317.3759571011696,\n",
       "               317.220280271191,\n",
       "               317.0242158690106,\n",
       "               316.8502213291878,\n",
       "               316.73639858567043,\n",
       "               316.57771208119294,\n",
       "               316.43129711152295,\n",
       "               316.21279499327454,\n",
       "               316.07632307848286,\n",
       "               315.8910006080604,\n",
       "               315.7525134591295,\n",
       "               315.59081329867155,\n",
       "               315.4644331858097,\n",
       "               315.3162421174646,\n",
       "               315.18565085172173,\n",
       "               315.0566271546553,\n",
       "               314.9334064936262,\n",
       "               314.8159024709549,\n",
       "               314.67545524458626,\n",
       "               314.50518441754696,\n",
       "               314.2915923677056,\n",
       "               314.1080645524329,\n",
       "               314.01367972945843,\n",
       "               313.87504569629243,\n",
       "               313.7083176791222,\n",
       "               313.56982078195955,\n",
       "               313.42066162890217,\n",
       "               313.2554240510556,\n",
       "               313.0553817589788,\n",
       "               312.88506837376633,\n",
       "               312.72506672222937,\n",
       "               312.58404082914495,\n",
       "               312.35742141201274,\n",
       "               312.1596288983023,\n",
       "               311.99205381976003,\n",
       "               311.86318648134386,\n",
       "               311.67431696439974,\n",
       "               311.55443355841015,\n",
       "               311.3446862639966,\n",
       "               311.2237495786272,\n",
       "               311.0226255554793,\n",
       "               310.8721722091103,\n",
       "               310.742207737615,\n",
       "               310.6236712570184,\n",
       "               310.5072169095872,\n",
       "               310.3777579736551,\n",
       "               310.2053350761859,\n",
       "               309.995381638433,\n",
       "               309.8636267927385,\n",
       "               309.6831793501916,\n",
       "               309.4675519643097,\n",
       "               309.31701166649856,\n",
       "               309.13794808390185,\n",
       "               308.98338475871356,\n",
       "               308.8789166869238,\n",
       "               308.68405721929304,\n",
       "               308.5583834154519,\n",
       "               308.396000960934,\n",
       "               308.2707135808065,\n",
       "               308.07805569635116,\n",
       "               307.8582995209109,\n",
       "               307.66082055126964,\n",
       "               307.4500208208359,\n",
       "               307.3003188165484,\n",
       "               307.1443854785523,\n",
       "               306.96238031381097,\n",
       "               306.8106755486155,\n",
       "               306.64307116420576,\n",
       "               306.51348317929705,\n",
       "               306.32985987291335,\n",
       "               306.1658974077409,\n",
       "               306.026882463801,\n",
       "               305.90469415026763,\n",
       "               305.7253250801667,\n",
       "               305.58276540756657,\n",
       "               305.440639246514,\n",
       "               305.2524768433368,\n",
       "               305.09878428799027,\n",
       "               304.97248475168874,\n",
       "               304.78868479833415,\n",
       "               304.6598033954723,\n",
       "               304.50629054760344,\n",
       "               304.4021570750162,\n",
       "               304.302302758559,\n",
       "               304.19563223966713,\n",
       "               303.9804710988916,\n",
       "               303.8282070508169,\n",
       "               303.67631605662257,\n",
       "               303.50044956405026,\n",
       "               303.38627121630304,\n",
       "               303.2285382294904,\n",
       "               303.09345032171865,\n",
       "               302.9906786558395,\n",
       "               302.80609797017365],\n",
       "              'train-RMSE-std': [14.365106724844123,\n",
       "               14.39372619398098,\n",
       "               14.342520011108425,\n",
       "               14.279057349152666,\n",
       "               14.296305647890613,\n",
       "               14.36092045544325,\n",
       "               14.48368124677651,\n",
       "               14.426581783144968,\n",
       "               14.500322587447123,\n",
       "               14.630625090954847,\n",
       "               14.677204770035718,\n",
       "               14.627597300299588,\n",
       "               14.64771853559714,\n",
       "               14.619993043583241,\n",
       "               14.639241657825615,\n",
       "               14.537388610555631,\n",
       "               14.533276830426383,\n",
       "               14.50877754500618,\n",
       "               14.447178105038747,\n",
       "               14.416016879795238,\n",
       "               14.43416071989027,\n",
       "               14.466380781329708,\n",
       "               14.522947844568897,\n",
       "               14.527321895932701,\n",
       "               14.459167926345925,\n",
       "               14.43238187947521,\n",
       "               14.432417956824874,\n",
       "               14.460786630337818,\n",
       "               14.39650277526854,\n",
       "               14.317610276970237,\n",
       "               14.298328734548807,\n",
       "               14.394217887324682,\n",
       "               14.437585604809913,\n",
       "               14.35078152747569,\n",
       "               14.41328014987109,\n",
       "               14.44821634428634,\n",
       "               14.27261357240341,\n",
       "               14.214902966358245,\n",
       "               14.230101348237618,\n",
       "               14.15593867106413,\n",
       "               13.993942735902813,\n",
       "               13.890389416720305,\n",
       "               13.772188974543495,\n",
       "               13.746726406598858,\n",
       "               13.82714949531736,\n",
       "               13.715041206562624,\n",
       "               13.672924564362422,\n",
       "               13.687943762564753,\n",
       "               13.59802102489333,\n",
       "               13.55099454629338,\n",
       "               13.584793428316587,\n",
       "               13.627608182829826,\n",
       "               13.564633319882672,\n",
       "               13.423081131531765,\n",
       "               13.352915487441663,\n",
       "               13.344680772953161,\n",
       "               13.140643741549166,\n",
       "               13.205741843798888,\n",
       "               13.139885535489261,\n",
       "               12.977288526861496,\n",
       "               12.946250104891215,\n",
       "               13.072027382113063,\n",
       "               13.080079872759768,\n",
       "               13.026333075939403,\n",
       "               12.954425548310589,\n",
       "               12.996457152955808,\n",
       "               12.960393544744973,\n",
       "               12.935504934939196,\n",
       "               12.845210620879872,\n",
       "               12.809412438222656,\n",
       "               12.807977925650993,\n",
       "               12.82600119985001,\n",
       "               12.830388981867346,\n",
       "               12.850735371657274,\n",
       "               12.922132252647064,\n",
       "               12.969485292874426,\n",
       "               12.858293582517588,\n",
       "               12.940524454636375,\n",
       "               12.836146067875665,\n",
       "               12.858716233003449,\n",
       "               12.751084584648337,\n",
       "               12.800363333433738,\n",
       "               12.733655446326456,\n",
       "               12.740742936768797,\n",
       "               12.856317371294988,\n",
       "               12.797824737085032,\n",
       "               12.832235051729137,\n",
       "               12.693804704691367,\n",
       "               12.663948422220516,\n",
       "               12.71165982553689,\n",
       "               12.59387736785551,\n",
       "               12.57419566078121,\n",
       "               12.536955916538599,\n",
       "               12.47266181346283,\n",
       "               12.504158633268114,\n",
       "               12.562673618128905,\n",
       "               12.510421791504088,\n",
       "               12.41303430950767,\n",
       "               12.36811134842303,\n",
       "               12.307415003347359,\n",
       "               12.38759136827251,\n",
       "               12.502939381519191,\n",
       "               12.486652043317433,\n",
       "               12.48656089247681,\n",
       "               12.575145082941187,\n",
       "               12.54967407894286,\n",
       "               12.660715908204462,\n",
       "               12.580733710041082,\n",
       "               12.595957306403681,\n",
       "               12.570703860203919,\n",
       "               12.611381579816724,\n",
       "               12.731809416213215,\n",
       "               12.70643600681513,\n",
       "               12.662092041618276,\n",
       "               12.596636709686384,\n",
       "               12.663975038278524,\n",
       "               12.627919588963133,\n",
       "               12.591764739243613,\n",
       "               12.673998360284193,\n",
       "               12.600782021206284,\n",
       "               12.434555074242915,\n",
       "               12.46001744198802,\n",
       "               12.521144689760666,\n",
       "               12.453907613017835,\n",
       "               12.514444129521728,\n",
       "               12.419772428340446,\n",
       "               12.340015679140528,\n",
       "               12.2880435615731,\n",
       "               12.232460965038296,\n",
       "               12.189621110731931,\n",
       "               12.004878252741618,\n",
       "               12.113485687101658,\n",
       "               12.006029825685134,\n",
       "               12.016109855838794,\n",
       "               12.09751288085514,\n",
       "               12.000923726405647,\n",
       "               12.076970457246972,\n",
       "               12.05279632889473,\n",
       "               12.122066026379931,\n",
       "               12.220682530166247,\n",
       "               12.220454682010446,\n",
       "               12.22262405542543,\n",
       "               12.206644104474888,\n",
       "               12.269710464309163,\n",
       "               12.390474912339885,\n",
       "               12.20103455327493,\n",
       "               12.256386860511146,\n",
       "               12.302612072286726,\n",
       "               12.276988913131369,\n",
       "               12.36894273035702,\n",
       "               12.656506542778054,\n",
       "               12.62283370110512,\n",
       "               12.662478480289673,\n",
       "               12.731800496366702,\n",
       "               12.662276186503009,\n",
       "               12.600026845416998,\n",
       "               12.510451006272783,\n",
       "               12.60134064658803,\n",
       "               12.41589532367233,\n",
       "               12.400649312082226,\n",
       "               12.213060644382846,\n",
       "               12.224705406943452,\n",
       "               12.242548383445039,\n",
       "               12.19866897202455,\n",
       "               12.117832879155774,\n",
       "               12.145985688865272,\n",
       "               12.158754013118488,\n",
       "               12.092091502983374,\n",
       "               12.073061303611821,\n",
       "               12.00714191586892,\n",
       "               11.955080576424681,\n",
       "               12.093714841986502,\n",
       "               12.316484547487024,\n",
       "               12.273947449030945,\n",
       "               12.229690697198585,\n",
       "               12.234584616205902,\n",
       "               12.166499925663318,\n",
       "               12.28229131077264,\n",
       "               12.350565934498858,\n",
       "               12.401348657481165,\n",
       "               12.489699381347304,\n",
       "               12.422268139742316,\n",
       "               12.312975691532035,\n",
       "               12.307442429004633,\n",
       "               12.303355626515579,\n",
       "               12.510232738120187,\n",
       "               12.47406740742124,\n",
       "               12.514789533205784,\n",
       "               12.51920913554963,\n",
       "               12.519936670754209,\n",
       "               12.49810201344052,\n",
       "               12.591475652212203,\n",
       "               12.679658743586506,\n",
       "               12.61508453650871,\n",
       "               12.656411321032452,\n",
       "               12.60829954294092,\n",
       "               12.469025772292516,\n",
       "               12.536232955279493,\n",
       "               12.538896869773517,\n",
       "               12.545324857055594,\n",
       "               12.558077378063533,\n",
       "               12.508696429374183,\n",
       "               12.53297567209776,\n",
       "               12.55567194895218,\n",
       "               12.560266402501338,\n",
       "               12.587271972129118,\n",
       "               12.586453103309799,\n",
       "               12.657665653191875,\n",
       "               12.61702936832301,\n",
       "               12.568440808973088,\n",
       "               12.550481289533648,\n",
       "               12.601948212141167,\n",
       "               12.530571256744853,\n",
       "               12.54253884879586,\n",
       "               12.513424472521363,\n",
       "               12.550082173142666,\n",
       "               12.667537239790853,\n",
       "               12.757836832532195,\n",
       "               12.882933228726053,\n",
       "               12.824891595109323,\n",
       "               12.88681338089328,\n",
       "               12.921741250018734,\n",
       "               12.979451957138842,\n",
       "               12.923025520843993,\n",
       "               12.870436437436034,\n",
       "               12.840029107442472,\n",
       "               12.883869322015471,\n",
       "               12.89710418936777,\n",
       "               12.870793597435053,\n",
       "               12.909075969351399,\n",
       "               12.959221463383098,\n",
       "               13.010537177926846,\n",
       "               13.0833216379875,\n",
       "               13.02079375983113,\n",
       "               13.018676165147804,\n",
       "               13.105885220374985,\n",
       "               13.083520121214873,\n",
       "               13.016914988239908,\n",
       "               13.036413608247408,\n",
       "               12.982871055019181,\n",
       "               12.96971794152113,\n",
       "               12.91309676554777,\n",
       "               12.935123671872,\n",
       "               12.882082676958126,\n",
       "               12.95743386424439,\n",
       "               12.921815388079978,\n",
       "               12.838676764476906,\n",
       "               12.856781319441343,\n",
       "               12.859256951394638,\n",
       "               12.814038748680751,\n",
       "               12.967637456977888,\n",
       "               12.937865799665829,\n",
       "               12.894446640300696,\n",
       "               12.989657220407121,\n",
       "               13.056465718549186,\n",
       "               13.102929732865627,\n",
       "               13.1087512478537,\n",
       "               13.174907987484234,\n",
       "               13.268962705956165,\n",
       "               13.194404511932454,\n",
       "               13.171443398946803,\n",
       "               13.25521195566755,\n",
       "               13.23398012381095,\n",
       "               13.249247813101771,\n",
       "               13.220206263355868,\n",
       "               13.175956234933182,\n",
       "               13.257733861909681,\n",
       "               13.241718868046599,\n",
       "               13.284404915370414,\n",
       "               13.357131022349243,\n",
       "               13.285410132030613,\n",
       "               13.346286270477957,\n",
       "               13.341518704072142,\n",
       "               13.497310025479814,\n",
       "               13.51788340231396,\n",
       "               13.458970556116821,\n",
       "               13.567486160674555,\n",
       "               13.533821177939082,\n",
       "               13.489469544087518,\n",
       "               13.524558555593972,\n",
       "               13.484558640140651,\n",
       "               13.341979072341273,\n",
       "               13.369886900624952,\n",
       "               13.33759755048481,\n",
       "               13.372541525799209,\n",
       "               13.531828772845564,\n",
       "               13.525685537169744,\n",
       "               13.50144912754075,\n",
       "               13.414479328577354,\n",
       "               13.371317439169845,\n",
       "               13.299257727040182,\n",
       "               13.25421595020502,\n",
       "               13.257921162961313,\n",
       "               13.294036604166353,\n",
       "               13.318316990137307,\n",
       "               13.230606760921303,\n",
       "               13.202942469664732,\n",
       "               13.341217496802365,\n",
       "               13.276864025161885,\n",
       "               13.265491247633536,\n",
       "               13.07354634520055,\n",
       "               12.956325136487527,\n",
       "               13.016321584731697,\n",
       "               12.971745199259729,\n",
       "               12.943902727918116,\n",
       "               12.921925773122693,\n",
       "               12.919851696551648,\n",
       "               12.850496286010523,\n",
       "               12.893125917466653,\n",
       "               12.89633860760445,\n",
       "               12.893178387336077,\n",
       "               13.025071735666558,\n",
       "               12.845431076096181,\n",
       "               12.847450247257612,\n",
       "               12.779389134626445,\n",
       "               12.772820816525838,\n",
       "               12.821750467945076,\n",
       "               12.761083110223002,\n",
       "               12.856238314445479,\n",
       "               12.905364047779994,\n",
       "               12.813702907040893,\n",
       "               12.840489769821161,\n",
       "               12.794296708150654,\n",
       "               12.83240831325791,\n",
       "               12.827856215704887,\n",
       "               12.779292738973396,\n",
       "               12.846076963927624,\n",
       "               12.793090765132046,\n",
       "               12.876000190178672,\n",
       "               12.766631873560984,\n",
       "               12.702476228167345,\n",
       "               12.647897197832656,\n",
       "               12.695075924593727,\n",
       "               12.794422049941678,\n",
       "               12.813587408050982,\n",
       "               12.728706056752978,\n",
       "               12.75278853078065,\n",
       "               12.799482557405796,\n",
       "               12.795841696026521,\n",
       "               12.887274117590511,\n",
       "               12.969718825941658,\n",
       "               12.84888122764148,\n",
       "               12.88508270683033,\n",
       "               12.904538340018977,\n",
       "               12.91719015562957,\n",
       "               12.80714264817665,\n",
       "               12.787946335585989,\n",
       "               12.870796709408902,\n",
       "               12.891371941919308,\n",
       "               12.935597965818422,\n",
       "               12.923575535448771,\n",
       "               13.09608272622336,\n",
       "               13.012259575079128,\n",
       "               12.968817997996348,\n",
       "               12.963618889738735,\n",
       "               12.938237665440601,\n",
       "               13.085914534164036,\n",
       "               13.101303824592769,\n",
       "               13.102657960647296,\n",
       "               13.100635574763027,\n",
       "               13.107631343065707,\n",
       "               13.182148284544581,\n",
       "               13.19143871803005,\n",
       "               13.124173531305788,\n",
       "               13.09877182315688,\n",
       "               13.072788226837304,\n",
       "               13.266908474966609,\n",
       "               13.185864984632815,\n",
       "               13.18558179425086,\n",
       "               13.205093767609869,\n",
       "               13.272372649793274,\n",
       "               13.178870027304587,\n",
       "               13.281386799945505,\n",
       "               13.332960400883175,\n",
       "               13.341175662885108,\n",
       "               13.31482355937585,\n",
       "               13.330533742485327,\n",
       "               13.376935952738108,\n",
       "               13.352826511659845,\n",
       "               13.40326913091484,\n",
       "               13.421975646390905,\n",
       "               13.431162800797356,\n",
       "               13.440463937273844,\n",
       "               13.40181889386921,\n",
       "               13.442591565632409,\n",
       "               13.384345806076189,\n",
       "               13.467381301414083,\n",
       "               13.378974938772163,\n",
       "               13.332883520344597,\n",
       "               13.352125822139763,\n",
       "               13.364338816777352,\n",
       "               13.402163127956571,\n",
       "               13.444422196938222,\n",
       "               13.469898632214012,\n",
       "               13.548405331360598,\n",
       "               13.652494976043599,\n",
       "               13.501380664993388,\n",
       "               13.541315820607428,\n",
       "               13.644583181325768,\n",
       "               13.66545011725059,\n",
       "               13.699865942119022,\n",
       "               13.63746023465979,\n",
       "               13.596301827886974,\n",
       "               13.56818787725281,\n",
       "               13.65524044155324,\n",
       "               13.6623171320945,\n",
       "               13.625856175390108,\n",
       "               13.651151675801497,\n",
       "               13.65309790623912,\n",
       "               13.674049535920359,\n",
       "               13.672666893286399,\n",
       "               13.665711448832305,\n",
       "               13.773880548534143,\n",
       "               13.784960111138512,\n",
       "               13.74935411420668,\n",
       "               13.726307020499279,\n",
       "               13.759651582532697,\n",
       "               13.727895380014736,\n",
       "               13.710185043860555,\n",
       "               13.79074367067194,\n",
       "               13.875193080721044,\n",
       "               13.790962251557437,\n",
       "               13.7449782892876,\n",
       "               13.841082082075465,\n",
       "               13.89896764306347,\n",
       "               13.854692718125966,\n",
       "               13.755876870994708,\n",
       "               13.747982353763852,\n",
       "               13.716917670709938,\n",
       "               13.765275879890769,\n",
       "               13.755023301626661,\n",
       "               13.751043843621678,\n",
       "               13.696625826603588,\n",
       "               13.808633233454442,\n",
       "               13.82333725225774,\n",
       "               13.825051950978029,\n",
       "               13.76322407210272,\n",
       "               13.712834716300465,\n",
       "               13.587804069324742,\n",
       "               13.5688331140676,\n",
       "               13.462619695643207,\n",
       "               13.460506465749638,\n",
       "               13.417680139146835,\n",
       "               13.3877284959343,\n",
       "               13.380205691745978,\n",
       "               13.42999154603885,\n",
       "               13.462448326475492,\n",
       "               13.507466721109829,\n",
       "               13.533640012723906,\n",
       "               13.558019614625243,\n",
       "               13.515170904316868,\n",
       "               13.463503764461612,\n",
       "               13.508107006934956,\n",
       "               13.566790389009906,\n",
       "               13.503709616929712,\n",
       "               13.544141983835958,\n",
       "               13.525694959274718,\n",
       "               13.475608565214367,\n",
       "               13.482314898305805,\n",
       "               13.468351391179606,\n",
       "               13.436495729336478,\n",
       "               13.424104553394766,\n",
       "               13.484302375152234,\n",
       "               13.592178975615813,\n",
       "               13.564617774641095,\n",
       "               13.58101802743444,\n",
       "               13.553549994394142,\n",
       "               13.586939664119571,\n",
       "               13.559449882447309,\n",
       "               13.560084409182767,\n",
       "               13.58087296690358,\n",
       "               13.584152695555963,\n",
       "               13.594421473833432,\n",
       "               13.645221532223305,\n",
       "               13.613488919228955,\n",
       "               13.536007375561129,\n",
       "               13.621473457716615,\n",
       "               13.605965779081236,\n",
       "               13.612530227500125,\n",
       "               13.648320581184665,\n",
       "               13.735389081736797,\n",
       "               13.704583019206073,\n",
       "               13.706629402322298,\n",
       "               13.692431750134366,\n",
       "               13.641313669075851,\n",
       "               13.678546283444948,\n",
       "               13.599658463351668,\n",
       "               13.605717557961047,\n",
       "               13.638471838970501,\n",
       "               13.649075174382652,\n",
       "               13.628765804032657,\n",
       "               13.65708427322899,\n",
       "               13.699202591762306,\n",
       "               13.681304506058426,\n",
       "               13.689606583517687,\n",
       "               13.76928028366608,\n",
       "               13.788191452216617,\n",
       "               13.856147213228487,\n",
       "               13.872547884154613,\n",
       "               13.891792158591867,\n",
       "               13.83149800670089,\n",
       "               13.823355647355935,\n",
       "               13.829799223260204,\n",
       "               13.811246134001072,\n",
       "               13.801364302135973,\n",
       "               13.890541009882389,\n",
       "               13.934392691941065,\n",
       "               13.9122687174653,\n",
       "               13.95252211358861,\n",
       "               14.008770374518589,\n",
       "               14.034889189683108,\n",
       "               13.960221333814316,\n",
       "               13.986247396408569,\n",
       "               13.960756217399767,\n",
       "               13.967370831556963,\n",
       "               14.053941433927488,\n",
       "               14.037426816110656,\n",
       "               13.99749472807472,\n",
       "               14.013825832489847,\n",
       "               13.976774392651397,\n",
       "               13.947342487043565,\n",
       "               13.91975097015745,\n",
       "               13.878417264862351,\n",
       "               13.869971753575504,\n",
       "               13.85808500562019,\n",
       "               13.850014416684573,\n",
       "               13.862949938497703,\n",
       "               13.904750135576908,\n",
       "               13.873046283176471,\n",
       "               13.872130121760584,\n",
       "               13.908511419271425,\n",
       "               13.889772679982585,\n",
       "               13.91804876942727,\n",
       "               13.955686868278649,\n",
       "               13.953594053677389,\n",
       "               13.951035882911992,\n",
       "               13.9915541969055,\n",
       "               13.934957781215031,\n",
       "               13.93427223778722,\n",
       "               13.95754838791037,\n",
       "               14.059142206073432,\n",
       "               14.090091158508418,\n",
       "               14.078788849768626,\n",
       "               14.12819507934366,\n",
       "               14.101966770665236,\n",
       "               14.200567335764847,\n",
       "               14.174306351193014,\n",
       "               14.216015491359896,\n",
       "               14.185762515403429,\n",
       "               14.108126713367936,\n",
       "               14.120275522838591,\n",
       "               14.07536962095448,\n",
       "               14.151978791768105,\n",
       "               14.193550946890584,\n",
       "               14.265619430526447,\n",
       "               14.28596719446989,\n",
       "               14.351828662094468,\n",
       "               14.369389126888855,\n",
       "               14.398058799291519,\n",
       "               14.439914850379218,\n",
       "               14.496149760536214,\n",
       "               14.438209635544586,\n",
       "               14.515579645714235,\n",
       "               14.568662339872198,\n",
       "               14.566483322301124,\n",
       "               14.646272799092367,\n",
       "               14.61477822291366,\n",
       "               14.69694734761815,\n",
       "               14.723135366349652,\n",
       "               14.681766402075805,\n",
       "               14.69417355065213,\n",
       "               14.652273855203532,\n",
       "               14.62956714452408,\n",
       "               14.693378957243876,\n",
       "               14.684851278092735,\n",
       "               14.760553719723015,\n",
       "               14.75802581732007,\n",
       "               14.809784538853005,\n",
       "               14.745284786870355,\n",
       "               14.71328620103691,\n",
       "               14.668477899173263,\n",
       "               14.67013068395128,\n",
       "               14.703377527567538,\n",
       "               14.690310516046296,\n",
       "               14.735577673432392,\n",
       "               14.752253576260673,\n",
       "               14.792882194830689,\n",
       "               14.789870007639443,\n",
       "               14.816048789626889,\n",
       "               14.859342146515571,\n",
       "               14.872837312508183,\n",
       "               14.91190465412182,\n",
       "               14.848357553705668,\n",
       "               14.857480242280953,\n",
       "               14.843013699295577,\n",
       "               14.833721579503754,\n",
       "               14.799507324140123,\n",
       "               14.826450854015937,\n",
       "               14.750157673459212,\n",
       "               14.794286832420449,\n",
       "               14.816845961402995,\n",
       "               14.814999843329796,\n",
       "               14.842871558997288,\n",
       "               14.827864155765292,\n",
       "               14.902807747994785,\n",
       "               14.932248388476983,\n",
       "               14.968600196550453,\n",
       "               14.966214346461392,\n",
       "               14.924533922854355,\n",
       "               14.964267656984985,\n",
       "               14.929270379115135,\n",
       "               14.925759442651078,\n",
       "               14.97397497244415,\n",
       "               14.946424866772443,\n",
       "               14.993044047105526,\n",
       "               14.978111996545454,\n",
       "               15.016889780980948,\n",
       "               14.999104852669287,\n",
       "               14.934373060694963,\n",
       "               14.932888125323025,\n",
       "               14.978543051207552,\n",
       "               15.005224324725061,\n",
       "               15.026529421285153,\n",
       "               15.1287100801491,\n",
       "               15.211981202479684,\n",
       "               15.1777581071578,\n",
       "               15.329706764773324,\n",
       "               15.344088596487438,\n",
       "               15.370464628776434,\n",
       "               15.398214438307798,\n",
       "               15.434150810843338,\n",
       "               15.401862223275808,\n",
       "               15.40678547401679,\n",
       "               15.451537892950599,\n",
       "               15.35747448323874,\n",
       "               15.38381165415298,\n",
       "               15.343021094441124,\n",
       "               15.208092616348353,\n",
       "               15.18082735287673,\n",
       "               15.217243415047783,\n",
       "               15.196658822397076,\n",
       "               15.218787734989727,\n",
       "               15.25664484980355,\n",
       "               15.255897621745214,\n",
       "               15.207343459329191,\n",
       "               15.18640149307799,\n",
       "               15.10864410133933,\n",
       "               15.071087927983765,\n",
       "               15.090024599030938,\n",
       "               15.024480584188126,\n",
       "               15.038573915797683,\n",
       "               15.03914762628704,\n",
       "               15.01761168253617,\n",
       "               15.00542743964284,\n",
       "               14.909537188814982,\n",
       "               14.956287456313943,\n",
       "               14.907199147391598,\n",
       "               14.92032065209549,\n",
       "               14.905375351683801,\n",
       "               14.884917271470627,\n",
       "               14.957554268936653,\n",
       "               14.914700130813387,\n",
       "               14.852057219359713,\n",
       "               14.749740245302261,\n",
       "               14.830177918017183,\n",
       "               14.968548783923302,\n",
       "               14.980417763047184,\n",
       "               14.93127567399917,\n",
       "               14.943286408225635,\n",
       "               14.979311274574957,\n",
       "               14.982193373385071,\n",
       "               14.990817181600864,\n",
       "               14.968368154092875,\n",
       "               14.98924426429768,\n",
       "               15.015292425936575,\n",
       "               15.079446912334705,\n",
       "               15.125053615627378,\n",
       "               15.221832873196393,\n",
       "               15.09769016679176,\n",
       "               15.15023779826829,\n",
       "               15.107492151989582,\n",
       "               15.058541419546716,\n",
       "               15.071502770154876,\n",
       "               15.115367224931566,\n",
       "               15.066547320008906,\n",
       "               15.073895657792994,\n",
       "               15.12860019528943,\n",
       "               15.11696522973684,\n",
       "               15.176959056420724,\n",
       "               15.20010011594154,\n",
       "               15.17213231171049,\n",
       "               15.215302370043494,\n",
       "               15.228397547601153,\n",
       "               15.218787557055851,\n",
       "               15.332697654343145,\n",
       "               15.284849817572345,\n",
       "               15.256925448952686,\n",
       "               15.276797023049546,\n",
       "               15.36650354190784,\n",
       "               15.413517246477532,\n",
       "               15.493528494539248,\n",
       "               15.577519506645668,\n",
       "               15.651808308236847,\n",
       "               15.696725861770153,\n",
       "               15.745398424287885,\n",
       "               15.733955244547925,\n",
       "               15.718873909926465,\n",
       "               15.774054328428091,\n",
       "               15.776285673761675,\n",
       "               15.794685252634673,\n",
       "               15.754554135658088,\n",
       "               15.79386463821103,\n",
       "               15.78735654854742,\n",
       "               15.837416978254696,\n",
       "               15.987429722201284,\n",
       "               15.948619213521791,\n",
       "               15.844550830371627,\n",
       "               15.863619959839458,\n",
       "               15.84549792998748,\n",
       "               15.905334474006125,\n",
       "               15.88604349480304,\n",
       "               15.892690754022093,\n",
       "               15.889270403658625,\n",
       "               15.838617923360147,\n",
       "               15.801561876075855,\n",
       "               15.746141266762379,\n",
       "               15.79922715132634,\n",
       "               15.82533588276243,\n",
       "               15.8037238114666,\n",
       "               15.83714341247171,\n",
       "               15.824647044757377,\n",
       "               15.811132428629096,\n",
       "               15.829896257888521,\n",
       "               15.870338657344478,\n",
       "               15.920308567278557,\n",
       "               15.933719043772024,\n",
       "               15.907260537260957,\n",
       "               15.819292918889602,\n",
       "               15.8764657696783,\n",
       "               15.874644740865007,\n",
       "               15.844056893870944,\n",
       "               15.772036772880156,\n",
       "               15.772249469403512,\n",
       "               15.793852672011207,\n",
       "               15.806597670050639,\n",
       "               15.802248492932591,\n",
       "               15.795818187521824,\n",
       "               15.81676565291278,\n",
       "               15.829810842621361,\n",
       "               15.799268714851406,\n",
       "               15.790343064012104,\n",
       "               15.800161642008344,\n",
       "               15.868762209181677,\n",
       "               15.888520104849784,\n",
       "               15.918171994943693,\n",
       "               15.921637098732182,\n",
       "               15.88197533315605,\n",
       "               15.868101095019512,\n",
       "               15.851035622780728,\n",
       "               15.830584452607734,\n",
       "               15.843853251669252,\n",
       "               15.81860657621051,\n",
       "               15.814988542803988,\n",
       "               15.789815925574098,\n",
       "               15.804743306816855,\n",
       "               15.81931959475056,\n",
       "               15.836617283834645,\n",
       "               15.80599426564153,\n",
       "               15.816286690953055,\n",
       "               15.77304992763926,\n",
       "               15.829311177483115,\n",
       "               15.912105363610273,\n",
       "               15.871838276759814,\n",
       "               15.893246508616198,\n",
       "               15.844104338722758,\n",
       "               15.863506441416526,\n",
       "               15.829831867323394,\n",
       "               15.818171090417371,\n",
       "               15.826248995679803,\n",
       "               15.861360647101074,\n",
       "               15.803118025505945,\n",
       "               15.760437671024313,\n",
       "               15.747417911410356,\n",
       "               15.720039051283337,\n",
       "               15.66857098833341,\n",
       "               15.716303685285228,\n",
       "               15.726621902843632,\n",
       "               15.673205907735253,\n",
       "               15.681564403814182,\n",
       "               15.696789734178113,\n",
       "               15.643266327374866,\n",
       "               15.625623059818244,\n",
       "               15.61023515991114,\n",
       "               15.62110537330292,\n",
       "               15.66726752091084,\n",
       "               15.675896612156263,\n",
       "               15.684635343359277,\n",
       "               15.633435598013918,\n",
       "               15.62224621648288,\n",
       "               15.59631836372049,\n",
       "               15.64236296668423,\n",
       "               15.619263441836203,\n",
       "               15.518386107637639,\n",
       "               15.419876581970685,\n",
       "               15.369868410053416,\n",
       "               15.361874084522306,\n",
       "               15.382904217244066,\n",
       "               15.332105714202699,\n",
       "               15.29988689299541,\n",
       "               15.322130045537408,\n",
       "               15.24250202245441,\n",
       "               15.215297674104006,\n",
       "               15.165530810964341,\n",
       "               15.150610440993097,\n",
       "               15.159092798443988,\n",
       "               15.10753271177085,\n",
       "               15.077654450955926,\n",
       "               15.083168666252313,\n",
       "               15.076913168429908,\n",
       "               15.031009109657338,\n",
       "               15.02038872827998,\n",
       "               14.993764412743271,\n",
       "               15.035891026039918,\n",
       "               15.041832447114224,\n",
       "               15.053648665471826,\n",
       "               15.037810243546101,\n",
       "               15.016840615592423,\n",
       "               15.084669678399264,\n",
       "               15.07360090260906,\n",
       "               15.034712483566441,\n",
       "               15.05771691390061,\n",
       "               15.074916068099823,\n",
       "               15.13215487039931,\n",
       "               15.114199510829422,\n",
       "               15.070928287360166,\n",
       "               15.042259374234217,\n",
       "               14.997162862858353,\n",
       "               15.015041702410619,\n",
       "               15.017777792379043,\n",
       "               14.992116074916243,\n",
       "               14.977679077995486,\n",
       "               14.969686516250471,\n",
       "               14.915924133764063,\n",
       "               14.941691607397862,\n",
       "               14.947627587050642,\n",
       "               14.966618550069759,\n",
       "               14.95597123225319,\n",
       "               14.989336594713963,\n",
       "               14.959968042325078,\n",
       "               14.94933254236174,\n",
       "               14.961867965378085,\n",
       "               14.960890701804686,\n",
       "               14.949731466445863,\n",
       "               14.9458068353357,\n",
       "               14.985220884807413,\n",
       "               14.97780029580866,\n",
       "               14.963393806899864,\n",
       "               14.892570530432632,\n",
       "               14.866868830588471,\n",
       "               14.86107089067422,\n",
       "               14.885077608029023,\n",
       "               14.919976814716534,\n",
       "               14.851125745122847,\n",
       "               14.837952513426506,\n",
       "               14.810838341164198,\n",
       "               14.824863831904558,\n",
       "               14.795210931195111,\n",
       "               14.796438963083435,\n",
       "               14.783046655316666,\n",
       "               14.82284525787494,\n",
       "               14.825967906427232,\n",
       "               14.815442506774174,\n",
       "               14.862920982717228,\n",
       "               14.786557797075892,\n",
       "               14.78365175223688,\n",
       "               14.734556530752199,\n",
       "               14.683410175833256,\n",
       "               14.694533946270411,\n",
       "               14.644384322990119,\n",
       "               14.65662314661773,\n",
       "               14.681906401805103,\n",
       "               14.69976770520836,\n",
       "               14.719507516609577,\n",
       "               14.631791374043003,\n",
       "               14.56633745426041,\n",
       "               14.589962353224786,\n",
       "               14.617671823497389,\n",
       "               14.66750346473205,\n",
       "               14.68535026222992,\n",
       "               14.682728883550896,\n",
       "               14.665730986784606,\n",
       "               14.642335231405703,\n",
       "               14.619093172864341,\n",
       "               14.533681773448144,\n",
       "               14.49419986512148,\n",
       "               14.422678112377179,\n",
       "               14.405017096494099,\n",
       "               14.32926976265667,\n",
       "               14.32878685449461,\n",
       "               14.30479920982163,\n",
       "               14.373132117449526,\n",
       "               14.330133543911055,\n",
       "               14.335738574827324,\n",
       "               14.367634699493097,\n",
       "               14.345605649269052,\n",
       "               14.322545144448148,\n",
       "               14.347239921328466,\n",
       "               14.369172658882933,\n",
       "               14.339512781579932,\n",
       "               14.315724643398273,\n",
       "               14.284216624590506,\n",
       "               14.28900855328143,\n",
       "               14.291315527884684,\n",
       "               14.198874282907534,\n",
       "               14.159256200127206,\n",
       "               14.161179504879915,\n",
       "               14.138930859908122,\n",
       "               14.108180935772902,\n",
       "               14.093143477545203,\n",
       "               14.130618366836524,\n",
       "               14.119717283220082,\n",
       "               14.131504203901553,\n",
       "               14.138165312952648,\n",
       "               14.132984612895822,\n",
       "               14.120436202967356,\n",
       "               14.108168009466374,\n",
       "               14.145839196034842,\n",
       "               14.172543076243937,\n",
       "               14.161085962682439,\n",
       "               14.154518402242982,\n",
       "               14.141160876972112,\n",
       "               14.171891255648877,\n",
       "               14.164658362895741,\n",
       "               14.2364938868939,\n",
       "               14.317064651505854,\n",
       "               14.245351203655845,\n",
       "               14.229506624272316,\n",
       "               14.203557337615601,\n",
       "               14.154043600572464,\n",
       "               14.112860413378295,\n",
       "               14.125258425219366,\n",
       "               14.103218000243178,\n",
       "               14.146973949597967,\n",
       "               14.157937159523534,\n",
       "               14.127734279956734,\n",
       "               14.090780954243915,\n",
       "               14.053455155402279,\n",
       "               14.127256811005575,\n",
       "               14.08156952279169,\n",
       "               14.085161459208962,\n",
       "               14.11822737545608,\n",
       "               14.07061823471077,\n",
       "               14.095040139615907,\n",
       "               14.063493734455113,\n",
       "               14.029384691837606,\n",
       "               14.056334266257702,\n",
       "               14.102148539247779,\n",
       "               14.04545064114686,\n",
       "               14.015852838527547,\n",
       "               14.019686144573898,\n",
       "               13.993936883462231,\n",
       "               13.997620246407083,\n",
       "               13.999578389422233,\n",
       "               14.004311508002507,\n",
       "               13.985641400612007,\n",
       "               14.045775630213239,\n",
       "               14.087850383155242,\n",
       "               14.092451203252846,\n",
       "               14.096495395685517,\n",
       "               14.096432841892062,\n",
       "               14.088011116062836,\n",
       "               14.069222353761461,\n",
       "               14.037275852331831,\n",
       "               14.021586972642083,\n",
       "               14.05988773668936,\n",
       "               13.99252978101132,\n",
       "               13.924514091772274,\n",
       "               13.915086511000066,\n",
       "               13.90114777786132,\n",
       "               13.948852139252818,\n",
       "               13.963072333298944,\n",
       "               13.93007067767439,\n",
       "               13.92804251297994,\n",
       "               13.94044693211073,\n",
       "               13.948629357962238,\n",
       "               13.955132027150661,\n",
       "               13.921458550853357,\n",
       "               13.933723919109728,\n",
       "               13.95080962433569,\n",
       "               13.958196221085684,\n",
       "               13.953461105856594,\n",
       "               13.951600051873589,\n",
       "               13.909336275583161,\n",
       "               13.836531898630506,\n",
       "               13.910915225764326,\n",
       "               13.896192074016376,\n",
       "               13.876166865540204,\n",
       "               13.878041201804463,\n",
       "               13.873892170288753,\n",
       "               13.8578867826247]})}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative RMSLE in train: 0.8193828353501815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n",
      "  return 1 - np.sqrt(np.square(np.log10(y_pred +1) - np.log10(y_true +1)).mean())\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pool_train = Pool(X_train, y_train,\n",
    "                  cat_features = ['Genre', 'BookCategory', 'Authors', 'Print', 'Type'])\n",
    "\n",
    "pool_test = Pool(X_test, cat_features = ['Genre', 'BookCategory', 'Authors', 'Print', 'Type'])\n",
    "\n",
    "\n",
    "cb = CatBoostRegressor(loss_function='RMSE',\n",
    "                       eval_metric=NegativeRMSLEMetric(),\n",
    "                       task_type='CPU',\n",
    "                       random_state=random_state,\n",
    "                       verbose=False)\n",
    "\n",
    "grid = {'learning_rate': [0.025, 0.05],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'iterations': [500, 1000]}\n",
    "\n",
    "grid_search_results = cb.grid_search(grid, X=pool_train, cv=10,\n",
    "               partition_random_seed=random_state, verbose=True, plot=True)\n",
    "\n",
    "y_pred = cb.predict(pool_train)\n",
    "cb_nrmsle = nrmsle(y_train, y_pred)\n",
    "print(\"Negative RMSLE in train:\", cb_nrmsle)\n",
    "\n",
    "# Calculate the predictions of the submissions\n",
    "y_pred = cb.predict(pool_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specified parameters: \n",
      "\n",
      "loss_function :   RMSE\n",
      "verbose :   False\n",
      "eval_metric :   <__main__.NegativeRMSLEMetric object at 0x7f359df4d040>\n",
      "task_type :   CPU\n",
      "random_state :   420\n",
      "depth :   10\n",
      "iterations :   1000\n",
      "learning_rate :   0.025\n",
      "\n",
      "All parameters: \n",
      "\n",
      "nan_mode :   Min\n",
      "eval_metric :   PythonUserDefinedPerObject\n",
      "combinations_ctr :   ['Borders:CtrBorderCount=15:CtrBorderType=Uniform:TargetBorderCount=1:TargetBorderType=MinEntropy:Prior=0/1:Prior=0.5/1:Prior=1/1', 'Counter:CtrBorderCount=15:CtrBorderType=Uniform:Prior=0/1']\n",
      "iterations :   1000\n",
      "sampling_frequency :   PerTree\n",
      "fold_permutation_block :   0\n",
      "leaf_estimation_method :   Newton\n",
      "counter_calc_method :   SkipTest\n",
      "grow_policy :   SymmetricTree\n",
      "penalties_coefficient :   1\n",
      "boosting_type :   Plain\n",
      "model_shrink_mode :   Constant\n",
      "feature_border_type :   GreedyLogSum\n",
      "ctr_leaf_count_limit :   18446744073709551615\n",
      "bayesian_matrix_reg :   0.10000000149011612\n",
      "one_hot_max_size :   2\n",
      "force_unit_auto_pair_weights :   False\n",
      "l2_leaf_reg :   3\n",
      "random_strength :   1\n",
      "rsm :   1\n",
      "boost_from_average :   True\n",
      "max_ctr_complexity :   4\n",
      "model_size_reg :   0.5\n",
      "simple_ctr :   ['Borders:CtrBorderCount=15:CtrBorderType=Uniform:TargetBorderCount=1:TargetBorderType=MinEntropy:Prior=0/1:Prior=0.5/1:Prior=1/1', 'Counter:CtrBorderCount=15:CtrBorderType=Uniform:Prior=0/1']\n",
      "pool_metainfo_options :   {'tags': {}}\n",
      "subsample :   0.800000011920929\n",
      "use_best_model :   False\n",
      "random_seed :   420\n",
      "depth :   10\n",
      "ctr_target_border_count :   1\n",
      "posterior_sampling :   False\n",
      "has_time :   False\n",
      "store_all_simple_ctr :   False\n",
      "border_count :   254\n",
      "classes_count :   0\n",
      "auto_class_weights :   None\n",
      "sparse_features_conflict_fraction :   0\n",
      "leaf_estimation_backtracking :   AnyImprovement\n",
      "best_model_min_trees :   1\n",
      "model_shrink_rate :   0\n",
      "min_data_in_leaf :   1\n",
      "loss_function :   RMSE\n",
      "learning_rate :   0.02500000037252903\n",
      "score_function :   Cosine\n",
      "task_type :   CPU\n",
      "leaf_estimation_iterations :   1\n",
      "bootstrap_type :   MVS\n",
      "max_leaves :   1024\n",
      "permutation_count :   4\n"
     ]
    }
   ],
   "source": [
    "print(\"Specified best parameters: \\n\")\n",
    "for param, val in cb.get_params().items():\n",
    "    print(param, \":  \", val)\n",
    "    \n",
    "#print(cb.get_best_iteration())  \n",
    "# print(\"\\nAll parameters: \\n\")\n",
    "# for param, val in cb.get_all_params().items():\n",
    "#     print(param, \":  \", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run model with the best scores with 10-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6f920eaa5746aa8f3f201a0e3f321c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/catboost/core.py:6201: UserWarning: Failed to import numba for optimizing custom metrics and objectives\n",
      "  return _cv(params, pool, fold_count, inverted, partition_random_seed, shuffle, stratified,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold [0/10]\n",
      "\n",
      "bestTest = 0.7305993576\n",
      "bestIteration = 998\n",
      "\n",
      "Training on fold [1/10]\n",
      "\n",
      "bestTest = 0.7422871261\n",
      "bestIteration = 997\n",
      "\n",
      "Training on fold [2/10]\n",
      "\n",
      "bestTest = 0.7310382333\n",
      "bestIteration = 995\n",
      "\n",
      "Training on fold [3/10]\n",
      "\n",
      "bestTest = 0.7491836925\n",
      "bestIteration = 999\n",
      "\n",
      "Training on fold [4/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6634/1355284671.py:27: RuntimeWarning: invalid value encountered in log10\n",
      "  error_sum += w * ((np.log10(approx[i]+1)  - np.log10(target[i]+1))**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bestTest = 0.7328288986\n",
      "bestIteration = 995\n",
      "\n",
      "Training on fold [5/10]\n",
      "\n",
      "bestTest = 0.730501842\n",
      "bestIteration = 909\n",
      "\n",
      "Training on fold [6/10]\n",
      "\n",
      "bestTest = 0.7484076332\n",
      "bestIteration = 802\n",
      "\n",
      "Training on fold [7/10]\n",
      "\n",
      "bestTest = 0.7353307372\n",
      "bestIteration = 673\n",
      "\n",
      "Training on fold [8/10]\n",
      "\n",
      "bestTest = 0.7456644867\n",
      "bestIteration = 842\n",
      "\n",
      "Training on fold [9/10]\n",
      "\n",
      "bestTest = 0.7515430163\n",
      "bestIteration = 998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "\n",
    "# 10-fold CV\n",
    "scores = cv(pool_train,\n",
    "            cb.get_params(),\n",
    "            fold_count=10, \n",
    "            plot=\"True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the mean score of the left out folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterations</th>\n",
       "      <th>test-NegativeRMSLEMetric-mean</th>\n",
       "      <th>test-NegativeRMSLEMetric-std</th>\n",
       "      <th>train-NegativeRMSLEMetric-mean</th>\n",
       "      <th>train-NegativeRMSLEMetric-std</th>\n",
       "      <th>test-RMSE-mean</th>\n",
       "      <th>test-RMSE-std</th>\n",
       "      <th>train-RMSE-mean</th>\n",
       "      <th>train-RMSE-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>0.739431</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.528826</td>\n",
       "      <td>115.376442</td>\n",
       "      <td>299.159908</td>\n",
       "      <td>9.827738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     iterations  test-NegativeRMSLEMetric-mean  test-NegativeRMSLEMetric-std  \\\n",
       "999         999                       0.739431                      0.008511   \n",
       "\n",
       "     train-NegativeRMSLEMetric-mean  train-NegativeRMSLEMetric-std  \\\n",
       "999                             NaN                            NaN   \n",
       "\n",
       "     test-RMSE-mean  test-RMSE-std  train-RMSE-mean  train-RMSE-std  \n",
       "999      571.528826     115.376442       299.159908        9.827738  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.sort_values(by=['test-NegativeRMSLEMetric-mean'], ascending=False)[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean score by iteration of the boosting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterations</th>\n",
       "      <th>test-NegativeRMSLEMetric-mean</th>\n",
       "      <th>test-NegativeRMSLEMetric-std</th>\n",
       "      <th>train-NegativeRMSLEMetric-mean</th>\n",
       "      <th>train-NegativeRMSLEMetric-std</th>\n",
       "      <th>test-RMSE-mean</th>\n",
       "      <th>test-RMSE-std</th>\n",
       "      <th>train-RMSE-mean</th>\n",
       "      <th>train-RMSE-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>0.739431</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.528826</td>\n",
       "      <td>115.376442</td>\n",
       "      <td>299.159908</td>\n",
       "      <td>9.827738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>0.739419</td>\n",
       "      <td>0.008485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.542563</td>\n",
       "      <td>115.386807</td>\n",
       "      <td>299.296800</td>\n",
       "      <td>9.822228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>0.008467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.555286</td>\n",
       "      <td>115.387938</td>\n",
       "      <td>299.445896</td>\n",
       "      <td>9.822090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>0.739408</td>\n",
       "      <td>0.008449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.592188</td>\n",
       "      <td>115.390606</td>\n",
       "      <td>299.617052</td>\n",
       "      <td>9.813014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>0.739408</td>\n",
       "      <td>0.008435</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.604382</td>\n",
       "      <td>115.402112</td>\n",
       "      <td>299.836769</td>\n",
       "      <td>9.805539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>994</td>\n",
       "      <td>0.739407</td>\n",
       "      <td>0.008436</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.605238</td>\n",
       "      <td>115.403926</td>\n",
       "      <td>299.982154</td>\n",
       "      <td>9.808731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>993</td>\n",
       "      <td>0.739391</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.629053</td>\n",
       "      <td>115.390657</td>\n",
       "      <td>300.102759</td>\n",
       "      <td>9.777186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>992</td>\n",
       "      <td>0.739367</td>\n",
       "      <td>0.008398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.673455</td>\n",
       "      <td>115.398457</td>\n",
       "      <td>300.270355</td>\n",
       "      <td>9.757627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>988</td>\n",
       "      <td>0.739351</td>\n",
       "      <td>0.008414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.669884</td>\n",
       "      <td>115.367997</td>\n",
       "      <td>300.862691</td>\n",
       "      <td>9.837485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>987</td>\n",
       "      <td>0.739350</td>\n",
       "      <td>0.008406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.686657</td>\n",
       "      <td>115.329523</td>\n",
       "      <td>301.008496</td>\n",
       "      <td>9.882409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>978</td>\n",
       "      <td>0.739350</td>\n",
       "      <td>0.008405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.753019</td>\n",
       "      <td>115.277295</td>\n",
       "      <td>302.188197</td>\n",
       "      <td>9.997834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>981</td>\n",
       "      <td>0.739350</td>\n",
       "      <td>0.008420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.756676</td>\n",
       "      <td>115.285528</td>\n",
       "      <td>301.753047</td>\n",
       "      <td>9.968589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>980</td>\n",
       "      <td>0.739346</td>\n",
       "      <td>0.008422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.764810</td>\n",
       "      <td>115.285861</td>\n",
       "      <td>301.879355</td>\n",
       "      <td>9.967264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>991</td>\n",
       "      <td>0.739345</td>\n",
       "      <td>0.008401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.675426</td>\n",
       "      <td>115.389330</td>\n",
       "      <td>300.460729</td>\n",
       "      <td>9.756196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>982</td>\n",
       "      <td>0.739342</td>\n",
       "      <td>0.008426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.753051</td>\n",
       "      <td>115.284876</td>\n",
       "      <td>301.615493</td>\n",
       "      <td>9.939734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>984</td>\n",
       "      <td>0.739337</td>\n",
       "      <td>0.008401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.734526</td>\n",
       "      <td>115.281248</td>\n",
       "      <td>301.357812</td>\n",
       "      <td>9.863203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>989</td>\n",
       "      <td>0.739336</td>\n",
       "      <td>0.008401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.678181</td>\n",
       "      <td>115.367952</td>\n",
       "      <td>300.683362</td>\n",
       "      <td>9.782757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>985</td>\n",
       "      <td>0.739335</td>\n",
       "      <td>0.008407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.733944</td>\n",
       "      <td>115.290633</td>\n",
       "      <td>301.248099</td>\n",
       "      <td>9.871144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>977</td>\n",
       "      <td>0.739334</td>\n",
       "      <td>0.008413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.797308</td>\n",
       "      <td>115.252174</td>\n",
       "      <td>302.335597</td>\n",
       "      <td>10.013523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>976</td>\n",
       "      <td>0.739332</td>\n",
       "      <td>0.008398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571.804381</td>\n",
       "      <td>115.247145</td>\n",
       "      <td>302.485836</td>\n",
       "      <td>10.043536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     iterations  test-NegativeRMSLEMetric-mean  test-NegativeRMSLEMetric-std  \\\n",
       "999         999                       0.739431                      0.008511   \n",
       "998         998                       0.739419                      0.008485   \n",
       "997         997                       0.739412                      0.008467   \n",
       "996         996                       0.739408                      0.008449   \n",
       "995         995                       0.739408                      0.008435   \n",
       "994         994                       0.739407                      0.008436   \n",
       "993         993                       0.739391                      0.008427   \n",
       "992         992                       0.739367                      0.008398   \n",
       "988         988                       0.739351                      0.008414   \n",
       "987         987                       0.739350                      0.008406   \n",
       "978         978                       0.739350                      0.008405   \n",
       "981         981                       0.739350                      0.008420   \n",
       "980         980                       0.739346                      0.008422   \n",
       "991         991                       0.739345                      0.008401   \n",
       "982         982                       0.739342                      0.008426   \n",
       "984         984                       0.739337                      0.008401   \n",
       "989         989                       0.739336                      0.008401   \n",
       "985         985                       0.739335                      0.008407   \n",
       "977         977                       0.739334                      0.008413   \n",
       "976         976                       0.739332                      0.008398   \n",
       "\n",
       "     train-NegativeRMSLEMetric-mean  train-NegativeRMSLEMetric-std  \\\n",
       "999                             NaN                            NaN   \n",
       "998                             NaN                            NaN   \n",
       "997                             NaN                            NaN   \n",
       "996                             NaN                            NaN   \n",
       "995                             NaN                            NaN   \n",
       "994                             NaN                            NaN   \n",
       "993                             NaN                            NaN   \n",
       "992                             NaN                            NaN   \n",
       "988                             NaN                            NaN   \n",
       "987                             NaN                            NaN   \n",
       "978                             NaN                            NaN   \n",
       "981                             NaN                            NaN   \n",
       "980                             NaN                            NaN   \n",
       "991                             NaN                            NaN   \n",
       "982                             NaN                            NaN   \n",
       "984                             NaN                            NaN   \n",
       "989                             NaN                            NaN   \n",
       "985                             NaN                            NaN   \n",
       "977                             NaN                            NaN   \n",
       "976                             NaN                            NaN   \n",
       "\n",
       "     test-RMSE-mean  test-RMSE-std  train-RMSE-mean  train-RMSE-std  \n",
       "999      571.528826     115.376442       299.159908        9.827738  \n",
       "998      571.542563     115.386807       299.296800        9.822228  \n",
       "997      571.555286     115.387938       299.445896        9.822090  \n",
       "996      571.592188     115.390606       299.617052        9.813014  \n",
       "995      571.604382     115.402112       299.836769        9.805539  \n",
       "994      571.605238     115.403926       299.982154        9.808731  \n",
       "993      571.629053     115.390657       300.102759        9.777186  \n",
       "992      571.673455     115.398457       300.270355        9.757627  \n",
       "988      571.669884     115.367997       300.862691        9.837485  \n",
       "987      571.686657     115.329523       301.008496        9.882409  \n",
       "978      571.753019     115.277295       302.188197        9.997834  \n",
       "981      571.756676     115.285528       301.753047        9.968589  \n",
       "980      571.764810     115.285861       301.879355        9.967264  \n",
       "991      571.675426     115.389330       300.460729        9.756196  \n",
       "982      571.753051     115.284876       301.615493        9.939734  \n",
       "984      571.734526     115.281248       301.357812        9.863203  \n",
       "989      571.678181     115.367952       300.683362        9.782757  \n",
       "985      571.733944     115.290633       301.248099        9.871144  \n",
       "977      571.797308     115.252174       302.335597       10.013523  \n",
       "976      571.804381     115.247145       302.485836       10.043536  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.sort_values(by=['test-NegativeRMSLEMetric-mean'], ascending=False)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='iterations', ylabel='test-NegativeRMSLEMetric-mean'>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1130.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(data=scores, x=\"iterations\", y=\"test-NegativeRMSLEMetric-mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the model in all training data, using the optimal hyperparameters found,  and predict on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/catboost/core.py:1953: UserWarning: Failed to import numba for optimizing custom metrics and objectives\n",
      "  _check_train_params(params)\n"
     ]
    }
   ],
   "source": [
    "# Initialize CatBoostRegressor\n",
    "model = CatBoostRegressor()\n",
    "\n",
    "# Fit model with the optimal hyper-parameters\n",
    "model.set_params(**cb.get_params())\n",
    "model.fit(pool_train)\n",
    "\n",
    "# Get predictions, calculate the predictions of the submissions\n",
    "y_pred = model.predict(pool_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing/showing feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# sns.set(rc={'figure.figsize':(15.7,8.27)})\n",
    "# sns.set(font_scale = 2)\n",
    "\n",
    "# for i, importance in enumerate(model.get_feature_importance()):\n",
    "#     print(importance, \": \", X_train.columns[i])\n",
    "\n",
    "# ax = sns.barplot(x=X_train.columns, y=model.get_feature_importance())\n",
    "# for p in ax.patches:\n",
    "#     ax.annotate('{:.2f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01), rotation=90, fontsize = 15)\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving-exporting the predictions of the test-set to a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving-exporting the results to a .csv file\n",
    "pd.DataFrame(y_pred, columns = ['Price']).to_csv('submission_catboost_all_features.csv', index=False)\n",
    "# 0.74245"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Modelling with all features: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "P_AdM1kNlEoh"
   },
   "outputs": [],
   "source": [
    "for col_name in X_train.select_dtypes(include=['object']):\n",
    "    X_test[col_name] = X_test[col_name].astype('category')\n",
    "    X_train[col_name] = X_train[col_name].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "[CV 1/10; 1/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 1/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.748 total time=   5.6s\n",
      "[CV 3/10; 2/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 2/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.750 total time=   7.6s\n",
      "[CV 10/10; 2/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 2/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.723 total time=   9.8s\n",
      "[CV 6/10; 3/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 3/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.759 total time=  13.1s\n",
      "[CV 7/10; 4/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 4/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  26.4s\n",
      "[CV 5/10; 5/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 5/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.734 total time=  20.8s\n",
      "[CV 1/10; 6/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 6/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.748 total time=  26.9s\n",
      "[CV 1/10; 7/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 7/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.749 total time=  37.5s\n",
      "[CV 9/10; 7/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 7/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.741 total time=  34.5s\n",
      "[CV 7/10; 8/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 8/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.744 total time=  40.2s\n",
      "[CV 5/10; 9/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 9/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.734 total time=  35.2s\n",
      "[CV 3/10; 10/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 10/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.754 total time=  44.3s\n",
      "[CV 2/10; 11/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 11/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.761 total time=  39.6s\n",
      "[CV 9/10; 11/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 11/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.744 total time=  37.7s\n",
      "[CV 8/10; 12/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 12/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  35.8s\n",
      "[CV 6/10; 13/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 13/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.769 total time= 1.1min\n",
      "[CV 4/10; 14/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 14/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.4min\n",
      "[CV 2/10; 15/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 15/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.761 total time= 1.3min\n",
      "[CV 10/10; 15/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 15/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.729 total time= 1.4min\n",
      "[CV 8/10; 16/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 16/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.739 total time= 1.7min\n",
      "[CV 6/10; 17/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 17/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.769 total time= 2.4min\n",
      "[CV 4/10; 18/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 18/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.743 total time= 2.8min\n",
      "[CV 1/10; 19/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 19/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.755 total time=  47.5s\n",
      "[CV 5/10; 19/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 19/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.739 total time=  47.7s\n",
      "[CV 10/10; 19/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 19/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.730 total time=  50.1s\n",
      "[CV 6/10; 20/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 20/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.770 total time= 1.3min\n",
      "[CV 4/10; 21/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 21/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.742 total time= 1.4min\n",
      "[CV 10/10; 21/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 21/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.728 total time= 1.4min\n",
      "[CV 8/10; 22/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 22/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.739 total time= 1.6min\n",
      "[CV 6/10; 23/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 23/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.771 total time= 2.6min\n",
      "[CV 4/10; 24/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 24/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.743 total time= 2.5min\n",
      "[CV 7/10; 1/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 1/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.742 total time=   5.5s\n",
      "[CV 2/10; 2/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 2/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.758 total time=   7.7s\n",
      "[CV 1/10; 3/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 3/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.748 total time=  10.8s\n",
      "[CV 9/10; 3/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 3/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.736 total time=  11.9s\n",
      "[CV 6/10; 4/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 4/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.760 total time=  24.1s\n",
      "[CV 4/10; 5/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 5/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.739 total time=  24.0s\n",
      "[CV 2/10; 6/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 6/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.758 total time=  23.5s\n",
      "[CV 9/10; 6/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 6/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.732 total time=  21.4s\n",
      "[CV 4/10; 7/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 7/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.739 total time=  41.5s\n",
      "[CV 4/10; 8/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 8/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.739 total time=  34.5s\n",
      "[CV 2/10; 9/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 9/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.758 total time=  34.6s\n",
      "[CV 9/10; 9/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 9/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.741 total time=  40.0s\n",
      "[CV 7/10; 10/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 10/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.749 total time=  32.9s\n",
      "[CV 4/10; 11/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 11/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.743 total time=  39.3s\n",
      "[CV 1/10; 12/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 12/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.754 total time=  41.1s\n",
      "[CV 10/10; 12/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 12/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.729 total time=  38.7s\n",
      "[CV 8/10; 13/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 13/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.739 total time= 1.2min\n",
      "[CV 6/10; 14/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 14/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.769 total time= 1.4min\n",
      "[CV 4/10; 15/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 15/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.743 total time= 1.4min\n",
      "[CV 2/10; 16/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 16/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.762 total time= 1.6min\n",
      "[CV 10/10; 16/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 16/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.730 total time= 1.6min\n",
      "[CV 8/10; 17/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 17/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 2.5min\n",
      "[CV 6/10; 18/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 18/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.771 total time= 2.8min\n",
      "[CV 4/10; 19/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 19/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.741 total time=  44.7s\n",
      "[CV 7/10; 19/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 19/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.749 total time=  45.3s\n",
      "[CV 3/10; 20/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 20/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.3min\n",
      "[CV 1/10; 21/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 21/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.754 total time= 1.4min\n",
      "[CV 9/10; 21/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 21/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.741 total time= 1.3min\n",
      "[CV 7/10; 22/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 22/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.749 total time= 1.8min\n",
      "[CV 5/10; 23/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 23/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.7min\n",
      "[CV 3/10; 24/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 24/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.758 total time= 3.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/10; 1/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 1/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.750 total time=   5.3s\n",
      "[CV 10/10; 1/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 1/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.723 total time=   6.6s\n",
      "[CV 7/10; 2/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 2/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.742 total time=  11.5s\n",
      "[CV 7/10; 3/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 3/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.742 total time=  11.1s\n",
      "[CV 4/10; 4/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 4/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.739 total time=  22.9s\n",
      "[CV 2/10; 5/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 5/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.758 total time=  20.7s\n",
      "[CV 10/10; 5/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 5/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.725 total time=  26.0s\n",
      "[CV 8/10; 6/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 6/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.735 total time=  26.9s\n",
      "[CV 7/10; 7/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 7/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.744 total time=  40.9s\n",
      "[CV 6/10; 8/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 8/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.762 total time=  43.1s\n",
      "[CV 4/10; 9/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 9/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  42.4s\n",
      "[CV 2/10; 10/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 10/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.763 total time=  40.8s\n",
      "[CV 10/10; 10/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 10/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.729 total time=  35.6s\n",
      "[CV 8/10; 11/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 11/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.738 total time=  44.7s\n",
      "[CV 7/10; 12/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 12/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.750 total time=  35.1s\n",
      "[CV 5/10; 13/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 13/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.740 total time= 1.0min\n",
      "[CV 2/10; 14/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 14/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.760 total time= 1.3min\n",
      "[CV 9/10; 14/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 14/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.5min\n",
      "[CV 9/10; 15/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 15/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.749 total time= 1.4min\n",
      "[CV 7/10; 16/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 16/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.750 total time= 1.6min\n",
      "[CV 5/10; 17/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 17/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 2.4min\n",
      "[CV 3/10; 18/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 18/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.759 total time= 2.9min\n",
      "[CV 2/10; 19/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 19/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.763 total time=  50.5s\n",
      "[CV 6/10; 19/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 19/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.765 total time=  47.6s\n",
      "[CV 1/10; 20/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 20/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.2min\n",
      "[CV 9/10; 20/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 20/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.5min\n",
      "[CV 7/10; 21/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 21/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.751 total time= 1.6min\n",
      "[CV 5/10; 22/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 22/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.740 total time= 1.7min\n",
      "[CV 4/10; 23/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 23/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 3.0min\n",
      "[CV 2/10; 24/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 24/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.761 total time= 3.1min\n",
      "[CV 2/10; 25/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 25/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.762 total time= 2.1min\n",
      "[CV 8/10; 1/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 1/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.733 total time=   5.6s\n",
      "[CV 4/10; 2/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 2/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.739 total time=   7.4s\n",
      "[CV 9/10; 2/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 2/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.736 total time=  10.6s\n",
      "[CV 8/10; 3/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 3/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.733 total time=  12.0s\n",
      "[CV 5/10; 4/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 4/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.734 total time=  22.6s\n",
      "[CV 3/10; 5/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 5/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.751 total time=  26.6s\n",
      "[CV 3/10; 6/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 6/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.751 total time=  24.3s\n",
      "[CV 10/10; 6/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 6/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.725 total time=  25.3s\n",
      "[CV 8/10; 7/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 7/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.735 total time=  37.4s\n",
      "[CV 5/10; 8/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 8/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.734 total time=  37.9s\n",
      "[CV 3/10; 9/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 9/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.752 total time=  37.7s\n",
      "[CV 1/10; 10/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 10/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.751 total time=  36.6s\n",
      "[CV 8/10; 10/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 10/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.737 total time=  34.1s\n",
      "[CV 6/10; 11/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 11/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.767 total time=  42.2s\n",
      "[CV 4/10; 12/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 12/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.744 total time=  38.8s\n",
      "[CV 2/10; 13/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 13/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.763 total time= 1.1min\n",
      "[CV 10/10; 13/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 13/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.729 total time= 1.1min\n",
      "[CV 7/10; 14/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 14/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.752 total time= 1.3min\n",
      "[CV 5/10; 15/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 15/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.736 total time= 1.5min\n",
      "[CV 3/10; 16/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 16/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.756 total time= 1.5min\n",
      "[CV 1/10; 17/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 17/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.754 total time= 2.3min\n",
      "[CV 10/10; 17/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 17/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.733 total time= 2.5min\n",
      "[CV 7/10; 18/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 18/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.750 total time= 2.5min\n",
      "[CV 9/10; 19/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 19/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  45.9s\n",
      "[CV 5/10; 20/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 20/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.3min\n",
      "[CV 2/10; 21/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 21/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.760 total time= 1.6min\n",
      "[CV 1/10; 22/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 22/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.756 total time= 1.7min\n",
      "[CV 9/10; 22/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 22/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.742 total time= 1.8min\n",
      "[CV 8/10; 23/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 23/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 2.8min\n",
      "[CV 6/10; 24/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 24/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.772 total time= 2.9min\n",
      "[CV 4/10; 25/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 25/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.741 total time= 1.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 6/10; 1/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 1/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.759 total time=   5.7s\n",
      "[CV 6/10; 2/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 2/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.759 total time=   8.2s\n",
      "[CV 3/10; 3/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 3/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.750 total time=  10.4s\n",
      "[CV 10/10; 3/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 3/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.723 total time=  14.0s\n",
      "[CV 8/10; 4/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 4/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.735 total time=  25.5s\n",
      "[CV 6/10; 5/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 5/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.760 total time=  27.3s\n",
      "[CV 4/10; 6/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 6/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  25.5s\n",
      "[CV 2/10; 7/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 7/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.758 total time=  36.1s\n",
      "[CV 10/10; 7/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 7/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.725 total time=  37.8s\n",
      "[CV 8/10; 8/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 8/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.735 total time=  39.9s\n",
      "[CV 7/10; 9/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 9/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.744 total time=  43.1s\n",
      "[CV 5/10; 10/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 10/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.738 total time=  36.1s\n",
      "[CV 3/10; 11/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 11/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.755 total time=  43.2s\n",
      "[CV 2/10; 12/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 12/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.761 total time=  35.9s\n",
      "[CV 9/10; 12/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 12/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.748 total time=  35.0s\n",
      "[CV 7/10; 13/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 13/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.750 total time= 1.2min\n",
      "[CV 5/10; 14/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 14/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.4min\n",
      "[CV 3/10; 15/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 15/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.758 total time= 1.4min\n",
      "[CV 1/10; 16/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 16/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.751 total time= 1.6min\n",
      "[CV 9/10; 16/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 16/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.742 total time= 1.6min\n",
      "[CV 7/10; 17/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 17/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.752 total time= 2.6min\n",
      "[CV 5/10; 18/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 18/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.737 total time= 2.7min\n",
      "[CV 3/10; 19/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 19/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.756 total time=  51.5s\n",
      "[CV 8/10; 19/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 19/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.739 total time=  47.0s\n",
      "[CV 4/10; 20/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 20/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.4min\n",
      "[CV 3/10; 21/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 21/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.758 total time= 1.6min\n",
      "[CV 2/10; 22/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 22/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.763 total time= 1.7min\n",
      "[CV 10/10; 22/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 22/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.731 total time= 1.7min\n",
      "[CV 7/10; 23/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 23/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 2.7min\n",
      "[CV 5/10; 24/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 24/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.742 total time= 3.0min\n",
      "[CV 3/10; 25/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 25/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.757 total time= 1.8min\n",
      "[CV 2/10; 1/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 1/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.758 total time=   5.3s\n",
      "[CV 9/10; 1/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 1/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.736 total time=   7.8s\n",
      "[CV 8/10; 2/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 2/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.733 total time=   9.1s\n",
      "[CV 5/10; 3/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 3/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.735 total time=   9.1s\n",
      "[CV 3/10; 4/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 4/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.751 total time=  22.5s\n",
      "[CV 1/10; 5/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 5/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.748 total time=  22.9s\n",
      "[CV 9/10; 5/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 5/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.732 total time=  26.0s\n",
      "[CV 7/10; 6/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 6/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.743 total time=  26.4s\n",
      "[CV 5/10; 7/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 7/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.734 total time=  37.2s\n",
      "[CV 2/10; 8/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 8/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.758 total time=  37.0s\n",
      "[CV 1/10; 9/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 9/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.749 total time=  41.4s\n",
      "[CV 10/10; 9/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 9/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.725 total time=  43.2s\n",
      "[CV 9/10; 10/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 10/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  33.9s\n",
      "[CV 7/10; 11/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 11/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.750 total time=  46.0s\n",
      "[CV 5/10; 12/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 12/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.735 total time=  35.9s\n",
      "[CV 3/10; 13/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 13/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.756 total time= 1.2min\n",
      "[CV 1/10; 14/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 14/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.753 total time= 1.5min\n",
      "[CV 1/10; 15/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 15/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.754 total time= 1.4min\n",
      "[CV 8/10; 15/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 15/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.739 total time= 1.4min\n",
      "[CV 6/10; 16/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 16/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.769 total time= 1.6min\n",
      "[CV 4/10; 17/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 17/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 2.3min\n",
      "[CV 2/10; 18/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 18/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.760 total time= 2.9min\n",
      "[CV 10/10; 18/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 18/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.730 total time= 2.7min\n",
      "[CV 8/10; 20/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 20/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.5min\n",
      "[CV 6/10; 21/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 21/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.773 total time= 1.5min\n",
      "[CV 3/10; 22/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 22/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.757 total time= 1.6min\n",
      "[CV 2/10; 23/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 23/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.761 total time= 2.9min\n",
      "[CV 10/10; 23/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 23/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.733 total time= 2.8min\n",
      "[CV 8/10; 24/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 24/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.739 total time= 2.6min\n",
      "[CV 5/10; 25/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 25/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.740 total time= 1.5min\n",
      "[CV 3/10; 26/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 26/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.759 total time= 2.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/10; 1/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 1/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.739 total time=   5.7s\n",
      "[CV 5/10; 2/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 2/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.735 total time=   7.9s\n",
      "[CV 2/10; 3/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 3/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.758 total time=  11.0s\n",
      "[CV 1/10; 4/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 4/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.748 total time=  22.7s\n",
      "[CV 9/10; 4/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 4/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.732 total time=  21.0s\n",
      "[CV 7/10; 5/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 5/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time=  27.5s\n",
      "[CV 5/10; 6/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 6/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.734 total time=  22.5s\n",
      "[CV 3/10; 7/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 7/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.752 total time=  36.5s\n",
      "[CV 1/10; 8/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 8/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.749 total time=  36.8s\n",
      "[CV 9/10; 8/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 8/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time=  37.9s\n",
      "[CV 6/10; 9/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 9/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.762 total time=  38.2s\n",
      "[CV 4/10; 10/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 10/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.740 total time=  33.8s\n",
      "[CV 1/10; 11/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 11/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.753 total time=  40.3s\n",
      "[CV 10/10; 11/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 11/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.730 total time=  36.8s\n",
      "[CV 6/10; 12/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 12/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.768 total time=  33.4s\n",
      "[CV 4/10; 13/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 13/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.741 total time= 1.1min\n",
      "[CV 3/10; 14/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 14/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.4min\n",
      "[CV 10/10; 14/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 14/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.732 total time= 1.4min\n",
      "[CV 7/10; 15/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 15/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.750 total time= 1.3min\n",
      "[CV 5/10; 16/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 16/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.740 total time= 1.6min\n",
      "[CV 3/10; 17/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 17/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.3min\n",
      "[CV 1/10; 18/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 18/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.754 total time= 2.9min\n",
      "[CV 9/10; 18/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 18/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.750 total time= 2.6min\n",
      "[CV 7/10; 20/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 20/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.749 total time= 1.4min\n",
      "[CV 5/10; 21/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 21/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.741 total time= 1.6min\n",
      "[CV 4/10; 22/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 22/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.741 total time= 1.6min\n",
      "[CV 1/10; 23/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 23/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 2.9min\n",
      "[CV 9/10; 23/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 23/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 2.9min\n",
      "[CV 7/10; 24/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 24/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.752 total time= 2.7min\n",
      "[CV 6/10; 25/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 25/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.769 total time= 1.6min\n",
      "[CV 4/10; 26/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 26/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.7min\n",
      "[CV 5/10; 1/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 1/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.735 total time=   5.3s\n",
      "[CV 1/10; 2/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 2/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.748 total time=   8.8s\n",
      "[CV 4/10; 3/81] START learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 3/81] END learning_rate=0.02, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  12.7s\n",
      "[CV 2/10; 4/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 4/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.758 total time=  23.0s\n",
      "[CV 10/10; 4/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 4/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.725 total time=  26.4s\n",
      "[CV 8/10; 5/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 5/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.735 total time=  24.9s\n",
      "[CV 6/10; 6/81] START learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 6/81] END learning_rate=0.02, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.760 total time=  28.4s\n",
      "[CV 6/10; 7/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 7/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.762 total time=  38.7s\n",
      "[CV 3/10; 8/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 8/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.752 total time=  35.0s\n",
      "[CV 10/10; 8/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 8/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.725 total time=  35.5s\n",
      "[CV 8/10; 9/81] START learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 9/81] END learning_rate=0.02, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.735 total time=  36.9s\n",
      "[CV 6/10; 10/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 10/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.768 total time=  39.1s\n",
      "[CV 5/10; 11/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 11/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.738 total time=  39.8s\n",
      "[CV 3/10; 12/81] START learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 12/81] END learning_rate=0.02, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.757 total time=  42.1s\n",
      "[CV 1/10; 13/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 13/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.751 total time= 1.1min\n",
      "[CV 9/10; 13/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 13/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.741 total time= 1.3min\n",
      "[CV 8/10; 14/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 14/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.4min\n",
      "[CV 6/10; 15/81] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 15/81] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.769 total time= 1.4min\n",
      "[CV 4/10; 16/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 16/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.740 total time= 1.5min\n",
      "[CV 2/10; 17/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 17/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.760 total time= 2.2min\n",
      "[CV 9/10; 17/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 17/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.745 total time= 2.8min\n",
      "[CV 8/10; 18/81] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 18/81] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.738 total time= 2.7min\n",
      "[CV 2/10; 20/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 20/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.762 total time= 1.4min\n",
      "[CV 10/10; 20/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 20/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.732 total time= 1.4min\n",
      "[CV 8/10; 21/81] START learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 21/81] END learning_rate=0.02, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.738 total time= 1.4min\n",
      "[CV 6/10; 22/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 22/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.768 total time= 1.6min\n",
      "[CV 3/10; 23/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 23/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.9min\n",
      "[CV 1/10; 24/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 24/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.756 total time= 2.9min\n",
      "[CV 10/10; 24/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 24/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.729 total time= 2.3min\n",
      "[CV 10/10; 25/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 25/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.733 total time= 1.6min\n",
      "[CV 7/10; 26/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 26/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 2.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 9/10; 24/81] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 24/81] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.742 total time= 2.5min\n",
      "[CV 8/10; 25/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 25/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.738 total time= 1.6min\n",
      "[CV 6/10; 26/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 26/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.772 total time= 2.8min\n",
      "[CV 3/10; 27/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 27/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.758 total time= 3.6min\n",
      "[CV 2/10; 28/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 28/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.758 total time=  12.6s\n",
      "[CV 6/10; 28/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 28/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.760 total time=  15.4s\n",
      "[CV 10/10; 28/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 28/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.726 total time=   8.5s\n",
      "[CV 5/10; 29/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 29/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.735 total time=   8.4s\n",
      "[CV 8/10; 29/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 29/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.735 total time=   7.7s\n",
      "[CV 2/10; 30/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 30/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.758 total time=   8.1s\n",
      "[CV 6/10; 30/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 30/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.760 total time=   9.6s\n",
      "[CV 10/10; 30/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 30/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.726 total time=  11.9s\n",
      "[CV 4/10; 31/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 31/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.739 total time=  20.8s\n",
      "[CV 10/10; 31/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 31/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.728 total time=  19.2s\n",
      "[CV 6/10; 32/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 32/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.762 total time=  14.4s\n",
      "[CV 3/10; 33/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 33/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.751 total time=  14.0s\n",
      "[CV 1/10; 34/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 34/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.753 total time=  22.3s\n",
      "[CV 9/10; 34/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 34/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  21.2s\n",
      "[CV 7/10; 35/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 35/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.745 total time=  23.0s\n",
      "[CV 5/10; 36/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 36/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.736 total time=  21.2s\n",
      "[CV 3/10; 37/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 37/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.758 total time=  21.9s\n",
      "[CV 1/10; 38/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 38/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.754 total time=  27.4s\n",
      "[CV 9/10; 38/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 38/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.745 total time=  26.4s\n",
      "[CV 7/10; 39/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 39/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.750 total time=  25.8s\n",
      "[CV 5/10; 40/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 40/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.739 total time=  41.8s\n",
      "[CV 3/10; 41/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 41/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.759 total time=  49.6s\n",
      "[CV 1/10; 42/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 42/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.756 total time=  48.4s\n",
      "[CV 9/10; 42/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 42/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.743 total time=  50.0s\n",
      "[CV 7/10; 43/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 43/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.752 total time= 1.0min\n",
      "[CV 5/10; 44/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 44/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.2min\n",
      "[CV 3/10; 45/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 45/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.758 total time= 1.3min\n",
      "[CV 9/10; 25/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 25/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.742 total time= 1.6min\n",
      "[CV 8/10; 26/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 26/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 2.9min\n",
      "[CV 6/10; 27/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 27/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.772 total time= 3.5min\n",
      "[CV 3/10; 28/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 28/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.750 total time=  13.8s\n",
      "[CV 7/10; 28/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 28/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  12.8s\n",
      "[CV 1/10; 29/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 29/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.750 total time=   7.9s\n",
      "[CV 4/10; 29/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 29/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.738 total time=   9.4s\n",
      "[CV 9/10; 29/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 29/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.741 total time=   9.0s\n",
      "[CV 3/10; 30/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 30/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.750 total time=   7.8s\n",
      "[CV 7/10; 30/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 30/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.743 total time=  11.0s\n",
      "[CV 1/10; 31/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 31/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.752 total time=  19.6s\n",
      "[CV 6/10; 31/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 31/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.762 total time=  18.1s\n",
      "[CV 2/10; 32/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 32/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.758 total time=  16.8s\n",
      "[CV 8/10; 32/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 32/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.736 total time=  16.1s\n",
      "[CV 6/10; 33/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 33/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.762 total time=  14.6s\n",
      "[CV 4/10; 34/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 34/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.739 total time=  22.0s\n",
      "[CV 2/10; 35/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 35/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.758 total time=  21.4s\n",
      "[CV 10/10; 35/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 35/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.728 total time=  22.0s\n",
      "[CV 8/10; 36/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 36/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.736 total time=  22.8s\n",
      "[CV 7/10; 37/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 37/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.750 total time=  22.1s\n",
      "[CV 4/10; 38/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 38/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.744 total time=  26.5s\n",
      "[CV 2/10; 39/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 39/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.761 total time=  28.1s\n",
      "[CV 10/10; 39/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 39/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.728 total time=  26.9s\n",
      "[CV 8/10; 40/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 40/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.739 total time=  42.3s\n",
      "[CV 6/10; 41/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 41/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.768 total time=  50.0s\n",
      "[CV 4/10; 42/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 42/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.743 total time=  51.2s\n",
      "[CV 2/10; 43/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 43/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.757 total time= 1.0min\n",
      "[CV 10/10; 43/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 43/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.730 total time= 1.0min\n",
      "[CV 8/10; 44/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 44/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.2min\n",
      "[CV 5/10; 45/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 45/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.739 total time= 1.2min\n",
      "[CV 4/10; 46/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 46/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  19.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/10; 25/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 25/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.757 total time= 2.0min\n",
      "[CV 7/10; 25/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 25/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.750 total time= 1.6min\n",
      "[CV 5/10; 26/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 26/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 3.0min\n",
      "[CV 5/10; 27/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 27/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.742 total time= 3.6min\n",
      "[CV 4/10; 28/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 28/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.738 total time=  15.8s\n",
      "[CV 9/10; 28/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 28/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.741 total time=  12.3s\n",
      "[CV 3/10; 29/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 29/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.750 total time=   7.7s\n",
      "[CV 7/10; 29/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 29/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.743 total time=  10.4s\n",
      "[CV 1/10; 30/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 30/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.750 total time=   8.5s\n",
      "[CV 5/10; 30/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 30/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.735 total time=  11.7s\n",
      "[CV 9/10; 30/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 30/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.741 total time=  10.4s\n",
      "[CV 3/10; 31/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 31/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.751 total time=  19.7s\n",
      "[CV 9/10; 31/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 31/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  18.0s\n",
      "[CV 5/10; 32/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 32/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.736 total time=  15.4s\n",
      "[CV 1/10; 33/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 33/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.752 total time=  15.9s\n",
      "[CV 10/10; 33/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 33/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.728 total time=  15.4s\n",
      "[CV 8/10; 34/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 34/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.736 total time=  22.5s\n",
      "[CV 6/10; 35/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 35/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.763 total time=  22.1s\n",
      "[CV 4/10; 36/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 36/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  21.2s\n",
      "[CV 2/10; 37/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 37/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.760 total time=  23.4s\n",
      "[CV 10/10; 37/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 37/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.729 total time=  22.5s\n",
      "[CV 8/10; 38/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 38/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.740 total time=  26.2s\n",
      "[CV 6/10; 39/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 39/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.768 total time=  26.8s\n",
      "[CV 4/10; 40/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 40/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.746 total time=  42.8s\n",
      "[CV 2/10; 41/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 41/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.761 total time=  50.3s\n",
      "[CV 10/10; 41/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 41/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.734 total time=  50.7s\n",
      "[CV 8/10; 42/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 42/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.740 total time=  49.7s\n",
      "[CV 6/10; 43/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 43/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.768 total time= 1.1min\n",
      "[CV 4/10; 44/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 44/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.3min\n",
      "[CV 2/10; 45/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 45/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.759 total time= 1.2min\n",
      "[CV 10/10; 45/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 45/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.729 total time= 1.1min\n",
      "[CV 4/10; 27/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 27/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.742 total time= 3.4min\n",
      "[CV 1/10; 28/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 28/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.750 total time=  14.3s\n",
      "[CV 5/10; 28/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 28/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.735 total time=  13.7s\n",
      "[CV 8/10; 28/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 28/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.735 total time=  11.1s\n",
      "[CV 2/10; 29/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 29/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.758 total time=   7.7s\n",
      "[CV 6/10; 29/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 29/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.760 total time=   9.0s\n",
      "[CV 10/10; 29/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 29/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.726 total time=   8.6s\n",
      "[CV 4/10; 30/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 30/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.738 total time=   8.7s\n",
      "[CV 8/10; 30/81] START learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 30/81] END learning_rate=0.035, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.735 total time=  12.1s\n",
      "[CV 2/10; 31/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 31/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.758 total time=  20.1s\n",
      "[CV 8/10; 31/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 31/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.736 total time=  19.9s\n",
      "[CV 4/10; 32/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 32/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.739 total time=  15.2s\n",
      "[CV 10/10; 32/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 32/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.728 total time=  14.4s\n",
      "[CV 8/10; 33/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 33/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.736 total time=  15.1s\n",
      "[CV 6/10; 34/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 34/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.763 total time=  21.7s\n",
      "[CV 4/10; 35/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 35/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.739 total time=  22.2s\n",
      "[CV 2/10; 36/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 36/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.758 total time=  22.3s\n",
      "[CV 10/10; 36/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 36/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.728 total time=  22.7s\n",
      "[CV 8/10; 37/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 37/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.738 total time=  22.4s\n",
      "[CV 6/10; 38/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 38/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.750 total time=  28.3s\n",
      "[CV 5/10; 39/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 39/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  27.9s\n",
      "[CV 3/10; 40/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 40/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.760 total time=  42.1s\n",
      "[CV 1/10; 41/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 41/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.754 total time=  48.4s\n",
      "[CV 9/10; 41/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 41/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.745 total time=  47.6s\n",
      "[CV 7/10; 42/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 42/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.751 total time=  49.9s\n",
      "[CV 5/10; 43/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 43/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.739 total time= 1.0min\n",
      "[CV 3/10; 44/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 44/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.759 total time= 1.2min\n",
      "[CV 10/10; 44/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 44/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.734 total time= 1.2min\n",
      "[CV 9/10; 45/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 45/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.741 total time= 1.1min\n",
      "[CV 3/10; 47/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 47/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.757 total time=  33.8s\n",
      "[CV 1/10; 48/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 48/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.756 total time=  37.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/10; 26/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 26/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 2.7min\n",
      "[CV 9/10; 26/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 26/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 3.2min\n",
      "[CV 8/10; 27/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 27/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.739 total time= 3.2min\n",
      "[CV 5/10; 31/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 31/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.736 total time=  18.3s\n",
      "[CV 1/10; 32/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 32/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.752 total time=  18.7s\n",
      "[CV 7/10; 32/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 32/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.744 total time=  15.6s\n",
      "[CV 5/10; 33/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 33/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.736 total time=  14.9s\n",
      "[CV 3/10; 34/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 34/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.751 total time=  21.5s\n",
      "[CV 1/10; 35/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 35/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.753 total time=  21.2s\n",
      "[CV 9/10; 35/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 35/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time=  21.6s\n",
      "[CV 7/10; 36/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 36/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.745 total time=  21.9s\n",
      "[CV 5/10; 37/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 37/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.739 total time=  22.6s\n",
      "[CV 2/10; 38/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 38/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.761 total time=  28.1s\n",
      "[CV 1/10; 39/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 39/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.755 total time=  27.1s\n",
      "[CV 9/10; 39/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 39/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.744 total time=  25.9s\n",
      "[CV 7/10; 40/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 40/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.751 total time=  42.2s\n",
      "[CV 5/10; 41/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 41/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.738 total time=  50.9s\n",
      "[CV 3/10; 42/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 42/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.757 total time=  51.8s\n",
      "[CV 1/10; 43/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 43/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.755 total time= 1.1min\n",
      "[CV 9/10; 43/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 43/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.746 total time= 1.0min\n",
      "[CV 7/10; 44/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 44/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.3min\n",
      "[CV 6/10; 45/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 45/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.768 total time= 1.2min\n",
      "[CV 3/10; 46/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 46/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.754 total time=  19.7s\n",
      "[CV 7/10; 46/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 46/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.750 total time=  19.7s\n",
      "[CV 6/10; 47/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 47/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.770 total time=  34.6s\n",
      "[CV 5/10; 48/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 48/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.738 total time=  38.1s\n",
      "[CV 3/10; 49/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 49/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.755 total time=  37.8s\n",
      "[CV 1/10; 50/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 50/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.1min\n",
      "[CV 9/10; 50/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 50/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.745 total time= 1.1min\n",
      "[CV 7/10; 51/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 51/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.750 total time= 1.1min\n",
      "[CV 5/10; 52/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 52/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.740 total time=  55.7s\n",
      "[CV 2/10; 26/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 26/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.761 total time= 2.7min\n",
      "[CV 10/10; 26/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 26/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.733 total time= 3.1min\n",
      "[CV 7/10; 27/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 27/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.752 total time= 3.4min\n",
      "[CV 7/10; 31/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 31/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.744 total time=  18.5s\n",
      "[CV 3/10; 32/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 32/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.751 total time=  16.5s\n",
      "[CV 9/10; 32/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 32/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.742 total time=  14.3s\n",
      "[CV 7/10; 33/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 33/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.744 total time=  16.0s\n",
      "[CV 5/10; 34/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 34/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.736 total time=  21.0s\n",
      "[CV 3/10; 35/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 35/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.751 total time=  21.6s\n",
      "[CV 1/10; 36/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 36/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.753 total time=  21.7s\n",
      "[CV 9/10; 36/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 36/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.742 total time=  21.5s\n",
      "[CV 6/10; 37/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 37/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.766 total time=  22.8s\n",
      "[CV 5/10; 38/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 38/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.737 total time=  26.8s\n",
      "[CV 3/10; 39/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 39/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.757 total time=  27.6s\n",
      "[CV 1/10; 40/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 40/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.756 total time=  43.5s\n",
      "[CV 9/10; 40/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 40/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.746 total time=  43.1s\n",
      "[CV 7/10; 41/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 41/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.750 total time=  52.2s\n",
      "[CV 6/10; 42/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 42/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.768 total time=  50.9s\n",
      "[CV 4/10; 43/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 43/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.746 total time= 1.1min\n",
      "[CV 2/10; 44/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 44/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.760 total time= 1.3min\n",
      "[CV 1/10; 45/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 45/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.756 total time= 1.2min\n",
      "[CV 8/10; 45/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 45/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.740 total time= 1.1min\n",
      "[CV 2/10; 47/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 47/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.763 total time=  34.0s\n",
      "[CV 10/10; 47/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 47/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.732 total time=  35.7s\n",
      "[CV 8/10; 48/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 48/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.738 total time=  34.6s\n",
      "[CV 6/10; 49/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 49/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.772 total time=  37.4s\n",
      "[CV 4/10; 50/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 50/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.1min\n",
      "[CV 3/10; 51/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 51/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.759 total time= 1.2min\n",
      "[CV 1/10; 52/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 52/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.758 total time=  56.9s\n",
      "[CV 8/10; 52/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 52/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  55.6s\n",
      "[CV 6/10; 53/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 53/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.771 total time= 1.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/10; 27/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 27/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.756 total time= 3.4min\n",
      "[CV 9/10; 27/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 27/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.742 total time= 2.8min\n",
      "[CV 2/10; 33/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 33/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.758 total time=  14.2s\n",
      "[CV 9/10; 33/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 33/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.742 total time=  14.3s\n",
      "[CV 7/10; 34/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 34/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.745 total time=  22.1s\n",
      "[CV 5/10; 35/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 35/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.736 total time=  21.9s\n",
      "[CV 3/10; 36/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 36/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.751 total time=  21.5s\n",
      "[CV 1/10; 37/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 37/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.755 total time=  22.0s\n",
      "[CV 9/10; 37/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 37/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.745 total time=  21.9s\n",
      "[CV 7/10; 38/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 38/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.749 total time=  26.8s\n",
      "[CV 4/10; 39/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 39/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.743 total time=  27.2s\n",
      "[CV 2/10; 40/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 40/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.758 total time=  42.2s\n",
      "[CV 10/10; 40/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 40/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.730 total time=  42.7s\n",
      "[CV 8/10; 41/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 41/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.741 total time=  49.2s\n",
      "[CV 5/10; 42/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 42/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  50.7s\n",
      "[CV 3/10; 43/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 43/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.760 total time= 1.0min\n",
      "[CV 1/10; 44/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 44/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.754 total time= 1.2min\n",
      "[CV 9/10; 44/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 44/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.745 total time= 1.2min\n",
      "[CV 7/10; 45/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 45/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.751 total time= 1.2min\n",
      "[CV 1/10; 47/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 47/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.756 total time=  32.9s\n",
      "[CV 9/10; 47/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 47/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.745 total time=  34.6s\n",
      "[CV 7/10; 48/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 48/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.749 total time=  35.0s\n",
      "[CV 5/10; 49/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 49/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.740 total time=  37.2s\n",
      "[CV 3/10; 50/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 50/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.1min\n",
      "[CV 1/10; 51/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 51/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.756 total time= 1.1min\n",
      "[CV 9/10; 51/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 51/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.744 total time= 1.1min\n",
      "[CV 7/10; 52/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 52/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.751 total time=  56.6s\n",
      "[CV 5/10; 53/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 53/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.6min\n",
      "[CV 3/10; 54/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 54/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.759 total time= 1.7min\n",
      "[CV 2/10; 55/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 55/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.757 total time=   5.2s\n",
      "[CV 4/10; 55/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 55/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.738 total time=   5.6s\n",
      "[CV 2/10; 27/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 27/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.761 total time= 3.6min\n",
      "[CV 10/10; 27/81] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 27/81] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.730 total time= 2.7min\n",
      "[CV 4/10; 33/81] START learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 33/81] END learning_rate=0.035, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  13.7s\n",
      "[CV 2/10; 34/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 34/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.758 total time=  20.8s\n",
      "[CV 10/10; 34/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 34/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.728 total time=  21.5s\n",
      "[CV 8/10; 35/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 35/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.736 total time=  22.4s\n",
      "[CV 6/10; 36/81] START learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 36/81] END learning_rate=0.035, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.763 total time=  21.2s\n",
      "[CV 4/10; 37/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 37/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.746 total time=  24.3s\n",
      "[CV 3/10; 38/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 38/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.758 total time=  26.6s\n",
      "[CV 10/10; 38/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 38/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.733 total time=  27.0s\n",
      "[CV 8/10; 39/81] START learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 39/81] END learning_rate=0.035, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  25.8s\n",
      "[CV 6/10; 40/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 40/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.767 total time=  42.0s\n",
      "[CV 4/10; 41/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 41/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.744 total time=  51.2s\n",
      "[CV 2/10; 42/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 42/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.760 total time=  51.5s\n",
      "[CV 10/10; 42/81] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 42/81] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.728 total time=  50.1s\n",
      "[CV 8/10; 43/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 43/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.739 total time= 1.0min\n",
      "[CV 6/10; 44/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 44/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.768 total time= 1.2min\n",
      "[CV 4/10; 45/81] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 45/81] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.743 total time= 1.2min\n",
      "[CV 2/10; 46/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 46/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.763 total time=  21.4s\n",
      "[CV 6/10; 46/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 46/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.770 total time=  19.8s\n",
      "[CV 10/10; 46/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 46/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.728 total time=  19.6s\n",
      "[CV 8/10; 47/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 47/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.738 total time=  33.4s\n",
      "[CV 6/10; 48/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 48/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.770 total time=  37.4s\n",
      "[CV 4/10; 49/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 49/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.744 total time=  38.0s\n",
      "[CV 2/10; 50/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 50/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.763 total time= 1.1min\n",
      "[CV 10/10; 50/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 50/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.1min\n",
      "[CV 8/10; 51/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 51/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.738 total time= 1.1min\n",
      "[CV 6/10; 52/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 52/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.771 total time=  55.0s\n",
      "[CV 4/10; 53/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 53/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.6min\n",
      "[CV 2/10; 54/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 54/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.760 total time= 1.7min\n",
      "[CV 10/10; 54/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/10; 47/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 47/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.741 total time=  34.5s\n",
      "[CV 2/10; 48/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 48/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.760 total time=  37.9s\n",
      "[CV 10/10; 48/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 48/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.729 total time=  33.6s\n",
      "[CV 8/10; 49/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 49/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  37.7s\n",
      "[CV 6/10; 50/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 50/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.771 total time= 1.1min\n",
      "[CV 4/10; 51/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 51/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.741 total time= 1.1min\n",
      "[CV 2/10; 52/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 52/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.763 total time=  56.7s\n",
      "[CV 9/10; 52/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 52/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  56.1s\n",
      "[CV 7/10; 53/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 53/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.754 total time= 1.6min\n",
      "[CV 5/10; 54/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 54/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.740 total time= 1.7min\n",
      "[CV 5/10; 55/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 55/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.735 total time=   5.4s\n",
      "[CV 9/10; 55/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 55/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.741 total time=   5.5s\n",
      "[CV 3/10; 56/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 56/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.752 total time=   5.6s\n",
      "[CV 7/10; 56/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 56/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.741 total time=   5.6s\n",
      "[CV 1/10; 57/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 57/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.750 total time=   5.6s\n",
      "[CV 5/10; 57/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 57/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.735 total time=   5.2s\n",
      "[CV 9/10; 57/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 57/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.741 total time=   5.1s\n",
      "[CV 4/10; 58/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 58/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.738 total time=  11.0s\n",
      "[CV 10/10; 58/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 58/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.726 total time=  10.3s\n",
      "[CV 6/10; 59/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 59/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.761 total time=  10.4s\n",
      "[CV 4/10; 60/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 60/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.738 total time=  10.4s\n",
      "[CV 2/10; 61/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 61/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.756 total time=  15.2s\n",
      "[CV 10/10; 61/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 61/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.727 total time=  15.4s\n",
      "[CV 8/10; 62/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 62/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.735 total time=  15.6s\n",
      "[CV 6/10; 63/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 63/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.762 total time=  15.7s\n",
      "[CV 4/10; 64/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 64/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.744 total time=  16.4s\n",
      "[CV 2/10; 65/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 65/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.758 total time=  20.0s\n",
      "[CV 10/10; 65/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 65/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.731 total time=  19.9s\n",
      "[CV 8/10; 66/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 66/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  19.7s\n",
      "[CV 6/10; 67/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 67/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.767 total time=  33.0s\n",
      "[CV 3/10; 68/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 68/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.759 total time=  39.6s\n",
      "[CV 9/10; 48/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 48/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.743 total time=  32.9s\n",
      "[CV 7/10; 49/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 49/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.750 total time=  37.7s\n",
      "[CV 5/10; 50/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 50/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.1min\n",
      "[CV 2/10; 51/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 51/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.760 total time= 1.2min\n",
      "[CV 10/10; 51/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 51/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.730 total time= 1.1min\n",
      "[CV 10/10; 52/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 52/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.730 total time=  56.3s\n",
      "[CV 8/10; 53/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 53/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.6min\n",
      "[CV 6/10; 54/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 54/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.771 total time= 1.7min\n",
      "[CV 6/10; 55/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 55/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.760 total time=   5.5s\n",
      "[CV 10/10; 55/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 55/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.725 total time=   5.6s\n",
      "[CV 4/10; 56/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 56/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.738 total time=   5.6s\n",
      "[CV 8/10; 56/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 56/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.735 total time=   5.7s\n",
      "[CV 2/10; 57/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 57/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.757 total time=   5.5s\n",
      "[CV 6/10; 57/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 57/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.760 total time=   5.3s\n",
      "[CV 10/10; 57/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 57/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.725 total time=   5.3s\n",
      "[CV 5/10; 58/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 58/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.735 total time=  10.9s\n",
      "[CV 1/10; 59/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 59/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.751 total time=  10.8s\n",
      "[CV 7/10; 59/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 59/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time=  10.4s\n",
      "[CV 5/10; 60/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 60/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.735 total time=  10.1s\n",
      "[CV 3/10; 61/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 61/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.752 total time=  15.4s\n",
      "[CV 1/10; 62/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 62/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.752 total time=  15.7s\n",
      "[CV 9/10; 62/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 62/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.739 total time=  15.3s\n",
      "[CV 7/10; 63/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 63/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.743 total time=  15.6s\n",
      "[CV 5/10; 64/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 64/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.736 total time=  16.3s\n",
      "[CV 3/10; 65/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 65/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.759 total time=  20.4s\n",
      "[CV 1/10; 66/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 66/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.756 total time=  19.8s\n",
      "[CV 9/10; 66/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 66/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.744 total time=  20.7s\n",
      "[CV 7/10; 67/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 67/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.754 total time=  33.8s\n",
      "[CV 5/10; 68/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 68/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.739 total time=  40.7s\n",
      "[CV 4/10; 69/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 69/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.744 total time=  40.1s\n",
      "[CV 1/10; 70/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 70/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.754 total time=  49.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/10; 46/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 46/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.756 total time=  21.3s\n",
      "[CV 5/10; 46/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 46/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.739 total time=  19.7s\n",
      "[CV 9/10; 46/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 46/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  19.1s\n",
      "[CV 7/10; 47/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 47/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.753 total time=  34.3s\n",
      "[CV 4/10; 48/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 48/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.742 total time=  37.7s\n",
      "[CV 2/10; 49/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 49/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.763 total time=  38.0s\n",
      "[CV 10/10; 49/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 49/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.730 total time=  38.3s\n",
      "[CV 8/10; 50/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 50/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.1min\n",
      "[CV 6/10; 51/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 51/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.771 total time= 1.1min\n",
      "[CV 3/10; 52/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 52/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.755 total time=  55.1s\n",
      "[CV 1/10; 53/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 53/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.6min\n",
      "[CV 9/10; 53/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 53/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.6min\n",
      "[CV 7/10; 54/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 54/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.749 total time= 1.6min\n",
      "[CV 2/10; 58/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 58/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.757 total time=  10.8s\n",
      "[CV 8/10; 58/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 58/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.735 total time=  10.5s\n",
      "[CV 4/10; 59/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 59/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.738 total time=  10.4s\n",
      "[CV 3/10; 60/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 60/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.753 total time=  10.4s\n",
      "[CV 1/10; 61/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 61/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.752 total time=  15.7s\n",
      "[CV 9/10; 61/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 61/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.739 total time=  15.2s\n",
      "[CV 7/10; 62/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 62/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.743 total time=  15.6s\n",
      "[CV 5/10; 63/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 63/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.734 total time=  15.5s\n",
      "[CV 3/10; 64/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 64/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.755 total time=  16.3s\n",
      "[CV 1/10; 65/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 65/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.754 total time=  19.8s\n",
      "[CV 9/10; 65/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 65/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.747 total time=  20.1s\n",
      "[CV 7/10; 66/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 66/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.747 total time=  19.7s\n",
      "[CV 5/10; 67/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 67/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.736 total time=  34.1s\n",
      "[CV 4/10; 68/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 68/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.744 total time=  40.2s\n",
      "[CV 2/10; 69/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 69/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.760 total time=  38.8s\n",
      "[CV 10/10; 69/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 69/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.732 total time=  40.1s\n",
      "[CV 8/10; 70/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 70/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.735 total time=  50.0s\n",
      "[CV 6/10; 71/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 71/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.769 total time=  59.6s\n",
      "[CV 8/10; 46/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 46/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.741 total time=  18.8s\n",
      "[CV 5/10; 47/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 47/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.741 total time=  33.6s\n",
      "[CV 3/10; 48/81] START learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 48/81] END learning_rate=0.035, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.759 total time=  38.4s\n",
      "[CV 1/10; 49/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 49/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.756 total time=  37.8s\n",
      "[CV 9/10; 49/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 49/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  37.8s\n",
      "[CV 7/10; 50/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 50/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.753 total time= 1.1min\n",
      "[CV 5/10; 51/81] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 51/81] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.739 total time= 1.2min\n",
      "[CV 4/10; 52/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 52/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.744 total time=  57.4s\n",
      "[CV 2/10; 53/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 53/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.763 total time= 1.6min\n",
      "[CV 10/10; 53/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 53/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.734 total time= 1.6min\n",
      "[CV 8/10; 54/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 54/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.738 total time= 1.6min\n",
      "[CV 6/10; 58/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 58/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.761 total time=  10.9s\n",
      "[CV 2/10; 59/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 59/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time=  10.2s\n",
      "[CV 9/10; 59/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 59/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.740 total time=  10.2s\n",
      "[CV 7/10; 60/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 60/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.743 total time=  10.4s\n",
      "[CV 5/10; 61/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 61/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.734 total time=  15.1s\n",
      "[CV 3/10; 62/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 62/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.752 total time=  15.4s\n",
      "[CV 1/10; 63/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 63/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.752 total time=  15.4s\n",
      "[CV 9/10; 63/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 63/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  16.0s\n",
      "[CV 7/10; 64/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 64/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.753 total time=  16.7s\n",
      "[CV 5/10; 65/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 65/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.739 total time=  20.3s\n",
      "[CV 3/10; 66/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 66/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.758 total time=  20.2s\n",
      "[CV 1/10; 67/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 67/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.753 total time=  32.7s\n",
      "[CV 9/10; 67/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 67/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  33.2s\n",
      "[CV 7/10; 68/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 68/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.748 total time=  38.2s\n",
      "[CV 5/10; 69/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 69/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.735 total time=  39.2s\n",
      "[CV 3/10; 70/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 70/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.757 total time=  49.7s\n",
      "[CV 1/10; 71/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 71/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.755 total time=  57.1s\n",
      "[CV 9/10; 71/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 71/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.747 total time=  58.1s\n",
      "[CV 7/10; 72/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 72/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.749 total time=  57.1s\n",
      "[CV 7/10; 73/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 73/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.751 total time=  18.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/10; 54/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 54/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.740 total time= 1.7min\n",
      "[CV 1/10; 55/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 55/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.750 total time=   5.4s\n",
      "[CV 3/10; 55/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 55/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.752 total time=   6.0s\n",
      "[CV 7/10; 55/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 55/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.741 total time=   5.7s\n",
      "[CV 1/10; 56/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 56/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.750 total time=   5.8s\n",
      "[CV 5/10; 56/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 56/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.735 total time=   5.6s\n",
      "[CV 9/10; 56/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 56/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.741 total time=   5.6s\n",
      "[CV 3/10; 57/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 57/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.752 total time=   5.3s\n",
      "[CV 7/10; 57/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 57/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.741 total time=   5.2s\n",
      "[CV 1/10; 58/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 58/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.751 total time=  10.7s\n",
      "[CV 7/10; 58/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 58/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  10.4s\n",
      "[CV 3/10; 59/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 59/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.753 total time=  10.1s\n",
      "[CV 1/10; 60/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 60/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.751 total time=  10.2s\n",
      "[CV 9/10; 60/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 60/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.740 total time=  10.2s\n",
      "[CV 7/10; 61/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 61/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  15.9s\n",
      "[CV 5/10; 62/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 62/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.734 total time=  15.1s\n",
      "[CV 3/10; 63/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 63/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.752 total time=  15.2s\n",
      "[CV 1/10; 64/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 64/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.754 total time=  15.4s\n",
      "[CV 9/10; 64/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 64/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.741 total time=  16.8s\n",
      "[CV 7/10; 65/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 65/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.748 total time=  19.4s\n",
      "[CV 5/10; 66/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 66/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.736 total time=  19.6s\n",
      "[CV 3/10; 67/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 67/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.757 total time=  33.8s\n",
      "[CV 1/10; 68/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 68/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.755 total time=  39.7s\n",
      "[CV 9/10; 68/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 68/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.747 total time=  40.1s\n",
      "[CV 8/10; 69/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 69/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  40.0s\n",
      "[CV 6/10; 70/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 70/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.767 total time=  49.4s\n",
      "[CV 4/10; 71/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 71/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.743 total time=  58.6s\n",
      "[CV 2/10; 72/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 72/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.760 total time=  58.0s\n",
      "[CV 10/10; 72/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 72/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.733 total time= 1.0min\n",
      "[CV 4/10; 74/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 74/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.742 total time=  32.5s\n",
      "[CV 2/10; 75/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 75/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.760 total time=  34.6s\n",
      "[CV 8/10; 55/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 55/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.735 total time=   5.6s\n",
      "[CV 2/10; 56/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 56/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.757 total time=   5.4s\n",
      "[CV 6/10; 56/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 56/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.760 total time=   5.5s\n",
      "[CV 10/10; 56/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 56/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.725 total time=   5.6s\n",
      "[CV 4/10; 57/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 57/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.738 total time=   5.4s\n",
      "[CV 8/10; 57/81] START learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 57/81] END learning_rate=0.05, max_depth=5, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.735 total time=   5.4s\n",
      "[CV 3/10; 58/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 58/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.753 total time=  10.8s\n",
      "[CV 9/10; 58/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 58/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.740 total time=  10.3s\n",
      "[CV 5/10; 59/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 59/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.735 total time=  10.0s\n",
      "[CV 2/10; 60/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 60/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.757 total time=  10.1s\n",
      "[CV 10/10; 60/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 60/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.726 total time=  10.3s\n",
      "[CV 8/10; 61/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 61/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.735 total time=  15.5s\n",
      "[CV 6/10; 62/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 62/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.762 total time=  15.4s\n",
      "[CV 4/10; 63/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 63/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.738 total time=  15.5s\n",
      "[CV 2/10; 64/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 64/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.761 total time=  15.5s\n",
      "[CV 10/10; 64/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 64/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.729 total time=  16.4s\n",
      "[CV 8/10; 65/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 65/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.739 total time=  20.1s\n",
      "[CV 6/10; 66/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 66/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.772 total time=  19.8s\n",
      "[CV 4/10; 67/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 67/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  33.0s\n",
      "[CV 2/10; 68/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 68/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time=  39.2s\n",
      "[CV 10/10; 68/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 68/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.731 total time=  39.3s\n",
      "[CV 7/10; 69/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 69/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.748 total time=  38.4s\n",
      "[CV 5/10; 70/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 70/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.737 total time=  49.3s\n",
      "[CV 3/10; 71/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 71/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.759 total time=  59.1s\n",
      "[CV 1/10; 72/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 72/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.757 total time=  58.6s\n",
      "[CV 9/10; 72/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 72/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.745 total time=  57.5s\n",
      "[CV 3/10; 74/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 74/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.759 total time=  32.3s\n",
      "[CV 1/10; 75/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 75/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.753 total time=  34.2s\n",
      "[CV 9/10; 75/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 75/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.743 total time=  33.5s\n",
      "[CV 7/10; 76/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 76/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.751 total time=  37.0s\n",
      "[CV 5/10; 77/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 77/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/10; 53/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 53/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.6min\n",
      "[CV 1/10; 54/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 54/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.757 total time= 1.7min\n",
      "[CV 9/10; 54/81] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 54/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.744 total time= 1.5min\n",
      "[CV 8/10; 59/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 59/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.735 total time=  10.6s\n",
      "[CV 6/10; 60/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 60/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.761 total time=  10.6s\n",
      "[CV 4/10; 61/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 61/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.738 total time=  15.4s\n",
      "[CV 2/10; 62/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 62/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.756 total time=  15.2s\n",
      "[CV 10/10; 62/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 62/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.727 total time=  15.1s\n",
      "[CV 8/10; 63/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 63/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.735 total time=  15.8s\n",
      "[CV 6/10; 64/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 64/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.767 total time=  16.7s\n",
      "[CV 4/10; 65/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 65/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.745 total time=  20.9s\n",
      "[CV 2/10; 66/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 66/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.760 total time=  20.2s\n",
      "[CV 10/10; 66/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 66/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.731 total time=  19.8s\n",
      "[CV 8/10; 67/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 67/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.735 total time=  33.3s\n",
      "[CV 6/10; 68/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 68/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.769 total time=  39.3s\n",
      "[CV 3/10; 69/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 69/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.758 total time=  40.4s\n",
      "[CV 2/10; 70/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 70/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.760 total time=  50.0s\n",
      "[CV 10/10; 70/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 70/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.730 total time=  50.4s\n",
      "[CV 8/10; 71/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 71/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.0min\n",
      "[CV 6/10; 72/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 72/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.772 total time=  57.8s\n",
      "[CV 4/10; 73/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 73/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  18.5s\n",
      "[CV 9/10; 73/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 73/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.744 total time=  19.1s\n",
      "[CV 7/10; 74/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 74/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.749 total time=  32.5s\n",
      "[CV 5/10; 75/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 75/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.740 total time=  34.3s\n",
      "[CV 3/10; 76/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 76/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.754 total time=  36.7s\n",
      "[CV 1/10; 77/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 77/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.755 total time= 1.0min\n",
      "[CV 9/10; 77/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 77/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.1min\n",
      "[CV 7/10; 78/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 78/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.749 total time= 1.1min\n",
      "[CV 5/10; 79/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 79/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  55.0s\n",
      "[CV 3/10; 80/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 80/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.759 total time= 1.6min\n",
      "[CV 1/10; 81/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 81/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.754 total time= 1.7min\n",
      "The best estimator across ALL searched params:\n",
      " LGBMRegressor(learning_rate=0.035, max_depth=15, n_estimators=2000,\n",
      "              num_leaves=124, objective='regression_l1', random_state=420)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/10; 78/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 78/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.758 total time= 1.1min\n",
      "[CV 1/10; 79/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 79/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.754 total time=  56.5s\n",
      "[CV 7/10; 79/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 79/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.752 total time=  55.6s\n",
      "[CV 5/10; 80/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 80/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.6min\n",
      "[CV 3/10; 81/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 81/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.759 total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n",
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n",
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n",
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 10/10; 54/81] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.731 total time= 1.5min\n",
      "[CV 10/10; 59/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 59/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.726 total time=  10.3s\n",
      "[CV 8/10; 60/81] START learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 60/81] END learning_rate=0.05, max_depth=5, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.735 total time=  10.4s\n",
      "[CV 6/10; 61/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 61/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.762 total time=  15.4s\n",
      "[CV 4/10; 62/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 62/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.738 total time=  15.5s\n",
      "[CV 2/10; 63/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 63/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.756 total time=  15.1s\n",
      "[CV 10/10; 63/81] START learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 63/81] END learning_rate=0.05, max_depth=5, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.727 total time=  16.2s\n",
      "[CV 8/10; 64/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 64/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.735 total time=  16.9s\n",
      "[CV 6/10; 65/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 65/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.769 total time=  19.8s\n",
      "[CV 4/10; 66/81] START learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 66/81] END learning_rate=0.05, max_depth=10, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.744 total time=  20.3s\n",
      "[CV 2/10; 67/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 67/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.761 total time=  33.8s\n",
      "[CV 10/10; 67/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 67/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.730 total time=  33.8s\n",
      "[CV 8/10; 68/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 68/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.739 total time=  40.6s\n",
      "[CV 6/10; 69/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 69/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.772 total time=  38.6s\n",
      "[CV 4/10; 70/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 70/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  49.0s\n",
      "[CV 2/10; 71/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 71/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.757 total time=  57.4s\n",
      "[CV 10/10; 71/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 71/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.732 total time=  58.6s\n",
      "[CV 8/10; 72/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 72/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  59.0s\n",
      "[CV 1/10; 74/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 74/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.754 total time=  31.4s\n",
      "[CV 9/10; 74/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 74/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.739 total time=  31.9s\n",
      "[CV 7/10; 75/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 75/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.748 total time=  34.1s\n",
      "[CV 5/10; 76/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 76/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  37.7s\n",
      "[CV 3/10; 77/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 77/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.759 total time= 1.1min\n",
      "[CV 1/10; 78/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 78/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.754 total time= 1.1min\n",
      "[CV 9/10; 78/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 78/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.744 total time= 1.1min\n",
      "[CV 8/10; 79/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 79/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.738 total time=  56.3s\n",
      "[CV 6/10; 80/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 80/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.768 total time= 1.6min\n",
      "[CV 4/10; 81/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 81/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.740 total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n",
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n",
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n",
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/10; 72/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 72/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.744 total time=  59.6s\n",
      "[CV 2/10; 73/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 73/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.761 total time=  18.6s\n",
      "[CV 6/10; 73/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 73/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.764 total time=  18.4s\n",
      "[CV 2/10; 74/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 74/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.763 total time=  32.5s\n",
      "[CV 10/10; 74/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 74/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.730 total time=  32.8s\n",
      "[CV 8/10; 75/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 75/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.738 total time=  33.9s\n",
      "[CV 6/10; 76/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 76/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.764 total time=  37.1s\n",
      "[CV 4/10; 77/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 77/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.1min\n",
      "[CV 2/10; 78/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 78/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.760 total time= 1.1min\n",
      "[CV 10/10; 78/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 78/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.732 total time= 1.1min\n",
      "[CV 10/10; 79/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 79/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.730 total time=  56.0s\n",
      "[CV 8/10; 80/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 80/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.6min\n",
      "[CV 6/10; 81/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 81/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.774 total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n",
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 10/10; 75/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 75/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.731 total time=  32.8s\n",
      "[CV 8/10; 76/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 76/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.737 total time=  37.3s\n",
      "[CV 6/10; 77/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 77/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.768 total time= 1.0min\n",
      "[CV 4/10; 78/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 78/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.741 total time= 1.1min\n",
      "[CV 2/10; 79/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 79/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.760 total time=  57.3s\n",
      "[CV 9/10; 79/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 79/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.745 total time=  56.3s\n",
      "[CV 7/10; 80/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 80/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.6min\n",
      "[CV 5/10; 81/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 81/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.741 total time= 1.7min\n",
      "[CV 5/10; 74/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 74/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.738 total time=  32.0s\n",
      "[CV 3/10; 75/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 75/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.758 total time=  33.9s\n",
      "[CV 1/10; 76/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 76/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.754 total time=  37.5s\n",
      "[CV 9/10; 76/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 76/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.745 total time=  36.9s\n",
      "[CV 7/10; 77/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 77/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.1min\n",
      "[CV 5/10; 78/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 78/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.740 total time= 1.1min\n",
      "[CV 3/10; 79/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 79/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.754 total time=  56.2s\n",
      "[CV 1/10; 80/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 80/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.755 total time= 1.6min\n",
      "[CV 9/10; 80/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 80/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.6min\n",
      "[CV 7/10; 81/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 81/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.750 total time= 1.3min\n",
      "[CV 9/10; 70/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 70/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  49.8s\n",
      "[CV 7/10; 71/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 71/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.749 total time=  58.5s\n",
      "[CV 5/10; 72/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 72/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.735 total time=  58.2s\n",
      "[CV 3/10; 73/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 73/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.753 total time=  19.0s\n",
      "[CV 8/10; 73/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 73/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.737 total time=  18.6s\n",
      "[CV 6/10; 74/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 74/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.769 total time=  32.6s\n",
      "[CV 4/10; 75/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 75/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.741 total time=  34.7s\n",
      "[CV 2/10; 76/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 76/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.761 total time=  37.4s\n",
      "[CV 10/10; 76/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 76/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.730 total time=  38.2s\n",
      "[CV 8/10; 77/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 77/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.1min\n",
      "[CV 6/10; 78/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 78/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.774 total time= 1.1min\n",
      "[CV 4/10; 79/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 79/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.741 total time=  55.7s\n",
      "[CV 2/10; 80/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 80/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.763 total time= 1.6min\n",
      "[CV 10/10; 80/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 80/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.731 total time= 1.6min\n",
      "[CV 8/10; 81/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 81/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.739 total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n",
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n",
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 9/10; 81/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 81/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.744 total time=  55.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n",
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n",
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n",
      "/tmp/ipykernel_6634/1355284671.py:4: RuntimeWarning: invalid value encountered in log10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/10; 69/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 69/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.757 total time=  39.0s\n",
      "[CV 9/10; 69/81] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 69/81] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.745 total time=  38.6s\n",
      "[CV 7/10; 70/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 70/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.754 total time=  50.2s\n",
      "[CV 5/10; 71/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 71/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.0min\n",
      "[CV 3/10; 72/81] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 72/81] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.758 total time=  59.7s\n",
      "[CV 1/10; 73/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 73/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.754 total time=  18.3s\n",
      "[CV 5/10; 73/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 73/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.741 total time=  18.5s\n",
      "[CV 10/10; 73/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 73/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=62, objective=regression_l1;, score=0.730 total time=  18.6s\n",
      "[CV 8/10; 74/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 74/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=124, objective=regression_l1;, score=0.742 total time=  32.1s\n",
      "[CV 6/10; 75/81] START learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 75/81] END learning_rate=0.05, max_depth=15, n_estimators=1000, num_leaves=136, objective=regression_l1;, score=0.773 total time=  33.9s\n",
      "[CV 4/10; 76/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 76/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  37.2s\n",
      "[CV 2/10; 77/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 77/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.763 total time= 1.1min\n",
      "[CV 10/10; 77/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 77/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.731 total time= 1.1min\n",
      "[CV 8/10; 78/81] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 78/81] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.738 total time= 1.1min\n",
      "[CV 6/10; 79/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 79/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.765 total time=  55.3s\n",
      "[CV 4/10; 80/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 80/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.6min\n",
      "[CV 2/10; 81/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 81/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.760 total time= 1.7min\n",
      "[CV 10/10; 81/81] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 81/81] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.733 total time=  53.3s\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "lgbm = lightgbm.LGBMRegressor(random_state = 420, n_jobs = -1)\n",
    "\n",
    "# Using many estimators with low learning rates\n",
    "lgbm_params = {'learning_rate': [0.02, 0.035, 0.05],\n",
    "                           'num_leaves':[62, 124, 136],\n",
    "                           'max_depth': [5, 10, 15],\n",
    "                           'n_estimators': [1000, 2000, 3000],\n",
    "                           'objective': ['regression_l1']}\n",
    "\n",
    "lgbm_grid_search = GridSearchCV(estimator = lgbm, \n",
    "                                param_grid =  lgbm_params,\n",
    "                                cv=10,\n",
    "                                verbose = 10,\n",
    "                                scoring = make_scorer(nrmsle, greater_is_better=True),\n",
    "                                n_jobs=-1)\n",
    "\n",
    "lgbm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"The best estimator across ALL searched params:\\n\", lgbm_grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator across ALL searched params:\n",
      " LGBMRegressor(learning_rate=0.035, max_depth=15, n_estimators=2000,\n",
      "              num_leaves=124, objective='regression_l1', random_state=420)\n",
      "The best score of all model parameters' combination on model: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>63.333721</td>\n",
       "      <td>1.768293</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.750035</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>169.099287</td>\n",
       "      <td>3.776577</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749898</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>94.198981</td>\n",
       "      <td>2.552990</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.749892</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>165.170166</td>\n",
       "      <td>3.917972</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749705</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>54.247144</td>\n",
       "      <td>1.888090</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.749651</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.306918</td>\n",
       "      <td>1.344839</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 5, 'n_est...</td>\n",
       "      <td>0.742534</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.040783</td>\n",
       "      <td>1.065241</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 5, 'n_est...</td>\n",
       "      <td>0.742534</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.553992</td>\n",
       "      <td>0.314116</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 5, 'n_est...</td>\n",
       "      <td>0.742342</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.233435</td>\n",
       "      <td>0.369529</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 5, 'n_est...</td>\n",
       "      <td>0.742342</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.653285</td>\n",
       "      <td>0.180233</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 5, 'n_est...</td>\n",
       "      <td>0.742342</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  \\\n",
       "49      63.333721         1.768293   \n",
       "25     169.099287         3.776577   \n",
       "52      94.198981         2.552990   \n",
       "22     165.170166         3.917972   \n",
       "51      54.247144         1.888090   \n",
       "..            ...              ...   \n",
       "4       23.306918         1.344839   \n",
       "5       24.040783         1.065241   \n",
       "1        8.553992         0.314116   \n",
       "2       11.233435         0.369529   \n",
       "0        5.653285         0.180233   \n",
       "\n",
       "                                               params  mean_test_score  \\\n",
       "49  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.750035   \n",
       "25  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.749898   \n",
       "52  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.749892   \n",
       "22  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.749705   \n",
       "51  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.749651   \n",
       "..                                                ...              ...   \n",
       "4   {'learning_rate': 0.02, 'max_depth': 5, 'n_est...         0.742534   \n",
       "5   {'learning_rate': 0.02, 'max_depth': 5, 'n_est...         0.742534   \n",
       "1   {'learning_rate': 0.02, 'max_depth': 5, 'n_est...         0.742342   \n",
       "2   {'learning_rate': 0.02, 'max_depth': 5, 'n_est...         0.742342   \n",
       "0   {'learning_rate': 0.02, 'max_depth': 5, 'n_est...         0.742342   \n",
       "\n",
       "    rank_test_score  \n",
       "49                1  \n",
       "25                2  \n",
       "52                3  \n",
       "22                4  \n",
       "51                5  \n",
       "..              ...  \n",
       "4                76  \n",
       "5                76  \n",
       "1                79  \n",
       "2                79  \n",
       "0                79  \n",
       "\n",
       "[81 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The best estimator across ALL searched params:\\n\", lgbm_grid_search.best_estimator_)\n",
    "\n",
    "print(\"The best score of all model parameters' combination on model: {:.4f}\".format(lgbm_grid_search.best_score_))\n",
    "display(pd.DataFrame(lgbm_grid_search.cv_results_).sort_values(by=['rank_test_score'])[['mean_fit_time', 'mean_score_time', \n",
    "                                           'params', 'mean_test_score', 'rank_test_score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgbm_grid_search.best_estimator_\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='Feature importance', ylabel='Features'>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1130.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(font_scale = 1)\n",
    "lightgbm.plot_importance(model, height=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "I37sRvRz2TjM",
    "outputId": "1ca5da86-a412-460f-add1-a0ae3879319c"
   },
   "outputs": [],
   "source": [
    "# Saving-exporting the results to a .csv file\n",
    "pd.DataFrame(predictions, columns = ['Price']).to_csv('submission_lgbm_all_features.csv', index=False)\n",
    "# 0.75938"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling without the No. Authors Features: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_df[data_df['Set'] == 'train']\n",
    "y_train = X_train['Price']\n",
    "X_train =  X_train.drop(['Set', 'Price', 'No. Authors'], axis=1)\n",
    "\n",
    "X_test = data_df[data_df['Set'] == 'test']\n",
    "#y_test = X_test['Price']\n",
    "X_test =  X_test.drop(['Set', 'Price', 'No. Authors'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in X_train.select_dtypes(include=['object']):\n",
    "    X_test[col_name] = X_test[col_name].astype('category')\n",
    "    X_train[col_name] = X_train[col_name].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 54 candidates, totalling 540 fits\n",
      "[CV 7/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.751 total time=  43.2s\n",
      "[CV 6/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.769 total time=  46.2s\n",
      "[CV 9/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.744 total time=  51.0s\n",
      "[CV 7/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.750 total time=  53.6s\n",
      "[CV 5/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.741 total time=  54.5s\n",
      "[CV 3/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.756 total time=  58.8s\n",
      "[CV 1/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.754 total time=  54.4s\n",
      "[CV 10/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.730 total time=  55.0s\n",
      "[CV 8/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.739 total time= 1.1min\n",
      "[CV 6/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.770 total time= 1.3min\n",
      "[CV 4/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.743 total time= 1.8min\n",
      "[CV 2/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.762 total time=  52.1s\n",
      "[CV 7/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.750 total time=  44.8s\n",
      "[CV 2/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.763 total time= 1.4min\n",
      "[CV 1/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.756 total time= 1.2min\n",
      "[CV 9/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.742 total time= 1.1min\n",
      "[CV 10/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.731 total time= 1.1min\n",
      "[CV 8/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.7min\n",
      "[CV 4/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.743 total time= 1.8min\n",
      "[CV 1/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.755 total time= 1.5min\n",
      "[CV 7/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.750 total time= 1.3min\n",
      "[CV 5/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.3min\n",
      "[CV 3/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.756 total time= 2.3min\n",
      "[CV 2/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.762 total time=  31.9s\n",
      "[CV 6/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.767 total time=  32.3s\n",
      "[CV 2/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.761 total time=  36.7s\n",
      "[CV 9/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.743 total time=  31.7s\n",
      "[CV 7/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.749 total time=  31.4s\n",
      "[CV 5/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.738 total time=  34.6s\n",
      "[CV 3/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.759 total time=  43.5s\n",
      "[CV 1/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.755 total time=  41.7s\n",
      "[CV 3/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.756 total time=  39.1s\n",
      "[CV 9/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.743 total time=  43.0s\n",
      "[CV 7/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.749 total time=  55.4s\n",
      "[CV 5/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.738 total time=  52.7s\n",
      "[CV 3/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.757 total time=  55.9s\n",
      "[CV 1/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.754 total time=  56.7s\n",
      "[CV 9/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.742 total time=  54.5s\n",
      "[CV 7/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.751 total time=  54.0s\n",
      "[CV 5/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.741 total time= 1.1min\n",
      "[CV 3/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.4min\n",
      "[CV 3/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.760 total time= 1.8min\n",
      "[CV 1/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.755 total time=  53.3s\n",
      "[CV 6/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.766 total time=  51.5s\n",
      "[CV 3/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.759 total time= 1.3min\n",
      "[CV 10/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.731 total time= 1.2min\n",
      "[CV 8/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.738 total time= 1.1min\n",
      "[CV 8/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.739 total time= 1.0min\n",
      "[CV 5/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.7min\n",
      "[CV 2/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.761 total time= 1.8min\n",
      "[CV 10/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.734 total time= 1.8min\n",
      "[CV 9/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.745 total time= 1.3min\n",
      "[CV 7/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.753 total time= 2.2min\n",
      "[CV 5/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.740 total time= 2.3min\n",
      "[CV 3/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.756 total time=  32.3s\n",
      "[CV 7/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.748 total time=  31.1s\n",
      "[CV 3/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.758 total time=  38.1s\n",
      "[CV 1/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.756 total time=  30.9s\n",
      "[CV 9/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.745 total time=  31.1s\n",
      "[CV 7/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.748 total time=  34.3s\n",
      "[CV 4/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.745 total time=  41.4s\n",
      "[CV 2/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.763 total time=  41.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.755 total time=  42.1s\n",
      "[CV 2/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.762 total time=  54.0s\n",
      "[CV 1/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.753 total time=  51.6s\n",
      "[CV 9/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.744 total time=  52.7s\n",
      "[CV 7/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.751 total time=  53.3s\n",
      "[CV 5/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.740 total time=  56.1s\n",
      "[CV 4/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.745 total time=  55.0s\n",
      "[CV 2/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.762 total time= 1.1min\n",
      "[CV 10/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.730 total time= 1.1min\n",
      "[CV 8/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.3min\n",
      "[CV 6/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.771 total time= 1.8min\n",
      "[CV 4/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.741 total time=  57.2s\n",
      "[CV 10/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.731 total time=  43.7s\n",
      "[CV 6/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.770 total time= 1.3min\n",
      "[CV 3/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.755 total time= 1.3min\n",
      "[CV 1/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.755 total time=  49.5s\n",
      "[CV 6/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.766 total time= 1.1min\n",
      "[CV 6/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.770 total time= 1.8min\n",
      "[CV 5/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.740 total time= 1.9min\n",
      "[CV 3/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.759 total time= 1.6min\n",
      "[CV 10/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.732 total time= 1.3min\n",
      "[CV 8/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.2min\n",
      "[CV 6/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.771 total time= 2.3min\n",
      "[CV 4/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.743 total time=  32.0s\n",
      "[CV 8/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.737 total time=  31.8s\n",
      "[CV 4/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.745 total time=  35.9s\n",
      "[CV 2/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.763 total time=  31.5s\n",
      "[CV 10/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.729 total time=  32.1s\n",
      "[CV 8/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.737 total time=  34.3s\n",
      "[CV 6/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.769 total time=  41.6s\n",
      "[CV 3/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.759 total time=  41.8s\n",
      "[CV 5/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.740 total time=  40.8s\n",
      "[CV 1/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.754 total time=  52.5s\n",
      "[CV 10/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.730 total time=  53.8s\n",
      "[CV 8/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.738 total time=  49.0s\n",
      "[CV 6/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.767 total time=  53.1s\n",
      "[CV 4/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.744 total time=  58.7s\n",
      "[CV 2/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.764 total time=  52.5s\n",
      "[CV 9/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.744 total time=  52.9s\n",
      "[CV 7/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.752 total time= 1.1min\n",
      "[CV 4/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.3min\n",
      "[CV 1/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.754 total time= 1.7min\n",
      "[CV 9/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.745 total time= 2.1min\n",
      "[CV 7/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.753 total time= 1.3min\n",
      "[CV 5/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.740 total time= 1.3min\n",
      "[CV 3/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.758 total time=  50.5s\n",
      "[CV 9/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.745 total time= 1.0min\n",
      "[CV 7/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.752 total time= 1.8min\n",
      "[CV 3/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.756 total time= 1.9min\n",
      "[CV 2/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.761 total time= 1.5min\n",
      "[CV 8/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.740 total time= 1.3min\n",
      "[CV 6/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.771 total time= 2.2min\n",
      "[CV 4/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.743 total time= 2.2min\n",
      "[CV 1/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.752 total time=  32.9s\n",
      "[CV 5/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.738 total time=  32.6s\n",
      "[CV 1/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.755 total time=  39.6s\n",
      "[CV 10/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.731 total time=  32.9s\n",
      "[CV 8/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.737 total time=  31.2s\n",
      "[CV 6/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.767 total time=  34.7s\n",
      "[CV 5/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.738 total time=  44.3s\n",
      "[CV 4/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  42.8s\n",
      "[CV 2/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.762 total time=  51.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 8/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.738 total time=  42.3s\n",
      "[CV 5/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.740 total time=  56.6s\n",
      "[CV 4/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.745 total time=  50.4s\n",
      "[CV 1/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.755 total time=  56.8s\n",
      "[CV 9/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.744 total time=  52.9s\n",
      "[CV 7/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.749 total time=  53.7s\n",
      "[CV 5/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  54.1s\n",
      "[CV 3/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.757 total time= 1.1min\n",
      "[CV 1/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.755 total time= 1.4min\n",
      "[CV 9/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.4min\n",
      "[CV 7/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.752 total time= 2.2min\n",
      "[CV 8/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.739 total time=  48.5s\n",
      "[CV 4/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.4min\n",
      "[CV 4/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.743 total time= 1.3min\n",
      "[CV 2/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.761 total time=  49.5s\n",
      "[CV 7/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.750 total time= 1.0min\n",
      "[CV 4/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.0min\n",
      "[CV 6/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.771 total time= 1.8min\n",
      "[CV 4/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.741 total time= 1.6min\n",
      "[CV 1/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 2.2min\n",
      "[CV 9/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.1min\n",
      "[CV 7/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.750 total time= 2.1min\n",
      "[CV 9/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.744 total time=  31.0s\n",
      "[CV 5/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.738 total time=  35.5s\n",
      "[CV 3/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.758 total time=  29.8s\n",
      "[CV 1/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.752 total time=  33.9s\n",
      "[CV 9/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.745 total time=  34.7s\n",
      "[CV 7/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.750 total time=  40.2s\n",
      "[CV 5/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  43.7s\n",
      "[CV 3/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.756 total time=  51.5s\n",
      "[CV 1/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.1min\n",
      "[CV 6/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.766 total time=  41.5s\n",
      "[CV 3/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.756 total time=  54.4s\n",
      "[CV 2/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.764 total time=  51.3s\n",
      "[CV 10/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.729 total time=  53.4s\n",
      "[CV 8/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.738 total time=  55.5s\n",
      "[CV 6/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.770 total time=  52.4s\n",
      "[CV 3/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.758 total time=  53.7s\n",
      "[CV 1/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.756 total time= 1.1min\n",
      "[CV 9/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.744 total time= 1.0min\n",
      "[CV 7/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.3min\n",
      "[CV 5/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.739 total time= 1.8min\n",
      "[CV 3/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.758 total time=  54.5s\n",
      "[CV 9/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.744 total time=  47.4s\n",
      "[CV 5/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.3min\n",
      "[CV 2/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.761 total time= 1.2min\n",
      "[CV 10/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.733 total time= 1.1min\n",
      "[CV 2/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.763 total time= 1.7min\n",
      "[CV 10/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.732 total time= 1.8min\n",
      "[CV 8/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.738 total time= 2.0min\n",
      "[CV 6/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.767 total time= 1.3min\n",
      "[CV 4/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.3min\n",
      "[CV 2/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.761 total time= 2.3min\n",
      "[CV 10/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.734 total time= 2.0min\n",
      "[CV 8/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.738 total time=  32.5s\n",
      "[CV 6/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.769 total time=  30.0s\n",
      "[CV 3/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.756 total time=  34.1s\n",
      "[CV 1/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.755 total time=  43.4s\n",
      "[CV 10/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.731 total time=  42.8s\n",
      "[CV 8/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.737 total time=  40.4s\n",
      "[CV 6/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.767 total time=  51.0s\n",
      "[CV 4/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.745 total time= 1.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.743 total time=  42.1s\n",
      "[CV 4/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.744 total time=  54.3s\n",
      "[CV 3/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.758 total time=  53.7s\n",
      "[CV 2/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.763 total time=  56.9s\n",
      "[CV 10/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.730 total time=  55.1s\n",
      "[CV 8/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.740 total time=  52.6s\n",
      "[CV 6/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.770 total time=  54.7s\n",
      "[CV 4/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.743 total time= 1.1min\n",
      "[CV 2/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.763 total time= 1.3min\n",
      "[CV 10/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.731 total time= 1.4min\n",
      "[CV 8/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.738 total time= 2.0min\n",
      "[CV 5/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.739 total time=  46.0s\n",
      "[CV 1/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.3min\n",
      "[CV 9/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.2min\n",
      "[CV 7/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.750 total time= 1.1min\n",
      "[CV 5/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.739 total time=  57.0s\n",
      "[CV 3/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.759 total time= 1.9min\n",
      "[CV 1/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.756 total time= 1.8min\n",
      "[CV 9/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.741 total time= 1.9min\n",
      "[CV 2/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.763 total time= 2.3min\n",
      "[CV 10/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.732 total time= 2.2min\n",
      "[CV 8/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.738 total time= 2.1min\n",
      "[CV 10/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.732 total time=  32.0s\n",
      "[CV 7/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.750 total time=  32.4s\n",
      "[CV 5/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.739 total time=  32.7s\n",
      "[CV 4/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  34.3s\n",
      "[CV 2/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.760 total time=  40.1s\n",
      "[CV 9/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.744 total time=  41.3s\n",
      "[CV 7/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.750 total time=  41.7s\n",
      "[CV 5/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.738 total time=  51.9s\n",
      "[CV 3/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.759 total time= 1.1min\n",
      "[CV 2/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.763 total time=  40.8s\n",
      "[CV 10/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.729 total time=  42.6s\n",
      "[CV 8/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.739 total time=  54.6s\n",
      "[CV 6/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.769 total time=  55.0s\n",
      "[CV 4/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  53.6s\n",
      "[CV 2/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.762 total time=  57.9s\n",
      "[CV 10/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.730 total time=  54.2s\n",
      "[CV 8/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  54.7s\n",
      "[CV 6/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.768 total time= 1.1min\n",
      "[CV 5/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.3min\n",
      "[CV 2/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.764 total time= 1.8min\n",
      "[CV 10/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.730 total time= 2.1min\n",
      "[CV 8/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.3min\n",
      "[CV 6/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.771 total time= 1.2min\n",
      "[CV 4/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.740 total time=  49.4s\n",
      "[CV 1/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.6min\n",
      "[CV 9/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.9min\n",
      "[CV 7/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.750 total time= 1.9min\n",
      "[CV 5/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.739 total time= 1.4min\n",
      "[CV 3/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.760 total time= 2.2min\n",
      "[CV 1/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.757 total time= 2.4min\n",
      "[CV 9/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.742 total time= 2.0min\n",
      "[CV 6/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.769 total time=  34.0s\n",
      "[CV 4/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.739 total time=  32.5s\n",
      "[CV 2/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.762 total time=  34.0s\n",
      "[CV 10/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.733 total time=  35.3s\n",
      "[CV 8/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.738 total time=  40.3s\n",
      "[CV 6/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.769 total time=  40.3s\n",
      "[CV 4/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  50.7s\n",
      "[CV 2/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.760 total time=  59.4s\n",
      "[CV 9/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 9/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.745 total time=  40.7s\n",
      "[CV 7/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.748 total time=  52.2s\n",
      "[CV 6/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.769 total time= 1.0min\n",
      "[CV 3/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.759 total time= 1.0min\n",
      "[CV 1/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.756 total time=  29.1s\n",
      "[CV 5/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.739 total time=  28.1s\n",
      "[CV 3/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.756 total time=  50.4s\n",
      "[CV 1/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.757 total time=  53.3s\n",
      "[CV 9/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.743 total time=  50.8s\n",
      "[CV 9/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.745 total time=  38.0s\n",
      "[CV 7/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.751 total time= 1.1min\n",
      "[CV 5/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.740 total time= 1.2min\n",
      "[CV 3/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.759 total time= 1.7min\n",
      "[CV 10/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.733 total time= 1.7min\n",
      "[CV 7/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.751 total time= 2.5min\n",
      "[CV 5/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.741 total time= 2.8min\n",
      "[CV 1/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.754 total time=  43.3s\n",
      "[CV 6/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.769 total time=  41.2s\n",
      "[CV 1/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.754 total time=  50.0s\n",
      "[CV 9/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.745 total time=  46.2s\n",
      "[CV 7/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.750 total time=  49.5s\n",
      "[CV 5/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.736 total time=  50.3s\n",
      "[CV 4/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time=  52.1s\n",
      "[CV 10/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.728 total time=  44.8s\n",
      "[CV 8/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.738 total time=  49.9s\n",
      "[CV 6/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.770 total time=  57.8s\n",
      "[CV 4/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.4min\n",
      "[CV 4/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.745 total time= 1.7min\n",
      "[CV 2/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.763 total time=  50.1s\n",
      "[CV 6/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.769 total time=  46.5s\n",
      "[CV 2/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.762 total time= 1.6min\n",
      "[CV 10/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.730 total time=  41.4s\n",
      "[CV 8/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.738 total time=  50.8s\n",
      "[CV 5/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.1min\n",
      "[CV 4/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.739 total time= 1.0min\n",
      "[CV 2/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.763 total time=  28.8s\n",
      "[CV 6/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.770 total time=  27.3s\n",
      "[CV 4/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.743 total time=  50.6s\n",
      "[CV 2/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.760 total time=  52.8s\n",
      "[CV 10/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.734 total time=  50.7s\n",
      "[CV 10/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.732 total time=  38.1s\n",
      "[CV 8/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.1min\n",
      "[CV 6/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.748 total time= 1.2min\n",
      "[CV 4/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.743 total time= 1.8min\n",
      "[CV 2/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.760 total time= 2.8min\n",
      "[CV 10/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.733 total time= 2.7min\n",
      "[CV 7/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.750 total time= 2.7min\n",
      "[CV 9/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.740 total time=  43.2s\n",
      "[CV 5/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.737 total time=  47.6s\n",
      "[CV 4/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.745 total time=  50.8s\n",
      "[CV 2/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.759 total time=  50.1s\n",
      "[CV 9/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.746 total time=  46.8s\n",
      "[CV 7/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.750 total time=  47.3s\n",
      "[CV 5/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.739 total time=  46.9s\n",
      "[CV 3/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.755 total time=  57.4s\n",
      "[CV 1/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.754 total time= 1.3min\n",
      "[CV 9/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.745 total time= 1.5min\n",
      "[CV 7/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.751 total time= 1.6min\n",
      "[CV 7/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.750 total time=  46.6s\n",
      "[CV 5/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.8min\n",
      "[CV 5/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.740 total time= 1.8min\n",
      "[CV 1/10; 49/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 49/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.754 total time= 1.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.753 total time=  51.9s\n",
      "[CV 9/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.744 total time=  50.6s\n",
      "[CV 7/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.0min\n",
      "[CV 5/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.739 total time= 1.1min\n",
      "[CV 4/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.743 total time=  27.9s\n",
      "[CV 9/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.746 total time=  29.4s\n",
      "[CV 8/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.739 total time=  51.3s\n",
      "[CV 6/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.763 total time=  53.3s\n",
      "[CV 4/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  37.7s\n",
      "[CV 1/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.755 total time= 1.1min\n",
      "[CV 9/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.1min\n",
      "[CV 7/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.750 total time= 1.5min\n",
      "[CV 5/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.740 total time= 1.8min\n",
      "[CV 4/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.7min\n",
      "[CV 2/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.759 total time= 2.8min\n",
      "[CV 10/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.735 total time= 2.7min\n",
      "[CV 8/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.738 total time=  47.4s\n",
      "[CV 6/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.758 total time=  48.8s\n",
      "[CV 4/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  51.5s\n",
      "[CV 3/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.759 total time=  55.0s\n",
      "[CV 2/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.761 total time=  45.5s\n",
      "[CV 10/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.730 total time=  47.6s\n",
      "[CV 8/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.740 total time= 1.0min\n",
      "[CV 6/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.769 total time= 1.4min\n",
      "[CV 3/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.752 total time= 1.7min\n",
      "[CV 1/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.754 total time=  51.6s\n",
      "[CV 5/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.742 total time=  48.2s\n",
      "[CV 4/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.5min\n",
      "[CV 10/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.8min\n",
      "[CV 8/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.737 total time= 1.8min\n",
      "[CV 9/10; 49/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 49/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.742 total time= 1.4min\n",
      "[CV 10/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.732 total time= 1.1min\n",
      "[CV 8/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.737 total time= 1.0min\n",
      "[CV 8/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.741 total time=  27.5s\n",
      "[CV 5/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.741 total time=  51.3s\n",
      "[CV 4/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.745 total time=  52.4s\n",
      "[CV 1/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.756 total time=  38.6s\n",
      "[CV 7/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.750 total time=  38.7s\n",
      "[CV 5/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.1min\n",
      "[CV 4/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.743 total time= 1.2min\n",
      "[CV 1/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.757 total time= 1.6min\n",
      "[CV 8/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.742 total time= 1.7min\n",
      "[CV 5/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 2.6min\n",
      "[CV 3/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.757 total time= 2.9min\n",
      "[CV 4/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.741 total time=  45.1s\n",
      "[CV 8/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.740 total time=  49.2s\n",
      "[CV 4/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.743 total time=  43.7s\n",
      "[CV 2/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.762 total time=  49.6s\n",
      "[CV 10/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.730 total time=  50.6s\n",
      "[CV 8/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.740 total time=  47.7s\n",
      "[CV 6/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.769 total time=  48.0s\n",
      "[CV 4/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.745 total time=  43.9s\n",
      "[CV 2/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.759 total time= 1.0min\n",
      "[CV 10/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.730 total time= 1.1min\n",
      "[CV 8/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.5min\n",
      "[CV 6/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.769 total time= 1.8min\n",
      "[CV 4/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.740 total time=  45.0s\n",
      "[CV 9/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.742 total time=  46.2s\n",
      "[CV 7/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.752 total time= 1.5min\n",
      "[CV 4/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.742 total time= 1.9min\n",
      "[CV 2/10; 49/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 49/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.762 total time= 1.4min\n",
      "[CV 1/10; 50/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 50/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 2.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 10/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.733 total time=  52.4s\n",
      "[CV 8/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.0min\n",
      "[CV 6/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.769 total time= 1.0min\n",
      "[CV 3/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.759 total time=  28.7s\n",
      "[CV 10/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.731 total time=  29.0s\n",
      "[CV 7/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.751 total time=  50.5s\n",
      "[CV 5/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.740 total time=  54.2s\n",
      "[CV 3/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.759 total time=  38.9s\n",
      "[CV 2/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.760 total time= 1.1min\n",
      "[CV 10/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.732 total time= 1.1min\n",
      "[CV 8/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.742 total time= 1.5min\n",
      "[CV 6/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.770 total time= 1.8min\n",
      "[CV 3/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 2.6min\n",
      "[CV 1/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.758 total time= 2.8min\n",
      "[CV 9/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.743 total time= 2.6min\n",
      "[CV 7/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.750 total time=  48.2s\n",
      "[CV 5/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.738 total time=  48.0s\n",
      "[CV 3/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.756 total time=  48.8s\n",
      "[CV 1/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.754 total time=  55.3s\n",
      "[CV 9/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.745 total time=  43.9s\n",
      "[CV 7/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.750 total time=  45.9s\n",
      "[CV 5/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.737 total time=  57.7s\n",
      "[CV 3/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.759 total time= 1.4min\n",
      "[CV 1/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.754 total time= 1.6min\n",
      "[CV 9/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.747 total time= 1.7min\n",
      "[CV 10/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.731 total time=  51.2s\n",
      "[CV 8/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.7min\n",
      "[CV 6/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.771 total time= 1.6min\n",
      "[CV 3/10; 49/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 49/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.757 total time= 1.3min\n",
      "[CV 7/10; 49/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 49/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.750 total time= 1.3min\n",
      "[CV 6/10; 50/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 50/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.772 total time= 2.0min\n",
      "[CV 7/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.750 total time= 1.1min\n",
      "[CV 7/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.750 total time=  29.0s\n",
      "[CV 6/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.769 total time=  51.2s\n",
      "[CV 3/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.757 total time=  53.9s\n",
      "[CV 2/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.763 total time=  38.4s\n",
      "[CV 8/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.741 total time=  37.5s\n",
      "[CV 6/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.769 total time= 1.1min\n",
      "[CV 3/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.757 total time= 1.2min\n",
      "[CV 2/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.762 total time= 1.6min\n",
      "[CV 7/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.750 total time= 1.7min\n",
      "[CV 6/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.770 total time= 2.6min\n",
      "[CV 4/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.743 total time= 2.8min\n",
      "[CV 2/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.759 total time=  41.2s\n",
      "[CV 5/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.737 total time=  43.6s\n",
      "[CV 2/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.762 total time=  51.4s\n",
      "[CV 1/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.753 total time=  49.6s\n",
      "[CV 9/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.747 total time=  48.8s\n",
      "[CV 7/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.746 total time=  49.0s\n",
      "[CV 5/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.737 total time=  51.4s\n",
      "[CV 3/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.752 total time=  48.5s\n",
      "[CV 1/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.754 total time= 1.0min\n",
      "[CV 9/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.733 total time= 1.0min\n",
      "[CV 7/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.4min\n",
      "[CV 5/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.739 total time= 1.7min\n",
      "[CV 3/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.757 total time=  48.5s\n",
      "[CV 8/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.737 total time=  46.8s\n",
      "[CV 6/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.760 total time= 1.6min\n",
      "[CV 3/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.757 total time= 2.0min\n",
      "[CV 4/10; 49/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 49/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.740 total time= 1.3min\n",
      "[CV 10/10; 49/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 49/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.732 total time= 1.4min\n",
      "[CV 8/10; 50/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 50/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.762 total time= 1.0min\n",
      "[CV 10/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.731 total time= 1.0min\n",
      "[CV 2/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.760 total time=  50.3s\n",
      "[CV 10/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.732 total time=  50.4s\n",
      "[CV 8/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.742 total time=  51.5s\n",
      "[CV 6/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.770 total time=  37.5s\n",
      "[CV 4/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.1min\n",
      "[CV 2/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.759 total time= 1.2min\n",
      "[CV 10/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.734 total time= 1.8min\n",
      "[CV 9/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.746 total time= 1.7min\n",
      "[CV 8/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.6min\n",
      "[CV 6/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.770 total time= 2.8min\n",
      "[CV 3/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.755 total time=  40.2s\n",
      "[CV 7/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.746 total time=  44.6s\n",
      "[CV 3/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.759 total time=  45.6s\n",
      "[CV 10/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.728 total time=  47.6s\n",
      "[CV 8/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.738 total time=  49.9s\n",
      "[CV 6/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.770 total time=  46.8s\n",
      "[CV 2/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.762 total time=  53.5s\n",
      "[CV 1/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.753 total time=  44.8s\n",
      "[CV 9/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.747 total time=  49.9s\n",
      "[CV 7/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.746 total time=  58.3s\n",
      "[CV 5/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.737 total time= 1.4min\n",
      "[CV 2/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.761 total time= 1.7min\n",
      "[CV 10/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.731 total time= 1.7min\n",
      "[CV 3/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.5min\n",
      "[CV 1/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.754 total time= 1.9min\n",
      "[CV 10/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.731 total time= 1.7min\n",
      "[CV 6/10; 49/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 49/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.769 total time= 1.3min\n",
      "[CV 5/10; 50/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 50/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 2.1min\n",
      "[CV 3/10; 51/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 51/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.758 total time= 1.9min\n",
      "[CV 1/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.756 total time= 1.0min\n",
      "[CV 9/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.746 total time= 1.0min\n",
      "[CV 1/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.754 total time=  50.0s\n",
      "[CV 9/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.744 total time=  51.4s\n",
      "[CV 7/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.749 total time=  51.0s\n",
      "[CV 5/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.739 total time=  38.3s\n",
      "[CV 3/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.1min\n",
      "[CV 1/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.757 total time= 1.2min\n",
      "[CV 9/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.743 total time= 1.8min\n",
      "[CV 1/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.755 total time= 2.8min\n",
      "[CV 9/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 2.8min\n",
      "[CV 8/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.742 total time= 2.7min\n",
      "[CV 10/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.729 total time=  44.6s\n",
      "[CV 6/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.769 total time=  45.3s\n",
      "[CV 3/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.752 total time=  51.0s\n",
      "[CV 1/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.754 total time=  50.7s\n",
      "[CV 10/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.729 total time=  48.2s\n",
      "[CV 8/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.738 total time=  48.0s\n",
      "[CV 6/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.769 total time=  47.4s\n",
      "[CV 4/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.741 total time=  58.1s\n",
      "[CV 2/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.761 total time= 1.3min\n",
      "[CV 10/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.729 total time= 1.7min\n",
      "[CV 8/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.738 total time= 1.7min\n",
      "[CV 1/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.755 total time= 1.4min\n",
      "[CV 9/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.6min\n",
      "[CV 7/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.751 total time= 1.8min\n",
      "[CV 5/10; 49/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 49/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.742 total time= 1.4min\n",
      "[CV 3/10; 50/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 50/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 2.2min\n",
      "[CV 1/10; 51/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 51/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.755 total time= 1.9min\n",
      "[CV 9/10; 51/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 51/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.742 total time= 1.9min\n",
      "[CV 7/10; 52/54] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 52/54] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.749 total time= 1.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator across ALL searched params:\n",
      " LGBMRegressor(learning_rate=0.02, max_depth=15, n_estimators=3000,\n",
      "              num_leaves=124, objective='regression_l1', random_state=420)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "lgbm = lightgbm.LGBMRegressor(random_state = 420, n_jobs = -1)\n",
    "\n",
    "# Using many estimators with low learning rates\n",
    "lgbm_params = {'learning_rate': [0.02, 0.035, 0.05],\n",
    "                           'num_leaves':[62, 124, 136],\n",
    "                           'max_depth': [10, 15],\n",
    "                           'n_estimators': [1500, 2000, 3000],\n",
    "                           'objective': ['regression_l1']}\n",
    "\n",
    "lgbm_grid_search = GridSearchCV(estimator = lgbm, \n",
    "                                param_grid =  lgbm_params,\n",
    "                                cv=10,\n",
    "                                verbose = 10,\n",
    "                                scoring = make_scorer(nrmsle, greater_is_better=True),\n",
    "                                n_jobs=-1)\n",
    "\n",
    "lgbm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"The best estimator across ALL searched params:\\n\", lgbm_grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator across ALL searched params:\n",
      " LGBMRegressor(learning_rate=0.02, max_depth=15, n_estimators=3000,\n",
      "              num_leaves=124, objective='regression_l1', random_state=420)\n",
      "The best score of all model parameters' combination on model: 0.7505\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>130.517815</td>\n",
       "      <td>2.595433</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.750464</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>104.268577</td>\n",
       "      <td>2.379258</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.750107</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>99.642117</td>\n",
       "      <td>2.961967</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.750049</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>36.976591</td>\n",
       "      <td>1.206871</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.749940</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>77.763356</td>\n",
       "      <td>1.316621</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749867</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>162.411212</td>\n",
       "      <td>3.128679</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.749798</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27.516474</td>\n",
       "      <td>0.968823</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.749703</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>110.726966</td>\n",
       "      <td>3.181831</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.749568</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>156.414271</td>\n",
       "      <td>3.345154</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.749330</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>52.866065</td>\n",
       "      <td>1.243552</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.749288</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>129.290871</td>\n",
       "      <td>2.099692</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749252</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>79.239705</td>\n",
       "      <td>2.425226</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.749231</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>120.939614</td>\n",
       "      <td>2.121020</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749186</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>64.679329</td>\n",
       "      <td>1.715871</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.749183</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>62.849923</td>\n",
       "      <td>1.641084</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.749131</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>119.844300</td>\n",
       "      <td>2.442081</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749121</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>114.470626</td>\n",
       "      <td>2.331438</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749096</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>59.845036</td>\n",
       "      <td>2.187701</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.749070</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>51.110671</td>\n",
       "      <td>1.293349</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.749031</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>49.407247</td>\n",
       "      <td>1.334153</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.748966</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>110.461029</td>\n",
       "      <td>2.529943</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748962</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>40.434425</td>\n",
       "      <td>1.463642</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.748919</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51.007300</td>\n",
       "      <td>1.334602</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.748918</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>107.264242</td>\n",
       "      <td>1.972744</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748886</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>71.125471</td>\n",
       "      <td>1.141064</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748815</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>33.839675</td>\n",
       "      <td>1.079191</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.748787</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>59.726428</td>\n",
       "      <td>2.117674</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.748783</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.191091</td>\n",
       "      <td>1.583659</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.748779</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>83.552937</td>\n",
       "      <td>1.832924</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748717</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54.301364</td>\n",
       "      <td>1.250409</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.748698</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>98.494187</td>\n",
       "      <td>3.043891</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.748678</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>79.051498</td>\n",
       "      <td>1.807042</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748638</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>45.368629</td>\n",
       "      <td>1.646716</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.748575</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>98.639450</td>\n",
       "      <td>2.757356</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748556</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>40.212291</td>\n",
       "      <td>1.434315</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.748531</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>55.343286</td>\n",
       "      <td>1.693679</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748516</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51.695470</td>\n",
       "      <td>1.582272</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.748515</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46.603047</td>\n",
       "      <td>1.466473</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748514</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>83.305547</td>\n",
       "      <td>2.655305</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.748456</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>48.353283</td>\n",
       "      <td>1.565441</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.748419</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>30.236651</td>\n",
       "      <td>1.084737</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.748367</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>48.675829</td>\n",
       "      <td>1.218536</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748348</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.410215</td>\n",
       "      <td>1.332461</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.748335</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>45.669340</td>\n",
       "      <td>1.632928</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.748327</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>49.487279</td>\n",
       "      <td>1.954839</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.748260</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>33.168598</td>\n",
       "      <td>1.247356</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.748094</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>31.176602</td>\n",
       "      <td>0.821275</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.748011</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>94.029789</td>\n",
       "      <td>2.275601</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.747950</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>47.534398</td>\n",
       "      <td>1.451408</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.747755</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>80.810853</td>\n",
       "      <td>2.133792</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.747429</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>138.190899</td>\n",
       "      <td>2.688690</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.747403</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>48.268155</td>\n",
       "      <td>1.399578</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.747293</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>42.323633</td>\n",
       "      <td>1.296560</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.747030</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>57.391747</td>\n",
       "      <td>2.546739</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.746440</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  \\\n",
       "16     130.517815         2.595433   \n",
       "13     104.268577         2.379258   \n",
       "33      99.642117         2.961967   \n",
       "30      36.976591         1.206871   \n",
       "10      77.763356         1.316621   \n",
       "35     162.411212         3.128679   \n",
       "27      27.516474         0.968823   \n",
       "8      110.726966         3.181831   \n",
       "34     156.414271         3.345154   \n",
       "5       52.866065         1.243552   \n",
       "17     129.290871         2.099692   \n",
       "7       79.239705         2.425226   \n",
       "53     120.939614         2.121020   \n",
       "31      64.679329         1.715871   \n",
       "6       62.849923         1.641084   \n",
       "49     119.844300         2.442081   \n",
       "50     114.470626         2.331438   \n",
       "25      59.845036         2.187701   \n",
       "29      51.110671         1.293349   \n",
       "28      49.407247         1.334153   \n",
       "14     110.461029         2.529943   \n",
       "22      40.434425         1.463642   \n",
       "2       51.007300         1.334602   \n",
       "47     107.264242         1.972744   \n",
       "11      71.125471         1.141064   \n",
       "19      33.839675         1.079191   \n",
       "26      59.726428         2.117674   \n",
       "3       53.191091         1.583659   \n",
       "15      83.552937         1.832924   \n",
       "4       54.301364         1.250409   \n",
       "44      98.494187         3.043891   \n",
       "48      79.051498         1.807042   \n",
       "41      45.368629         1.646716   \n",
       "51      98.639450         2.757356   \n",
       "23      40.212291         1.434315   \n",
       "12      55.343286         1.693679   \n",
       "1       51.695470         1.582272   \n",
       "45      46.603047         1.466473   \n",
       "43      83.305547         2.655305   \n",
       "40      48.353283         1.565441   \n",
       "20      30.236651         1.084737   \n",
       "9       48.675829         1.218536   \n",
       "0       40.410215         1.332461   \n",
       "37      45.669340         1.632928   \n",
       "24      49.487279         1.954839   \n",
       "21      33.168598         1.247356   \n",
       "18      31.176602         0.821275   \n",
       "46      94.029789         2.275601   \n",
       "39      47.534398         1.451408   \n",
       "32      80.810853         2.133792   \n",
       "52     138.190899         2.688690   \n",
       "38      48.268155         1.399578   \n",
       "36      42.323633         1.296560   \n",
       "42      57.391747         2.546739   \n",
       "\n",
       "                                               params  mean_test_score  \\\n",
       "16  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.750464   \n",
       "13  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.750107   \n",
       "33  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.750049   \n",
       "30  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.749940   \n",
       "10  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.749867   \n",
       "35  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.749798   \n",
       "27  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.749703   \n",
       "8   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.749568   \n",
       "34  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.749330   \n",
       "5   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.749288   \n",
       "17  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.749252   \n",
       "7   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.749231   \n",
       "53  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.749186   \n",
       "31  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.749183   \n",
       "6   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.749131   \n",
       "49  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.749121   \n",
       "50  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.749096   \n",
       "25  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.749070   \n",
       "29  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.749031   \n",
       "28  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.748966   \n",
       "14  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.748962   \n",
       "22  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.748919   \n",
       "2   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.748918   \n",
       "47  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.748886   \n",
       "11  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.748815   \n",
       "19  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.748787   \n",
       "26  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.748783   \n",
       "3   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.748779   \n",
       "15  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.748717   \n",
       "4   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.748698   \n",
       "44  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.748678   \n",
       "48  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.748638   \n",
       "41  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.748575   \n",
       "51  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.748556   \n",
       "23  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.748531   \n",
       "12  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.748516   \n",
       "1   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.748515   \n",
       "45  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.748514   \n",
       "43  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.748456   \n",
       "40  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.748419   \n",
       "20  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.748367   \n",
       "9   {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.748348   \n",
       "0   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.748335   \n",
       "37  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.748327   \n",
       "24  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.748260   \n",
       "21  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.748094   \n",
       "18  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.748011   \n",
       "46  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.747950   \n",
       "39  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.747755   \n",
       "32  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.747429   \n",
       "52  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.747403   \n",
       "38  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.747293   \n",
       "36  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.747030   \n",
       "42  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.746440   \n",
       "\n",
       "    rank_test_score  \n",
       "16                1  \n",
       "13                2  \n",
       "33                3  \n",
       "30                4  \n",
       "10                5  \n",
       "35                6  \n",
       "27                7  \n",
       "8                 8  \n",
       "34                9  \n",
       "5                10  \n",
       "17               11  \n",
       "7                12  \n",
       "53               13  \n",
       "31               14  \n",
       "6                15  \n",
       "49               16  \n",
       "50               17  \n",
       "25               18  \n",
       "29               19  \n",
       "28               20  \n",
       "14               21  \n",
       "22               22  \n",
       "2                23  \n",
       "47               24  \n",
       "11               25  \n",
       "19               26  \n",
       "26               27  \n",
       "3                28  \n",
       "15               29  \n",
       "4                30  \n",
       "44               31  \n",
       "48               32  \n",
       "41               33  \n",
       "51               34  \n",
       "23               35  \n",
       "12               36  \n",
       "1                37  \n",
       "45               38  \n",
       "43               39  \n",
       "40               40  \n",
       "20               41  \n",
       "9                42  \n",
       "0                43  \n",
       "37               44  \n",
       "24               45  \n",
       "21               46  \n",
       "18               47  \n",
       "46               48  \n",
       "39               49  \n",
       "32               50  \n",
       "52               51  \n",
       "38               52  \n",
       "36               53  \n",
       "42               54  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The best estimator across ALL searched params:\\n\", lgbm_grid_search.best_estimator_)\n",
    "\n",
    "print(\"The best score of all model parameters' combination on model: {:.4f}\".format(lgbm_grid_search.best_score_))\n",
    "display(pd.DataFrame(lgbm_grid_search.cv_results_).sort_values(by=['rank_test_score'])[['mean_fit_time', 'mean_score_time', \n",
    "                                           'params', 'mean_test_score', 'rank_test_score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgbm_grid_search.best_estimator_\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='Feature importance', ylabel='Features'>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9cAAAH/CAYAAABdFy5bAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAADyDklEQVR4nOzdeVRV1fvH8fdlckJxQjC1b4WmWRhhaqY5S4qiBs4FmqjlbCqKiQoO4GyGOZVmzqaSSqFSjlmW5RA2WmriiAyioMz3/v7w5y1SSq4Dip/XWq517zln77P35VnCc/c+extMJpMJEREREREREbGYVUE3QERERERERORBp+RaRERERERE5DYpuRYRERERERG5TUquRURERERERG6TkmsRERERERGR26TkWkREREREROQ2KbkWERG5D1WvXh0vLy/at29v/jdmzBiL64uJiWHcuHF3sIW5bd++nUmTJt21+vNy6tQpBg0adM/vKyIi8k82Bd0AERERubmPPvqIsmXL3pG6/vjjD+Li4u5IXTfTvHlzmjdvftfqz8vZs2c5ceLEPb+viIjIPxlMJpOpoBshIiIiuVWvXp19+/bdNLk+duwYkydPJjk5mZycHHx9fenYsSNGo5HQ0FB++OEHrly5gslkYtKkSTzyyCN069aNlJQUPDw86NChAxMnTuTTTz8F4NtvvzW/Dw8P5/Dhw1y4cIHq1aszY8YM5s+fT3R0NEajkUqVKjF+/HicnJxytSkiIoJt27axcOFCfH19efrpp/nmm29ITEzEz8+PxMRE9u/fT1paGu+88w7Vq1fH19cXFxcXfvzxRy5evEj79u0ZPHgwAF988QVz584lJycHe3t7Ro8eTa1atXK1r1q1ahw5coS4uDjq1KnD4sWLWbBgAV988QUZGRmkpaUxatQoWrZsSXh4OGfOnCE+Pp4zZ85QtmxZZs+ejZOTEydOnGDcuHEkJSVhZWVFv3798PT0JC4ujgkTJnDu3DmysrJo06YNb7755t3/4YuIyANJI9ciIiL3qR49emBl9dcTXEuWLMHBwYHBgwczbdo0nn76aVJSUujSpQtVq1bFZDJx4cIF1q5di5WVFYsWLeL9999nwYIFDB48mG3bthEWFsa33377r/c9c+YMn376KTY2NmzcuJGjR4+ybt06bGxsWLt2LUFBQbz//vv/WcfGjRv54Ycf6Ny5M/PnzycwMJDQ0FBWrFjBxIkTgWsjz6tXryYtLY3OnTvj6urKo48+yvjx41mzZg1VqlRh37599O/fn61bt97QvutfDCxevJgzZ87w9ddfs2LFCooWLcpnn33Gu+++S8uWLQH4/vvv2bhxI/b29rz55pusXbuWwYMHM2zYMDp27Mirr77KuXPn8PX1pVGjRgQEBNCzZ0+aNWtGRkYGffr04dFHH8XT0/N2fqwiIlJIKbkWERG5T91sWvgff/xBbGwsb7/9tvlYeno6P//8M927d8fBwYE1a9Zw6tQpvv32W0qUKJHv+7q5uWFjc+1PhJ07d3LkyBF8fHwAMBqNpKWl/Wcd1xPaKlWqAPDSSy8B8Oijj7J//37zdV26dMHW1hZbW1tatWrF3r17eeKJJ3jhhRfMZevXr0/ZsmX58ccfb2jf31WqVImpU6cSGRnJyZMnzSP419WtWxd7e3sAatasyaVLl0hOTubXX3+lU6dOAFSsWJEvvviCq1ev8t1333Hp0iXmzJkDwNWrV/n111+VXIuIyE0puRYREXmA5OTkUKpUKTZt2mQ+lpCQQMmSJdm1axeTJ0/m9ddfp3nz5jzxxBNs3rz5hjoMBgN/fyosKysr1/nixYubXxuNRnr37k337t0ByMzM5NKlS//ZTjs7u1zvbW1tb3rd35Nkk8mElZUVN3tizWQykZ2dfUP7/u6nn36if//+9OzZkwYNGlCnTh1CQkLM54sWLWp+ff0zuH5/g8FgPnf8+HEcHR0xmUysWbOGYsWKAZCUlESRIkX+td8iIvLw0mrhIiIiD5DHH3+cIkWKmJPrc+fO0bZtW3788Ue++uormjZtSvfu3XF1deWLL74gJycHAGtra3NyWrZsWc6ePUtiYiImk4kvvvgiz/s1bNiQ9evXk5qaCsCcOXMYOXLkHevP5s2bMRqNXLp0iS1bttCsWTNeeOEFvvrqK06dOgXAvn37OHfuHM8+++wN5a2trc1fDnz33Xc888wzvP7669StW5ft27eb+58Xe3t7nn76aTZu3Ahc+zy7detGeno6bm5ufPjhhwBcvnyZbt26sX379jvWdxERKVw0ci0iIvIAsbOzY968eUyePJkPPviA7OxshgwZQu3atSldujQjRozAy8sLa2trnn/+efNCZM899xzvvPMOAwYM4L333qNr1674+Pjg6OhIkyZN8rxfp06diIuLo3PnzhgMBipWrMiUKVPuWH/S09Pp2LEjV65coXv37tSvXx+A8ePHM3DgQHJycihatCgLFiygZMmSN5SvVq0a1tbWdOzYkQULFhAdHY2npye2trbUr1+fS5cumb8YyMvMmTMJCQlh+fLlGAwGJk+ejKOjIzNmzGDixIl4eXmRmZlJ27Ztadeu3R3ru4iIFC5aLVxEREQKhK+vL6+++iqtWrUq6KaIiIjcNk0LFxEREREREblNGrkWERERERERuU0auRYRERERERG5TUquRURERERERG6TkmsRERERERGR26TkWkREREREROQ2aZ/r+9TFi1cwGrXWnNyacuXsSUz8931cRf5JcSOWUNyIJRQ3YgnFjeTXvYgZKysDZcqUuOk5Jdf3KaPRpORa8kXxIpZQ3IglFDdiCcWNWEJxI/lVkDGjaeEiIiIiIiIit0nJtYiIiIiIiMhtUnItIiIiIiIicpuUXIuIiIiIiIjcJiXXIiIiIiIiIrdJq4WLiIiIiIg8hLZti2LVquUYDAaKFi3K0KEjqFGjJosXL2THjs+xsrKievWnCAh4myJFinDx4kUmTRpPXNw5DAYDI0eOwdX1WQB2797JkiULMRisKFmyJIGBY6lUqTIAbdu2oHz5Cub7du/ui4dH6wLp891kMJlMhW59+5CQEA4ePEhWVhaxsbG4uLgA4Ofnh4+Pzy3VsXr1agC6deuWr3sbjUbCwsL48ssvKVKkCK+99hqdOnXKXwdERERERETukvSMbH768RcGDXqDxYtXUr58efbt28v06WEEBYUwY0YYH364Eju7Irz9dgCurrXo3t2PsWMDqVbtSfz8evH7778REDCUNWs+wWCANm1asHTpaipXrsLatSv5/vv9TJ8+h9jYPxk5chhr1kTc9X45OpYkPj7lrt7DyspAuXL2Nz1XKEeux48fD8Dp06fx8/Nj06ZN+a4jv0n1dRs2bODYsWNs3rwZo9HIq6++ylNPPcUzzzyTr3r8J0Vz4WKaRW0QERERERHJS+TM9tja2jFq1FjKly8PQI0aNUlKSiQrK4vMzEwyMjKwsrImMzMTOzs7srOz+frrLxk2bBQA1apVp3LlKnz77dfUqfMCJpOJ1NRUANLS0rCzswPgyJEYrK2tGDToDS5fvkSTJs3x8+uFtbV1wXT+LiqUyfXNnDhxgnHjxpGcnEzx4sUZM2YMtWrVIjAwEIPBwNGjR0lNTaVfv3506NCB8PBwAAYNGkRkZCTz58/HYDDg6urKxIkTsbW1vel9fv75Z5o3b24Opnr16rF9+/Z8J9ciIiIiIiJ3S8WKj1Cx4iMAmEwmwsNn07BhI+rVq0+dOvXw8WmLjY0tjz76P9q39+HSpWRMJhNlypQx11GhghMXLlygePHijBgxmn79elGqlANGo5H58xcDkJOTQ5069ejffwgZGRmMHDmEEiVK0Llz9wLp99300CTXAQEB9O3bFw8PDw4fPsyQIUPYtm0bAHFxcaxZs4bExES8vb1p0KCBuVxcXBxhYWFERETg7OxMQEAAu3fvpkWLFje9T82aNdmyZQve3t5kZWXx1VdfUatWrXvSRxERERERkVvh6FgSgKtXrxIYGEhc3Hk++OADoqO3kZAQx969e7Gzs2P06NEsXvweffv2zVUOoEgRGxwcipOUdJbly5cQFRXFo48+yrJlyxg/PpBNmzbh7++X6759+vRm+fLlDBjwxl3tV0F4KJLrK1euEBsbi4eHBwBubm44ODhw/PhxALy9vbG1tcXZ2Rl3d3cOHDhgLnvo0CHc3d1xdnYGYPr06f96Lx8fH06ePEnnzp1xcnLixRdfJCMj4y71TEREREREJP/i41M4f/48o0a9xWOPPcasWe+RkWHgs8+20KRJS9LSTKSlZeDh0ZbZs6fRu/dAAI4dO0OpUqUAOH36LC++2JitW7dTs6YrxYqVIT4+BQ+PdoSFhfHHH6f55puvqFr1SapWrQbApUtXMRoNd+XZ6IJ+5vqh2IrLZDLxz3XbTCYTOTk5ALnm+xuNRmxs/vrO4e+vAZKSkkhKSsrzXpcuXcLX15fIyEg++OADsrKyePTRR+9EN0RERERERO6Iy5cvMWhQXxo3bkpISBhFihQF4Mkna7B7906ys7MxmUzs2bOTp592xcbGhvr1G7Bp07WFyf7443f+/PMEzz33PNWr1+Dw4YMkJSUC8OWXu6hY8RFKly7N8ePHWLx4ATk5OWRkpLNhw8c0b96ygHp9dz0UI9f29vZUqVKF6Oho87TwhIQEqlW79u3Jli1baNWqFWfPniUmJobJkyfzyy+/AODq6kpISAjx8fE4OjoSGhpKvXr18lwB/NChQ6xdu5YFCxYQFxfH559/zsqVK/Pd5sVBHpZ3WEREREREJA/pGdl88sl64uLOs2fPLvbs2WU+N336O3z00RJee60zdna2VK36pHkRs+HDA5kyZRK+vp0xGAyMHTsBe3t7ateuQ7duvgwa9AY2NraUKlWKsLCZAPTq1ZdZs6bSo0dXsrOzadq0BV5eHQqg13dfodyK67rrq4Xv2LGDY8eOERwcTHJyMra2tgQFBeHu7k5gYCBJSUkkJCSQmZnJsGHDaNasWa4FzbZu3cq8efMwGo24ubkREhKS5+p2JpOJ4OBgvv/+ewCGDBlino6eH4mJqRiNhfZHI3fYvZgCI4WP4kYsobgRSyhuxBKKG8mvgp4WXqiT61sRGBhI3bp18fb2Luim5KLkWvJDv3zEEoobsYTiRiyhuBFLKG4kvwo6uX4opoXfaVFRUSxcuPCm5yzZU1tEREREREQebA/9yPX9SiPXkh/6ZlcsobgRSyhuxBKKG7GE4kbySyPXIiIiIiIi/2LbtihWrVqOwWCgaNGiDB06gho1atKr12tkZmZgY2MLgIdHK7p39yM9PZ0pUyby+++/YTQa6ddvMI0aNeG7777lvffmmOvNyEjn1KlYPvhgOU8+WZ0FC8L5+uuvsLIyULnyowQEvE2ZMmUKqtvygHmgk+uQkBAOHjxIVlYWsbGxuLi4AODn54ePj88t1bF69WoAunXrZlEb4uLi8PHxYe/eveZjmzZtYtGiRQA0atSIUaNGWVS3iIiIiMjDLjb2T+bNm8PixSspX748+/bt5e23A1i5cj1nz57m00+/uGH73CVLFlKsWHFWrlzP+fPneeONntSo8RR16tRj6dJV5uuCgkbSuHEzatR4isjIjfz2268sWbICOzs75s2bw9y5sxk7dsK97rI8oB7o5Hr8+PHAX6uCW/K8s6VJNcDu3bsJDQ0lPj7efCwtLY3JkyezdetWSpUqRbdu3fj666958cUX81V3XlMNRPLi6FiyoJsgDyDFjVhCcSOWUNyIJRwdS3I5pRSjRo2lfPnyANSoUZOkpERiYg5TrFhxAgKGkJiYwPPP1+WNNwZQpEhR9uzZxfjxkwBwdnambt0X2LHjc7p2fc1c97ZtUZw7d47g4FAAHn/8Cfr3H4KdnR0A1avX5JNP1t3jHsuD7IFOrm/mxIkTjBs3juTkZIoXL86YMWOoVasWgYGBGAwGjh49SmpqKv369aNDhw65ttyKjIxk/vz5GAwGXF1dmThxIra2tnnea/369YSHh+Pl5WU+lpOTg9FoJC0tjeLFi5OdnU2RIkXy3Q//SdFcuJiW/w9ARERERKQQiZzZnlIlywLXtr0ND59Nw4aNyMrKxN29NsOGjcLGxpYJE4JYsOA9hgwZzoULcVSo4GSuw9GxAvHxF8zvs7KyWLjwPcaPn2we9X7mmVrm85cvX2bp0vfp0OHWZsOKAFgVdAPutICAAHx9fYmMjGT06NEMGTKEzMxM4NoU7jVr1vDRRx8xbdq0XCPOcXFxhIWFsWTJEj777DNycnLYvXv3v94rPDycJ598Mtcxe3t7hgwZQuvWrWnUqBGVKlXC3d39zndUREREROQhkpaWxtixgZw+fYpRo8bSsGFjxo6dSIkS9hQpUgRf317s2bMTAKPReEN5Kytr8+udO7fzyCOVePZZtxuuO3PmNAMH9qFWLTe8vTvftf5I4VOoRq6vXLlCbGwsHh4eALi5ueHg4MDx48cB8Pb2xtbWFmdnZ9zd3Tlw4IC57KFDh3B3d8fZ2RmA6dOnW9SGX3/9lQ0bNrBz505KlizJiBEjWLx4Mb17977N3omIiIiIPJyyslIYOPBNXFxcWL16JUWLFmXHjh2ULFmSOnXqAHD+fDGKFLHD0bEkjzzyCEZjmvlxhNTUZGrUqGF+v3fvDrp27XzD4wrffPMNb731Fr1798bf3//edlLuiIJ8BKVQJdcmk4l/7ixmMpnIyckBwNr6r2+rjEZjroUP/rkIQlJSEgBly5bNVxv27t1L/fr1KVeuHHAtoV+1apWSaxERERERCyQnJ9O9+6u0bt2WXr36kpKSRUpKFn/8cZKoqM3MnbsIGxtbFix4n8aNmxMfn0L9+i/x0UcrGDFiNBcuxLF79266dPEjPj4Fk8nE/v3fMXDg8FzbNh058gMBAUMIDg7lhRde1DZgD6CC3oqrUE0Lt7e3p0qVKkRHRwNw+PBhEhISqFatGgBbtmzBZDJx5swZYmJiqF27trmsq6srP/zwg3mqeGhoKNu3b893G2rUqMHXX3/N1atXMZlM7NixA1dX1zvQOxERERGRh8/q1auJizvPnj276Nmzu/lfs2YtcHOrTa9er/Hqqx0pVqw4r7/eBwB//zdIS7vKa691ZujQ/vTvP4RKlSoD15L1tLSruZ7JBli8eCEmk4kFC+aa7zF69Ih73l95cBlM/xzqfQBdXy18x44dHDt2jODgYJKTk7G1tSUoKAh3d3cCAwNJSkoiISGBzMxMhg0bRrNmzXItaLZ161bmzZuH0WjEzc2NkJCQXKPdealevTq//fab+f2iRYuIiIjA1tYWV1dXxo8fb9GiZiIiIiIiD7v0jGxSLmuhX/lvBT1yXSiS61sRGBhI3bp18fb2Luim3JLExFSMxofiRyN3wL34j0QKH8WNWEJxI5ZQ3IglFDeSXwWdXBeqZ67vtKioKBYuXHjTc5bsqS0iIiIiIiKF00Mzcv2g0ci15Ie+2RVLKG7EEoobsYTiRiyhuJH8KuiR60K1oJmIiIiIiIhIQVByLSIiIiIi+bZtWxQ9enSjZ8/uvPlmL3799edc5999dyYjRw69odzvvx+lffuXcx379tt9+Pv70rNnd3r1epVvv92X63xmZiZDh/Zn584v7ng/RO6UB/qZ65CQEA4ePEhWVhaxsbG4uLgA4Ofnh4+Pzy3VsXr1agC6deuWr3vn5OQwYcIEDhw4gMlkolOnTvTs2dN8PjU1la5du7JgwQIqV66cr7qBPKcaiOTF0bFkQTdBHkCKG7GE4kYsobgpXH799XfmzZvD4sUrKV++PPv27eXttwOIiPgMgO3bPyc6egs1az5jLpOdnc2GDWtZseIj0tP/Wv07NTWVkJAg5s5dxBNPuPDHH78zcGAfmjR5EYAff4xh5swpnDx5kvbtH4zFieXh9EAn1+PHjwf+2orLkkXG8ptUXxcREUFycjKbN28mPT2djh07UqdOHZ5++ml++OEHgoKC+PPPPy2qG8B/UjQXLmrLARERERG5/8x/qzajRo2lfPnyANSoUZOkpESysrI4c+Y0q1Yto2fP3uzf/425zNGjv3Ls2B9MmjSVESMGm49nZ2czfPgonnji2kDZY489jslk4uLFixQtWpp169bQp09/Vq1adm87KZJPD3RyfTMnTpxg3LhxJCcnU7x4ccaMGUOtWrUIDAzEYDBw9OhRUlNT6devHx06dMi1z3VkZCTz58/HYDDg6urKxIkTsbW1vel9qlWrhpubG1ZWVhQvXpwqVapw7tw5nn76aT7++GPGjx/PyJEj72XXRURERETuicqVK1OkiAMAJpOJ8PDZNGzYiKysLCZOHMeYMeP59ddfcpWpWfMZatZ8hnPnzuY6Xrp0aZo39zC/X7x4IVWq/I8qVaoQH59CSEgogJJrue8VuuQ6ICCAvn374uHhweHDhxkyZAjbtm0DIC4ujjVr1pCYmIi3tzcNGjQwl4uLiyMsLIyIiAicnZ0JCAhg9+7dtGjR4qb3cXNzM78+ePAgMTExTJs2DYDJkyffvQ6KiIiIiNwHHB1LcvXqVQIDA4mLO88HH3zAuHHjeP31HtSr9xxnzpzAzs7mhkcCMjJKYDAYbjienZ3NlClT2LNnD0uXLjXf4zo7OxtKlSqmRwzkXxVkfBSq5PrKlSvExsbi4XHtmy83NzccHBw4fvw4AN7e3tja2uLs7Iy7uzsHDhwwlz106BDu7u44OzsDMH369Fu65/79+xk2bBgzZszAwcHhDvdIREREROT+dOTI74wa9RaPPfYYs2a9x6lTF9i//zt+//0YH3ywhMuXL3HlSio9erzOjBnvmsslJV3BZDLl2jLp8uXLjB07CpPJxLx5i7G1vZYg/f2azMxsLl9O0/ZckqeC3oqrUCXXJpOJf27bbTKZyMnJAcDa2tp83Gg0YmPzV/f//hogKSkJgLJly+Z5v+joaIKDg5k9ezb16tW77faLiIiIiDwIkpOTGTSoL61bt6VXr74AVKhQlE2btpqviYqKZNeu7Uyb9s6/1pWZmcmwYQOpWrUaI0aMvuHvcpEHRaGKXHt7e6pUqUJ0dLR5WnhCQgLVqlUDYMuWLbRq1YqzZ88SExPD5MmT+eWXa8+CuLq6EhISQnx8PI6OjoSGhlKvXj06dep003vFxMQQHBzMkiVLqFGjxh3vy+Igj/++SERERESkAISHv0dc3Hn27NnFnj27zMfnzJmHg0PpfNW1a9d2fv31ZzIzM+jd2898fNasGZQt+8gdarHI3Wcw/XOo9wF0fbXwHTt2cOzYMYKDg0lOTsbW1pagoCDc3d0JDAwkKSmJhISE//92bBjNmjXLtaDZ1q1bmTdvHkajETc3N0JCQnKNdv9dv379OHjwoHkaOcDgwYNp3ry5+X2zZs1YtmyZRVtxJSamYjQ+8D8auUfuxRQYKXwUN2IJxY1YQnEjllDcSH4V9LTwQpFc34rAwEDq1q2Lt/eDsTeekmvJD/3yEUsobsQSihuxhOJGLKG4kfwq6OS6UE0Lv9OioqJYuHDhTc9Zsqe2iIiIiIiIFE4Pzcj1g0Yj15If+mZXLKG4EUsobsQSihuxhOJG8ksj1yIiIiIickds2xbFqlXLMRgMFC1alKFDR+DiUo3Zs6cRE3MYgHr1XqR//8G51hbav/8b5s17l6VLV91Q5549u5g0aTzR0bvNxxYvXsiOHZ9jZWVF9epPERDwNkWKFLnr/RO5n1kVdAPuptOnT/PMM8/Qvn172rdvj5eXF82aNePdd9/913K+vr7m1+3bt7/bzRQRERERuW2xsX8yb94cZs4MZ+nSVfTo0Yu33w5gw4a1JCcns2zZWpYuXc2PP8awY8fnAGRkpLNo0TzGjQskJyf7hjpPnYrlvffewWQymo8dPPg927dHs2TJCpYtW8uVK1fYsGHtPeunyP2q0I9cV6hQIdfz0XFxcbz88su0adMGFxeXm5bZv3+/+XVBPVud11QDkbw4OpYs6CbIA0hxI5ZQ3IglFDd3V3pGNra2dowaNZby5csDUKNGTZKSEvHx6ULHjl2xsrIiOfkiqakplCrlAMC3335Denoao0eP44MPFuSuMz2dCRPGMmjQW4SEBJmPG41GMjMzycjIwMrKmszMTOzs7O5dZ0XuU4U+uf6n+Ph4TCYTJUqUICgoiN9//52EhAQef/xx5s6dy4wZMwDo1KkT69ato3r16vz222+Eh4cTFxfHyZMnOXPmDJ06daJfv35kZWUxfvx4Dhw4gJOTEwaDgf79+/O///2PESNGcPXqVaysrAgKCsLNze2W2+k/KZoLF9Pu0qcgIiIiIoVJ5Mz2VKz4CBUrXtsX2mQyER4+m4YNG2FrawvA/PnhRER8TPXqT/Hss88B0KhRExo1asLBg9/fUOf06ZNp394bF5dquY4//3xd6tSph49PW2xsbHn00f/Rvr3PXe6hyP2vUE8LB7hw4QLt27enVatW1KtXj3feeYe5c+dy6tQpbG1tWbt2LZ9//jkZGRns3r2boKBr38qtW7fuhrp+++03Fi9ezLp161i0aBGXL19mzZo1pKWlsXXrVsLCwjhy5AgA69evp0mTJkRERBAQEMCBAwfuab9FRERE5OGUlpbG2LGBnD59ilGjxpqP9+s3iC1bdlKx4iPMmBH2r3VERKzD2tqGtm1vfETy0083cfbsWTZt2sqmTVupWPER5s6dfcf7IfKgKfQj19enhRuNRqZMmcJvv/3GCy+8gK2tLaVLl2blypUcP36cP//8k6tXr/5rXfXq1cPOzo5y5cpRunRpUlJS+Oqrr+jcuTMGg4FKlSpRv359AOrXr8+gQYP45ZdfaNy4Ma+99tq96K6IiIiIPKQcHUty9uxZBg58ExcXF1avXknRokU5cOAAZcuW5fHHHwegW7fOTJo0KddU/dKli2NjY20+9vnnUaSnp9O792tkZWWRkZFB796vsWjRIr755kt8fDrwv/85A+Dn9yoTJ068K1P/9TiB5FdBxkyhT66vs7KyYuTIkXTo0IElS5ZQtWpV3n33Xfz8/PD29ubixYv8165kf18B0WAwYDKZsLa2xmg03nBt7dq1+eyzz9i1axdRUVF88sknfPjhh3e8XyIiIiIiAMeOncbf35fWrdvSq1dfUlKySEnJYseOPfz00xHCwmZiZWXFunUR1KrlnmvLouTkq2Rn55iPzZ//19+t586dxc+vCx98sAKAxx6ryqefbuHFF5thbW3N5s2fUb16zTu+BZK24pL80lZc95CNjQ0jR45kyJAhtG3bltatW+Pj40NcXBzfffededTZ2tqa7OxsbGz+++N58cUXiYqKonnz5ly4cIH9+/fTo0cPpk2bRoUKFejZsyf16tXjlVdeudvdExEREZGH2CefrCcu7jx79uxiz55d5uOzZoWTkJBAz57dsbIyUKuWG2++OdDi+/j6vk54+Gxee60zdna2VK36JMOGjboDPRB5sBlM/zVc+wA7ffo0fn5+7NixI9fx119/HaPRSFJSEtbW1tjZ2eHk5MQTTzzBW2+9xaBBgzh+/DgRERHUqlXLvKAZwKBBgwBo1qwZy5Ytw8nJiQkTJnDo0CEcHR1JSkr6/2kxjgwfPpwrV65gbW1N79698fT0vOefgYiIiIgUfukZ2aRcLlyL4WrkWvKroEeuC3VyfS/s2rULk8lE06ZNSUlJoUOHDmzYsIHSpUvfVr2JiakYjfrRyK3RLx+xhOJGLKG4EUsobsQSihvJr4JOrh+qaeF3g4uLCyNHjuSdd94BYPDgwbedWIuIiIiIiMiDRcn1bapSpQqrV68u6GaIiIiIiIhIASr0+1yLiIiIiIiI3G0auRYRERGRArVtWxSrVi3HYDBQtGhRhg4dQY0aNVm2bAlbt35GTk4OHh6t6dWrLwaDgdOnTzFjRhjJyclkZ2fRpk17unV7LVedP//8IwMG9OGTT7ZQunRpTCYT778/nx07Pqdo0WI880wtBg16K9dWqyIit+OBTq5DQkI4ePAgWVlZxMbG4uLiAoCfnx8+Pj63VMf1Kd3dunWzqA1xcXH4+Piwd+9eANatW8eKFSvM50+fPk379u0ZN25cvurN6yF5kbw4OpYs6CbIA0hxI5ZQ3Igl8oqbX3/9nXnz5rB48UrKly/Pvn17efvtAAICRrNz5xcsXrwCKysrhg8fxI4dX9C8eUsmTw7G09MLL68OpKam0ru3H08+WZ3atesAkJyczIwZU8jKyjLfJyoqkq+/3sv77y+jZMmSLF36Ae+/P5+BA4fei+6LyEPggU6ux48fD/y15damTZvyXYelSTXA7t27CQ0NJT4+3nysU6dOdOrUCYDff/+dAQMGMHBg/vcR9J8UzYWLhWs7BREREZF/mv9WbUaNGkv58uUBqFGjJklJiezcuZ2WLVtRrFgxADw9vYiOjqJ585a0bdue5s09ALC3t6dy5cqcP38OAKPRyIQJY3njjQEMHz7IfJ/ffvuFl15qTMmS15L8Ro2aMnLkUCXXInLHFLpnrk+cOIGvry9eXl506dKFmJgYAAIDAxk9ejQ+Pj68/PLLbNy4EYDw8HDzHtaRkZF4enrSpk0bAgMDc33beTPr1683l72Z4OBg3nrrLcqWLXtnOiciIiJSyFSuXJkXX2wIgMlkIjx8Ng0bNiIhIYEKFZzM1zk6ViA+/gIAbdq0o2jRogB8883X/PhjDPXqvQjABx8soGbNp6lXr36u+9Ss+QxffbWH5ORkjEYjW7d+RmJiwr3ooog8JB7okeubCQgIoG/fvnh4eHD48GGGDBnCtm3bgGtTuNesWUNiYiLe3t40aNDAXC4uLo6wsDAiIiJwdnYmICCA3bt306JFizzv9W+J9ddff016ejqtW7e+c50TERERKYQcHUty9epVAgMDiYs7zwcffMDQoUMpVaqYeTp56dLFsbOzzTW9/JNPPmHKlCmEh4fz1FOPs2vXLv7441cWL16MldW1MaRy5UpQtmxJfH27cvXqJYYN60/x4sXp3LkzGzfa6jGH+5x+PpJfBRkzhSq5vnLlCrGxsXh4XJsm5ObmhoODA8ePHwfA29sbW1tbnJ2dcXd358CBA+ayhw4dwt3dHWdnZwCmT59+W21Zs2YNr7/++m3VISIiIvIwOHLkd0aNeovHHnuMWbPeIyPDQJky5Tlx4hTx8SkA/PHHScqUKU98fAomk4m5c99h167tzJ79HtWqVSc+PoVVq9Zw5sxZvLzam+t+9dXXePvt8TzySCVefLEp3t7dAfjppx+pVKmyuX65/zg6ltTPR/LlXsSMlZUhz/WxClVybTKZMJlMNxzLyckBwNra2nzcaDRiY/NX9//+GiApKQnAoindmZmZfPfdd0yZMiXfZUVEREQeJsnJyQwa1JfWrdvSq1df8/GGDRvz4Yfv066dN9bW1kRFReLp6QXAnDkz+PHHI3zwwXLKlCljLjN5cu7BkYYNn+fddxdSunRp9u//hgULwlm06CMAVqz4kJYtNcNQRO6cQpVc29vbU6VKFaKjo83TwhMSEqhWrRoAW7ZsoVWrVpw9e5aYmBgmT57ML7/8AoCrqyshISHEx8fj6OhIaGgo9erVMy9Olh+//fYbjz32GMWLF7e4L4uDPCwuKyIiIvKgCA9/j7i48+zZs4s9e3aZj8+ZM4/GjZvSp08PsrOzaNiwMa1atSEu7jwbNnyMs3NF3nprgPn6Tp260qZNuzzvU7fuCxw6dIAePbpiNBp56aUmdOnS/W52TUQeMoUquYZr07mDg4MJDw/H1taW8PBw7OzsAEhPT8fHx4fMzEwmTJiQ65tOJycnxowZg7+/P0ajETc3N7y9vS1qw6lTp8zTyy2VmJiK0Wj67wtF0LQpsYziRiyhuBFL/FvcdO3qR9eufjc95+fXCz+/XrmOOTk58+WX393Sfffu/T7X+zfeGMAbbwzI42oRkdtjMP1zHnUhFRgYSN26dS1OmO81JdeSH/pjVyyhuBFLKG7EEoobsYTiRvJLz1zfx6Kioli4cOFNz1myp7aIiIiIiIgUTg/NyPWDRiPXkh/6ZlcsobgRSyhuxBKKG7GE4kbySyPXIiIiIlIomEwmQkNDePxxF7p39+Xy5UvMmDGF48d/x86uCJ6eXnTs2JUTJ44TEhJkLmc05nD8+DEmT55G48bNWL16BZ99thlra2tKly7DyJFvU6lSZXJycggPn83+/fvIycmhW7fX6NChYwH2WETkLw90ch0SEsLBgwfJysoiNjYWFxcXAPz8/PDx8bmlOlavXg1At27d8nXvnJwcJkyYwIEDBzCZTHTq1ImePXsCEBkZyfz588nKyqJnz568+uqr+apbRERE5EHz558nmDVrKj/9dAR//2t/k7377iyKFStGVFQUcXGXGD16OBUrVqJBg5dYunSVuWx4+GyeeKIqjRs347vvvuWzzzaxcOGHlChhT0TEOkJDQ3jvvffZtCmC06djWbZsLVevXuXNN1/nySdrULPmMwXVbRERswc6uR4/fjwAp0+fxs/Pz6LnoPObVF8XERFBcnIymzdvJj09nY4dO1KnTh3Kly/P7NmziYiIwM7Ojq5du1KvXj2qVq2ar/rzmmogkhdHx5IF3QR5ACluxBKKG/mn9IxsIiI+xtPTCyenv3ZM+e23X3jrrZFYW1tja2tL/foN2bVrOw0avGS+5ocfDrFr13aWLVsDQLly5Rg+PJASJa79LVSjxlOsXHltb+o9e3bSrp03NjY2lCpViubNPYiO3qLkWkTuCw90cn0zJ06cYNy4cSQnJ1O8eHHGjBlDrVq1CAwMxGAwcPToUVJTU+nXrx8dOnQgPDwcgEGDBplHnA0GA66urkycOBFbW9ub3qdatWq4ublhZWVF8eLFqVKlCufOnePo0aO88MILlC5dGoCXX36ZrVu3MnDgwHz1w39SNBcupt3WZyEiIiJyL0TObM+wYaMAOHDgr22yatZ8hm3bomjWrCFXr15l9+4d2Njk/vNz7tx36Nu3vzmZfuKJvwYkMjMzWbBgLk2btgDgwoU4KlRwMp+vUMGJY8f+uGv9EhHJD6uCbsCdFhAQgK+vL5GRkYwePZohQ4aQmZkJQFxcHGvWrOGjjz5i2rRpxMfHm8vFxcURFhbGkiVL+Oyzz8jJyWH37t153sfNzY1q1aoBcPDgQWJiYqhTpw4XLlzA0dHRfF2FChWIi4u7S70VERERuX8NHPgWBoOBV155hbffHkGdOvWwsflr4OLIkR+4dCmZli1b3VD24sWLDBs2kGLFipn3pr7ZYq9WVoXuz1kReUAVqpHrK1euEBsbi4eHB3AtAXZwcOD48eMAeHt7Y2tri7OzM+7u7hw4cMBc9tChQ7i7u+PsfG0q0/Tp02/pnvv372fYsGHMmDEDBwcHbrb4usFguN2uiYiIiNzXrj8uULSoLfb2RXB0LElWVgpjx75tntG3aNEiqlV7wnzt11/vwsfHGycnh1x1/frrr/Tv358WLVowatQorK2tAahSpRLZ2VfM5a9evcT//ldZjyoUYvrZSn4VZMwUquTaZDLdkNyaTCZycnIAzP8xAxiNxlzTkv45RSkpKQmAsmXL5nm/6OhogoODmT17NvXq1QPAycmJ77//3nzNhQsXqFChgoU9EhEREXkwXN/+Jj09i9TUDOLjU1iyZBlXrqQSFjaJ3377kzVr1hIcPNl87b593/LWWyNzbZ1z+vQp+vbtSf/+g2nbtj1JSVfN5+rVa8CqVWt55pnnSUtLY/PmSEaMGK3tmgopbcUl+VXQW3EVqnk09vb2VKlShejoaAAOHz5MQkKCefr2li1bMJlMnDlzhpiYGGrXrm0u6+rqyg8//GCeKh4aGsr27dvzvFdMTAzBwcEsWbLEnFgDvPjii+zbt4+kpCTS0tKIjo6mUaNGd6O7IiIiIvc1X9+exMdfoG3btgwe3I9evfry1FNPm8+fPh1LxYoVc5VZufIj0tPTWb9+LT17dqdnz+706dMDgA4dOlKpUuX/P+ZHmzbtee652oiI3A8MppvNY37AXF8tfMeOHRw7dozg4GCSk5OxtbUlKCgId3d3AgMDSUpKIiEhgczMTIYNG0azZs1yLWi2detW5s2bh9FoxM3NjZCQkFyj3X/Xr18/Dh48aJ5GDjB48GCaN29OZGQkCxcuJCsri44dO9KnT5978jmIiIiIFIT0jGxSLue9EKtGIMUSihvJr4IeuS4UyfWtCAwMpG7dunh7exd0U25JYmLqTRftELkZ/fIRSyhuxBKKG7GE4kYsobiR/Cro5LpQPXN9p0VFRbFw4cKbnrNkT20REREREREpnB6akesHjUauJT/0za5YQnEjllDciCUUN2IJxY3kV0GPXBeqBc1ERERERERECoKmhYuIiIhIvphMJkJDQ3j8cRe6d/fl8uVLzJgxhd9//41ixYrh6elFx45dc5U5e/YM/v6+zJ49lxo1agKwfv0aNmz4mCJFivK//z3G8OGjKFXq2p7Xbdu2oHz5v7Yz7d7dFw+P1veukyIi+aTk+iYmTJhAQkIC7777rvnY3r17GT9+PJs2bcLe/ubTAO6kvKYaiOTF0bFkQTdBHkCKG7GE4ubhlZ6RzZGYn5k1ayo//XQEf38XAN59dxbFihVjxYp1GI1GRo8eTsWKlWjQ4CUAMjIymDhxLNnZWea6Dh78npUrl7Fw4YdUqODE1q2fMW3aZCZNmkZs7J/Y25di6dJVBdJPERFLKLm+ieHDh+Pl5cWOHTto1qwZV69eJTg4mNDQ0HuSWAP4T4rmwsW8t7QQERERudciZ7YnIuJjPD29cHL6azvS3377hbfeGom1tTXW1tbUr9+QXbu2m5PrkJAQWrf2YtmyJeYyv/76C88/X5cKFZwAaNy4GVOnTiIrK4sjR2KwtrZi0KA3uHz5Ek2aNMfPr1eeW6SKiNwP9Mz1TZQoUYJJkyYxYcIErl69yrvvvkuzZs0oVqwY3bp145VXXqFXr16cOnUKgP3795uPN2vWjC1btgDXtv968803ad26NTt27CjILomIiIjcEcOGjaJVqza5jtWs+QzbtkWRnZ3N1atX2b17B4mJCQBERm4kOzubdu1e+UeZpzlw4DvOnz8HQFTUZrKysrh06RI5OTnUqVOPmTPDmTv3ffbv38eGDWvvTQdFRCykkes8vPjiizRs2JDRo0dz/PhxVq1axauvvsqCBQt45JFH+PLLLxk7dixLly5lxYoVTJo0CRcXF/bt20doaCitW197Jqh06dIsWLCggHsjIiIicmdcfyygaFFb7O2L4OhYkuDgsUydOpU+fXxxdHSkSZNGHDp0iAsXYvn0009YuXIlxYoVw9raitKli+PoWJKWLRuTnDyIceNGYTAY8PHxoXTp0jg7l8bf3y/XPfv06c3y5csZMOCNguiyFCA9hiL5VZAxo+T6XwQGBtKkSRPee+89zp07x6lTp+jXr5/5fGpqKgDTp09n586dbN26lR9++IErV66Yr6lVq9Y9b7eIiIjI3XJ9m5v09CxSUzOIj0/h/Pk4evXqZ16MbMWKpTg6OrN69TouXUqha9euZGcbiYuL4623hjFgwBDc3Z+natWnWbRoGQBJSYkYjXPIyrJm+fI1VK36JFWrVgPg0qWrGI0Gbcv0kNFWXJJfBb0Vl5Lrf2Fvb0+pUqWoVKkSqampVK5cmU2bNgGQk5NDQsK16U7du3enXr161KtXj/r16zNixAhzHUWLFi2QtouIiIjcK5s2beDKlVSGDRtFUlIikZEbCQ6ezFNPPc2QIcPNf/B27OjF+PGTqFGjJrGxfzJkSH9WrPiYEiXsWbr0A1q08MBgMHD8+DF2797BpEnTyM7OYsOGj7VSuIjc95Rc36InnniCS5cu8f333/P888+zYcMGIiMjCQ8P588//2TVqlUUKVKE8PBwcnJybvt+i4M87kCrRURERO6c9Izsmx739e3JxInj8PXtjMkEvXr15amnnv7Xuh599DFee60Hffv2xGg0UquWG8OGjQSulZ81ayo9enQlOzubpk1b4OXV4U53R0TkjlJyfYvs7OyYM2cOkydPJiMjA3t7e6ZOnUrp0qXp1KkTbdq0wd7eHjc3N9LT07l69ept3S8xMRWj0XSHWi+FnaZNiSUUN2IJxY1cN2ZMsPl18eIlCAub+Z9l1q+PzPXex6cLPj5dbriuaNGivP32+Ntuo4jIvWQwmUzK4O5DSq4lP/THrlhCcSOWUNyIJRQ3YgnFjeRXQT9zra24RERERERERG6TkmsRERERERGR26RnrkVERETuEJPJRGhoCI8/7kL37r4EBY3k9OnT5vPnzp3Bzc2dqVNnm4+dPXsGf39fZs+eS40aNQEID5/Nzp1fmLe2evTR/zFhQliue7377kxOnz7FtGnv3P2OiYjIfyo0yfXp06dp1aoVLi4uABiNRq5cuUKHDh0YPHhwvuqaM2cOzzzzDM2bN78bTb0lec3jF8mLo2PJgm6CPIAUN2IJxc2N0jOyORLzM7NmTeWnn47g73/t75FJk6aZr/nll58IChrFsGGjzMcyMjKYOHEs2dlZuer78ccYQkJCcXV99qb32779c6Kjt1Cz5jN3oTciImKJQpNcA1SoUMG8DzVAXFwcL7/8Mm3atDEn3bdiyJAhd6N5+eI/KZoLF9MKuhkiIiJyCyJntici4mM8Pb1wcnK+4XxWVhaTJwczePDwXOdnzZpK69ZeLFu2xHwsMzOT33//jdWrVzBjxhQqV67MoEHDcXa+Vu7PP0+watUyevbszf7939z9zomIyC0p1M9cx8fHYzKZKFGiBIsWLeKVV16hXbt2TJs2DZPJRFhYGIsXLzZfP3jwYKKjowkMDCQiIgKAjRs38sorr9C+fXvefvvt//+GeSKrVq0C4OOPP6Z169bAtV+cjRs3Jisri4CAADp06ECHDh34+OOP733nRURE5J4aNmwUrVq1uem5Tz/dRLlyjjRu3NR8LDJyI9nZ2bRr90quaxMS4nF3f5433xzA0qWrePppV0aPHobJZOLq1atMnDiOMWPGU7x4ibvaHxERyZ9ClVxfuHCB9u3b06pVK+rVq8c777zD3LlzOXr0KD/++CPr169n48aNxMXFsXnzZtq3b89nn30GQGpqKgcPHqRJkybm+n7//Xc+/vhj1qxZw6ZNmyhXrhyLFy+mcePGfPPNtW+K9+3bx6VLl0hISODAgQO4ublx6NAhLl26xMaNG/nwww85ePBgQXwcIiIicp9Yu3YVPXr0Mr//7bdf2bhxAwEBb99w7SOPVGLGjHd59NHHMBgMdOvmy5kzZzh37ixTpkykY8cuPPFE1XvZfBERuQWFclq40WhkypQp/Pbbb7zwwgvMmjWLmJgYvL29AUhPT+eRRx6hffv2ZGZmcvLkSQ4dOkTTpk2xs7Mz1/ftt99y8uRJOnfuDFwbma5Zsyb+/v6MGzeOnJwcjh8/jqenJ9999x1HjhyhadOmVKtWjRMnTuDv70+jRo0YMWJEgXweIiIicu9cfxa9aFFb7O2LmN///PPPgBEPjyYYDAYAFi2KJiMjjYEDewOQmJjApEnjGDlyJJUqVeLXX3+lQ4cOwLVF0sCEk1Npjhw5zNmzp9iwYQ2XLl0iJSWFt98exvvvv3+vu5tvelZfLKG4kfwqyJgpVMn1dVZWVowcOZIOHTqwZMkScnJy6NGjB6+//joAly9fxtraGoB27doRFRXFoUOH6NOnT656cnJyaN26NUFBQQBcuXKFnJwcihQpQo0aNYiMjOSJJ56gXr167Nu3jwMHDtC7d2/KlCnDZ599xldffcXu3bt55ZVX+OyzzyhVqtS9/SBERETknomPTwEgPT2L1NQM8/udO7/Eza02CQmp5mv79h1M375/LbjasaMXQUETqFGjJseP/8HEiZN4/PEaPPJIJSIi1uHiUhVr6xJ88skWc5moqEh27dpOaOgs873uV46OJe/7Nsr9R3Ej+XUvYsbKypDn4tOFalr439nY2DBy5EgWLFhAzZo12bRpE1euXCE7O5sBAwawbds2ALy8vIiKiuLkyZM8//zzueqoV68en3/+OYmJiZhMJoKDg/noo48AaNy4Me+99x5169albt26bN++nWLFilG2bFm2b9/OiBEjaNKkCUFBQRQvXpxz587d889ARERECt6pU6dwdq54y9c/8URV3norgFGj3uLVVzuyZ89Oxo8PvYstFBGRO6FQjlxf16hRI9zc3Pjuu+/w8PCgc+fO5OTk8NJLL/HKK9cWD6lYsSJlypTBzc3NPFXruho1ajBw4EB69OiB0Wjkqaeeom/fvgA0adKE4OBg6tati4ODA+XKlTM/r92oUSO2bdtGmzZtKFKkCB4eHlSvXj1fbV8c5HH7H4CIiIjcE+kZ2ebXY8YE5zo3fPgo/sv69ZG53r/8sicvv+z5r2U8Pb3w9PS69UaKiMhdZTBde5BH7jOJiakYjfrRyK3RtCmxhOJGLKG4EUsobsQSihvJL00LFxEREREREXnAKbkWERERERERuU1KrkVERERERERuU6Fe0ExEREQKjslkIjQ0hMcfd6F7d18A2rZtQfnyFczXdO/ui4dHa06fPsWMGWEkJyeTnZ1Fmzbt6dbtNQB++OEw7747k5ycHOzs7Bg2bCQ1atQkMzOTd96ZzsGD31OsWDEaNGhEr159sbLS2IGIiNx7hTK5DgkJ4eDBg2RlZREbG4uLiwsAfn5++Pj43FIdq1evBqBbt275vv8HH3xAREQEAJ06dTLvr50feT0kL5IXR8eSBd0EeQApbsQS/xY36RnZpFxO488/TzBr1lR++ukI/v7Xfg/Hxv6JvX0pli5ddUO5yZOD8fT0wsurA6mpqfTu7ceTT1andu06TJw4ltGjx1G7dh12797JpEnBrFjxMcuXf8j58+f56KM12NraMn16KJ98sg4fny53re8iIiJ5KZTJ9fjx4wE4ffo0fn5+bNq0Kd91WJJUA5w8eZJVq1YRFRWF0WikTZs2NGvWjP/973/5qsd/UjQXLqZZ1AYREZGCEjmzPSlARMTHeHp64eTkbD535EgM1tZWDBr0BpcvX6JJk+b4+fXC2tqatm3b07z5tW0o7e3tqVy5MufPnwPAaDSSknIZgKtXr2BnZwfAb7/9QosWHhQpUgSAl15qwqpVy5Rci4hIgSiUyfXNnDhxgnHjxpGcnEzx4sUZM2YMtWrVIjAwEIPBwNGjR0lNTaVfv3506NCB8PBwAAYNGkRkZCTz58/HYDDg6urKxIkTsbW1vel9jEYjWVlZZGRkYDKZMJlM2Ng8NB+ziIgIAMOGXdvb+cCB78zHcnJyqFOnHv37DyEjI4ORI4dQokQJOnfuTps27czXffPN1/z4YwyBgeMAGD16LKNHj2DOnJmkpqYwe/Z7ANSs+Qzbt39OkybNsbW15fPPt5KYmHAPeykiIvKXhybrCwgIoG/fvnh4eHD48GGGDBnCtm3bAIiLi2PNmjUkJibi7e1NgwYNzOXi4uIICwsjIiICZ2dnAgIC2L17Ny1atLjpfR5//HHatm1L06ZNMZlMdOrUiUqVKt2TPoqIiNwP/j5tvGhRW+zti+DoWBJ/f79c1/Xp05vly5czYMAb5mOffPIJU6ZMITw8nKeeepyEhARmzAhjxYoVuLq68sUXXzBuXCDbtm1jyJABzJ49m4EDe1OqVCk8PT05efK4Hne4T+nnIpZQ3Eh+FWTMPBTJ9ZUrV4iNjcXD49p0Mzc3NxwcHDh+/DgA3t7e2Nra4uzsjLu7OwcOHDCXPXToEO7u7jg7X5vWNn369H+91549e/jxxx/58ssvMZlM9OnTh6ioKDw9Pe9S70RERO4v8fEp5tfp6VmkpmYQH5/C1q2fUbXqk1StWg2AS5euYjQaiI9PwWQyMXfuO+zatZ3Zs9+jWrXqxMensHPnlzg6OuHs/Bjx8Sk8+2w9rKysOXDgCI6OFWjXrjO9evUHYPv2aJycHsl1f7k/ODqW1M9F8k1xI/l1L2LGysqQ5/pYD8VymtenZ//zWE5ODgDW1tbm40ajMdc07n9O6U5KSiIpKSnPe+3cuZOXX36ZEiVKYG9vT9u2bfnuu+/yvF5ERORhcfz4MRYvXkBOTg4ZGels2PAxzZu3BGDOnBn88MMhPvhgOdWqVTeXcXGpxvHjx4iNPQnATz/9SHp6OlWqPMrevXuYPn0yJpOJq1evsmbNSjw8WhVI30RERB6KkWt7e3uqVKlCdHS0eVp4QkIC1apd++Z8y5YttGrVirNnzxITE8PkyZP55ZdfAHB1dSUkJIT4+HgcHR0JDQ2lXr16dOrU6ab3qlGjBtHR0XTr1g2j0ciePXt4+eWX893mxUEelndYRESkgKRnZOd5rlevvsyaNZUePbqSnZ1N06Yt8PLqQFzceTZs+Bhn54q89dYA8/WdOnWlTZt2jBgxmqCgkRgMBooUKcrkydMpUcKeNm3a8fPPP+Lr2wWjMQcvr1do2vTmj22JiIjcbQbTP4d0C5Hrq4Xv2LGDY8eOERwcTHJyMra2tgQFBeHu7k5gYCBJSUkkJCSQmZnJsGHDaNasWa4FzbZu3cq8efMwGo24ubkREhKSa7T774xGI1OnTmX37t3Y2NjQuHFjRowYgcFgyFfbExNTMRoL7Y9G7jBNmxJLKG7EEoobsYTiRiyhuJH8Kuhp4YU6ub4VgYGB1K1bF29v74JuSi5KriU/9MtHLKG4EUsobsQSihuxhOJG8qugk+uHYlr4nRYVFcXChQtves6SPbVFRERERETkwfbQj1zfrzRyLfmhb3bFEoobsYTiRiyhuBFLKG4kvzRyLSIiIsC1nSxCQ0N4/HEXunf3JTU1lSlTJnDy5J+YTCZatWrDa6/1zFXm7Nkz+Pv7Mnv2XGrUqAnAsWN/MHv2NK5cScXKypqAgLepUeMpAHbt2s6yZR+SlZWJs3NFgoJCcHAofY97KiIiUvgoub5P5fVtiEheHB1LFnQT5AGkuLk/pGdkcyTmZ2bNmspPPx3B398FgA8+mI+joxOTJk0jLS0NX9/OuLm588wztQDIyMhg4sSxZGdn/VVXejrDhg0gMHAs9es35MsvdzFhQhCrVm3g119/ZvbsaSxY8CEVKz7Cu+/OZNGieQQEvF0AvRYRESlc7rvkOjU1lZkzZ/Ldd99hbW1NqVKlCAwM5Omnn74j9YeHhzN37lzWrFnDc889Zz4+efJkli1bxm+//WZRvb6+vixfvhyA6tWrW1zPdf6TorlwMe226hARkQdD5Mz2RER8jKenF05OzubjQ4aMICcnB4DExASysjIpUeKvL19nzZpK69ZeLFu2xHxs//5veOSRytSv3xCAhg0bU7FiJQC2bdtCmzbtqVjxEQB69XqDS5eS73b3REREHgpWBd2AvzMajfTp0wcHBwc2btzIpk2bGDBgAH369OHixYt37D7Ozs5s27Yt132/++6726pz//79t9ssERF5iA0bNopWrdrkOmYwGLCxsWHChLH4+XXBza02jz76PwAiIzeSnZ1Nu3av5Cpz6tRJypUrR1jYBPz9fRk6dIA5QT916iQ5OTkEBg6jR49uzJo1leLFS9ybDoqIiBRy91Vy/e2333LhwgUGDx6Mjc21QfUXXniBsLAwjEYjCxYswNPTEy8vL6ZMmUJOTg6pqan07dsXb29vvL292b59+3/ep3nz5uzYscP8/sCBA7i5uZnfG41GJk2aRJs2bWjbti2LFi0yt69Xr17079+fl19+mcGDB5OZmcmkSZMA6NSpk7mOcePG0a5dO9q1a8fJkyfvxMcjIiIPqXHjJvLpp1+QknKZpUs/4LfffmXjxg03nc6dnZ3Nvn1f0a6dN4sXL6djx84EBAwhMzOT7OxsvvrqSwIC3ubDD1dStmw5pk2bVAA9EhERKXzuq2nhP//8M66urlhZ5c75GzduzO7du9mxYwcRERHY2NgwaNAg1qxZQ/HixalUqRKLFi3i2LFjrF+/nubNm//rfcqUKUPlypWJiYmhVq1aREVF4enpyerVqwFYvXo1586dY/PmzWRmZuLr68uTTz5JsWLFOHToEFu2bKFChQp07tyZvXv3EhQUxPLly1m3bp35Hi+++CITJkxg6tSprFmzhlGjRt35D0xERAqN68+/Fy1qi719ERwdS/Lll1/y5JNP4uTkBJTklVfaEx0dTU5OBhkZaQwc2Bu4NmV80qRxjBw5kscfr4KLiwtNmtQHwNvbi2nTJpOenkzlyo/wzDM1qVHjcQBee60rPXr0yPez93pWXyyhuBFLKG4kvwoyZu6r5NrKyoq8dgb75ptvaNOmDUWLFgXAx8eHjRs3MmLECGbNmkVcXBxNmjRhwIABt3Sv1q1bs23bNp5++mkOHTrE2LFjzee+/fZbXnnlFaytrSlWrBheXl7s27ePZs2aUa1aNZydrz0P5+LiwqVLl25af4sWLQCoWrUq33///S1/BiIi8nC6vnVIenoWqakZxMen8Mknm7G2vrbad1ZWFps2RVKnTj26dHmVvn0Hm8t27OhFUNAEatSoSWJiAqdOTeHLL/dTo8ZTHD58EJMJihRx4IUXGvHee+/QqdNrODiUZuPGT6le/al8bVuirXHEEoobsYTiRvJLW3H9zTPPPMOqVaswmUwYDAbz8VmzZrFv3z5eeSX3c2XZ2dk89thjbNmyhS+//JKdO3eyZMkStmzZkqv8zbRo0YJu3brRsGFDnn/++Vyj5UajMde1JpPJ/LxakSJFzMcNBkOeXwZcn9b+b9eIiIj8m4ED32LGjFD8/LpgMBh46aUmdOrU7V/LlCtXnrCwGcycOYX09DRsbe2YPHk6RYoUoWHDRsTHX2DgwL6YTCacnCoyevTYf61PREREbs19lVw///zzlCtXjrlz59K/f3+sra358ssviYiIYPjw4axevZouXbpgY2PDhg0beOGFF1ixYgWnTp1i9OjRNGrUiKZNm5KSkkKpUqX+9V5lypShUqVKzJkzh5EjR+Y698ILL7Bx40aaNm1KZmYmkZGRvPnmm/9an7W1NdnZ2eak+nYtDvK4I/WIiMj9Lz0j2/x6zJhg8+uSJUsSEhL2n+XXr4/M9d7NzZ333//opte+8kpHXnmlo2UNFRERkTzdV8m1wWBg3rx5hIWF0bZtW2xsbChTpgyLFi2iZs2anDt3Dh8fH7Kzs3nppZd47bXX/n8/z2F4eXlhY2PDwIED/zOxvq5Vq1a89957ubbkAujSpQt//vkn7du3Jysri3bt2tGyZUu+/fbbPOtq3rw57du3JyIi4rY+g+sSE1MxGjXiLbdG06bEEoobERERkTvHYNKc5fuSkmvJDyVJYgnFjVhCcSOWUNyIJRQ3kl965voumDp1Kl9//fUNx5955hkmT55cAC0SERERERGRwqxQJtfa9kpERERERETupUKZXIuIiNxvTCYToaEhPP64C927+5KamsqUKRM4efJPTCYTrVq14bXXeuYq8+mnm9izZxfTps021/H++/PZs2cnADVq1GTEiNEULVqUN9/sRXp6urlsbOxJ2rXrwNChAfesjyIiIg8zJdf3qbzm8YvkxdGxZEE3QR5Aipu7Kz0jm5TLafz55wlmzZrKTz8dwd/fBYAPPpiPo6MTkyZNIy0tDV/fzri5ufPMM7W4fPkSCxe+x7ZtUbi7P2+ub8+enXz33Td8+OEqbGxsGDs2kHXrVuPr+zoLFiwxX7d3724WLJhL79797nmfRUREHlb3fXK9detWFi1aRHZ2NiaTifbt29O7d2/69OnDpEmTcHJyyned1atXp2HDhixevNh8LCkpiZdeeok333yTQYMG5bvOHTt2cPLkSV5//XXCw8MBLKrnOv9J0Vy4mGZxeRERKXiRM9uTAkREfIynpxdOTs7mc0OGjCAnJweAxMQEsrIyKVHi2herO3Z8Trly5RkwYCj79u01l2ncuBkNGjTCxsaGK1dSSU6+SKlSDrnuefnyJaZPD2Pq1FnY2+uLWhERkXvlvk6u4+LimDp1KhEREZQpU4YrV67g6+vL448/zvvvv39bdf/5559cunQJB4drf5RER0ff8hZeN/PTTz/dVntERKTwGjbs2logBw58Zz5mMBiwsbFhwoSx7Nq1nZdeasKjj/4PgA4dru1DHRUVeUNdNjY2bNiwlvffn0/58hVo1KhprvMrVnxE/foNqFGj5t3qjoiIiNzEfZ1cX7x4kaysLPMzZCVKlGDKlCkUKVKEZs2asWzZMvbv38+XX37JpUuXOHXqFA0aNCA4OPg/627WrBlffPEFPj4+AGzbto2WLVuazx8+fJjJkyeTkZFBmTJlmDBhAv/73//w9fXF1dWVAwcOkJSURFBQEJUqVWLNmjUAPPLIIwDExMTQtWtX4uLi8Pb2vq1RbBEReXD9fep90aK22NsXyXUsPPwdrly5wuDBg/n442UMHjzYfK5kyaLY2dncMH3/zTd788Yb/rzzzjtMmPA2K1asACAjI4NPP91IRETEXZ3yr8cJxBKKG7GE4kbyqyBj5r5OrmvUqEHz5s1p0aIFTz31FPXq1cPLy4v//e9/ua47dOgQn376KdbW1rRq1Ypu3bpRvXr1f627devWLFiwAB8fH+Lj4zGZTDg6OgKQmZnJsGHDeOedd6hVqxZbtmxh2LBhbNiwAYCsrCzWrl3Ljh07mDNnDhEREXTt2hUAHx8fwsPDSUxMZM2aNaSmptKsWTNef/11Tc8TEXkI/X2/zfT0LFJTM4iPT+Hbb/fh4lKV8uWv/e5p1Kg5u3btyHV9Sko6mZnZ5mO//34Uk8nIk0/WAKB5c08++miZ+fzu3TtxcalG0aKl79o+n9p3ViyhuBFLKG4kvwp6n2uru3rnOyAkJIQdO3bQrVs3zp49S+fOnYmOjs51zXPPPYe9vT3FihWjSpUqXLp06T/rfe655zhx4gQpKSls27aNl19+2Xzuzz//pFSpUtSqVQu4lojHxsaSknLtB/XSSy8BUK1aNZKTk29a/0svvYSdnR1ly5alTJkyt9QmERF5eOzY8TlLlizCZDKRmZnJjh2fU7v28/9a5tix3wkNnWCe0bV162e5yhw+fJDatevc1XaLiIjIzd3XI9e7du3i6tWreHp64uPjg4+PDx9//DHr16/PdV2RIkXMrw0GAyaT6T/rNhgMNG3alO3btxMdHc0777zDypUrATAajTdcbzKZzAvPXL+fwWDIs34bm78+2ltt098tDvLI1/UiInL/Sc/IzvPcwIFvMWNGKH5+XTAYDLz0UhM6der2r/W1atWGM2dO07u3L9bW1jz22BMEBo4znz99OpYaNV7+lxpERETkbrmvk+uiRYsyceJEatWqReXKlTGZTPzxxx889dRT/PHHH7ddf+vWrQkLC6NkyZKULVvWfPyJJ54gOTmZmJgYatWqRVRUFI888gilS5fOsy5ra2syMjJuu03XJSamYjTmLyGXh5emTYklFDf33pgxwebXJUuWJCQk7F+v9/T0wtPTK9cxf/838Pd/46bXT58+57bbKCIiIpa5r5PrF154gYEDB/Lmm2+SlZUFXJtuPWDAACIjb1xBNb/c3NyIj4+nU6dOuY7b2dkxe/ZsJk6cSFpaGg4ODsyePftf66pTpw6jRo2ifPnyt90uERERERERebAYTPmdryz3hEauJT80AimWUNyIJRQ3YgnFjVhCcSP5VdALmt3XI9eWio2NzXPrq0mTJuHq6nqPWyQiIiIiIiKFWaFMrh999FE2bdpU0M0QEZEHiMlkIjQ0hMcfd6F7d18yMtKZOXMqv/76M0ajiZo1n2b48FEUKVKUP/74nZkzw0hLS8dggL59B1C/foM86/qnt98OoHz58gwbNupedlFERETuokKZXN+K7Oxs3n//fTZv3ozBYCAnJ4dXXnmFN954419XAb9X8ppqIJIXR8eSBd0EeQApbq6t6H0k5mdmzZrKTz8dwd/fBYCPPlpCTk4OS5euxmQyMWHCWJYvX0rv3m8yceJY/P3fpFGjJhw//gdvvNGLqKjt2Nra8uefJ26o6+9WrvyImJhDNGvW8l53VURERO6ihza5DgkJISEhgbVr11KqVClSU1MZMGAAJUuW5NVXXy3o5uE/KZoLF9MKuhkiIoVe5Mz2RER8jKenF05Ozubjbm7uODtXxMrKCoAnn6zOiRPHAVi8eAXW1tYAnDlzmpIlS5qvu1ld1x08+D3ffruP9u19SEm5fLe7JiIiIvfQQ5lcnz9/ns2bN7Nnzx5KlSoFgL29PePGjeOPP/4gISGBcePGcf78eQwGA8OHD+fFF18kPDycuLg4Tp48yZkzZ+jUqRP9+vUjIiKCTz75hOTkZJo2bYqfn99Ny4uIyP3p+vTsAwe+Mx+rW/cF8+vz58/x8cerGTlyDAA2NjaYTCY6d27P+fPnGDJkuDnZvlldAAkJ8cyZM4OZM+eyadOGu9ofERERufceyuQ6JiYGFxcXHBwcch13cXHBxcWFt956Cx8fH5o3b86FCxfo3r07GzduBOC3335j5cqVpKSk0KJFC/Mod1xcHFFRUdjY2ORZ3t5eU71FRB40v/76C2+/PQIfn840aPCS+bjBYODjjzdx9uwZBgzow2OPPUHt2nVuWkd2djbjx7/N4MHDtWWjiIhIIfVQJtdArueqt27dyvz58zEajdjZ2XH69GmOHz/Ou+++C1z7o+jUqVMA1KtXDzs7O8qVK0fp0qVJSbm21HvNmjWxsbn2cX799dc3Lf/UU0/dyy6KiMgtuv7sedGittjbFzG//+yzzwgJCWHs2LF4eXkBkJmZyeeff07r1q2xsrLC0bEGDRs24OzZP2nVqpm5zr/XdejQIeLizjF//hwAEhISyMnJwcrKxOTJk+9xb2+fntUXSyhuxBKKG8mvgoyZhzK5fvrppzl27BipqanY29vTqlUrWrVqxenTp/Hz88NoNPLRRx9RunRp4NqodPny5fniiy8oUqSIuR6DwcD1bcKLFi1qPp5XeRERuT9d3xMzPT2L1NQM4uNT2LnzC2bOnMrMmeHUqFEz176ZM2fOIjn5Kh4erUhIiOfrr/fRps0rua75e12VK1dl/fpPzecWL17IpUvJDB066oHbw1X7zoolFDdiCcWN5FdB73NtdVfvfJ+qVKkS7dq1Y9SoUVy+fG1BmZycHHbt2oWVlRUvvPACq1atAuCPP/6gXbt2pKXd+uJit1teREQK3sKF7wEmpkyZRM+e3enZszszZ04FIDR0Bps2baBnz+6MHDmU/v2HUKNGzYJtsIiIiBQog+n60OtDxmg08uGHHxIZGYnJZCIzMxM3Nzf69u1L8eLFGTduHGfPngVgxIgRNG7cmPDwcAAGDRoEQLNmzVi2bBn79+9n//79TJkyBbg2Un2z8iIicv9Jz8gm5bK+AL1VGkkSSyhuxBKKG8mvgh65fmiT6/tdYmIqRqN+NHJr9MtHLKG4EUsobsQSihuxhOJG8qugk+uHclq4iIiIiIiIyJ2k5FpERERERETkNim5FhEREREREblNSq5FRKRQMplMTJ4czKpVy83HUlJS6NGjK7/++rP5WEJCPMOGDaRHj274+XVh27Yo87n169fQrZs3PXt2Z/z4t7l8+dIN93n77QBmzZp6dzsjIiIi971Cuc91SEgIBw8eJCsri9jYWFxcXADw8/PDx8fnlupYvXo1AN26dbO4HYMHD6ZatWrm1cXzI6+H5EXy4uhYsqCbIA+gwhg36RnZHIn5mVmzpvLTT0fw97/2O2Dfvr3MmTOL8+fP5rp+4cL3qFnzGXr3fpP4+At0796R55+vy8mTf7Jy5TIWLvyQChWc2Lr1M6ZNm8ykSdPMZVeu/IiYmEM0a9bynvZRRERE7j+FMrkeP348AKdPn8bPz49Nmzblu47bSaoB1q9fz7fffku1atUsKu8/KZoLF7U1jIhIfkXObE9ExMd4enrh5ORsPr5u3VqCgoIJDh6T63qjMYfU1FRMJhPp6elYW1tjZWXFr7/+wvPP16VCBScAGjduxtSpk8jKysLW1paDB7/n22/30b69Dykpl+9pH0VEROT+UyiT65s5ceIE48aNIzk5meLFizNmzBhq1apFYGAgBoOBo0ePkpqaSr9+/ejQoUOuPa0jIyOZP38+BoMBV1dXJk6ciK2tbZ73OnnyJJ988gldu3a9V90TEZG/GTZsFAAHDnxnPjZrVvhNr33jjYEMGNCHnTu/IDn5IgMHvkWZMmWpWfNp1q9fw/nz53B2rkhU1GaysrK4dOkSYGLOnBnMnDmXTZs23IsuiYiIyH3uoUmuAwIC6Nu3Lx4eHhw+fJghQ4awbds2AOLi4lizZg2JiYl4e3vToEEDc7m4uDjCwsKIiIjA2dmZgIAAdu/eTYsWLW56n+zsbIKCgggJCWHLli33pG8iIpLb9enuRYvaYm9fJNf0d2trK0qXLm4+9tZb/ejbtw/du3fnzz//xNfXl4YN69GyZWOSkwcxbtwoDAYDPj4+lC5dGkfHkgwZMoSxY4N46qnH+eKLImRm2hXKKfZ5eZj6KneO4kYsobiR/CrImHkokusrV64QGxuLh4cHAG5ubjg4OHD8+HEAvL29sbW1xdnZGXd3dw4cOGAue+jQIdzd3XF2vja1cPr06f96r/DwcFq2bEnVqlXvUm9EROS/xMenAJCenkVqaob5PUBOjpHk5KvEx6eQnJzMgQMHmD49nPj4FEqUKEft2nXZuXMvDg4VqFr1aRYtWgZAUlIiRuMcjhz5jdjYU0yaFPq34zlcupRKYODYe9/Ze8zRsWSuz1PkVihuxBKKG8mvexEzVlaGPNfHeiiSa5PJhMlkuuFYTk4OANbW1ubjRqMRG5u/Ppa/vwZISkoCoGzZsje917Zt27Czs2PDhg0kJCQAUKxYMXr37n37HRERkTvKwcEBR8cK7Nq1nRYtXiY5OZnDhw/Rtm17EhLiGTKkPytWfEyJEvYsXfoBLVp48MwztYiI+Mxcx+LFC7l0Kdk8FV1EREQeTg9Fcm1vb0+VKlWIjo42TwtPSEgwLza2ZcsWWrVqxdmzZ4mJiWHy5Mn88ssvALi6uhISEkJ8fDyOjo6EhoZSr149OnXqdNN7bd261fz6+nPbliTWi4M88l1GRESurRZ+qwwGA1OmzOKdd6azdOlirKwM+Pr25NlnnwPgtdd60LdvT4xGI7VquTFs2Mi71WwRERF5wD0UyTVcm84dHBxMeHg4tra2hIeHY2dnB0B6ejo+Pj5kZmYyYcIEypQpYy7n5OTEmDFj8Pf3x2g04ubmhre3911vb2JiKkaj6b8vFEHTpsQyD0PcjBkTfMOx9esjc72vVu1J3nvv/ZuW9/Hpgo9Pl3+9h7//Gxa3T0RERAoPg+mf86UfMoGBgdStW/eeJMz5oeRa8uNhSJLkzlPciCUUN2IJxY1YQnEj+aVnrh9AUVFRLFy48KbnLNlTW0RERERERB5sD/3I9f1KI9eSH/pmVyyhuBFLKG7EEoobsYTiRvKroEeure7qnUVERO4Ak8nE5MnBrFq13HwsJSWFHj268uuvP99w/eXLl+nUqT07d35hPrZ9++f06NGVHj26Mnjwm5w6FWs+t2zZErp396FLlw4sXrzwhh0mRERERP5LoZwWHhISwsGDB8nKyiI2NhYXFxcA/Pz88PHxuaU6Vq9eDUC3bt3yde+cnBwmTJjAgQMHMJlMdOrUiZ49e+arDiDPb0NE8uLoWLKgmyAPoPs9btIzsjkS8zOzZk3lp5+O4O9/7f/zffv2MmfOLM6fP3tDGZPJxKRJ47lyJdV8LCkpkRkzwli6dBVOTs5s2LCW2bOnMWvWXPbt28vOnV+wePEKrKysGD58EDt2fEHz5i3vWT9FRETkwVcok+vx48cDcPr0afz8/Cx6Djq/SfV1ERERJCcns3nzZtLT0+nYsSN16tTh6aefzlc9/pOiuXAxzaI2iIgUFpEz2xMR8TGenl44OTmbj69bt5agoGCCg8fcUOajjxbj4lKVq1evmI+VLVuOyMhobGxsyM7O5vz585Qq5QDAnj27aNmyFcWKFQPA09OL6OgoJdciIiKSL4Uyub6ZEydOMG7cOJKTkylevDhjxoyhVq1aBAYGYjAYOHr0KKmpqfTr148OHTqY96geNGgQkZGRzJ8/H4PBgKurKxMnTsTW1vam96lWrRpubm5YWVlRvHhxqlSpwrlz5/KdXIuIyDXDho0C4MCB78zHZs0Kv+m1+/d/w6FDB5k1K5whQ/rlOmdjY8Ovv/7MyJFvkZGRzsyZ1+qIi4ujdu065uscHSsQH3/hTndDRERECrmH5pnrgIAAfH19iYyMZPTo0QwZMoTMzEzg2h9Wa9as4aOPPmLatGnEx8eby8XFxREWFsaSJUv47LPPyMnJYffu3Xnex83NjWrVqgFw8OBBYmJiqFOnTp7Xi4jInXH+/Hnmzp3NuHETsLa2vuk1NWrUZPPmbYSEhBEQMJSUlBRMJuMN11lZ3by8iIiISF4eipHrK1euEBsbi4eHB3AtAXZwcOD48eMAeHt7Y2tri7OzM+7u7hw4cMBc9tChQ7i7u+PsfG064vTp02/pnvv372fYsGHMmDEDBweHO9wjEZGHx/XnwosWtcXevkiu58Stra0oXbo4jo4liYxcR1ZWJqNGDQUgNjaWBQvCyclJp1mzZhw9epSXXnoJAC+vl5k9eypXrybxv/9VISMj1VxvRkYKlSs/ct8/j16Q9NmIJRQ3YgnFjeRXQcbMQ5Fcm0ymG1Z+NZlM5OTkAOQa4TAajdjY/PWx/P01QFJSEgBly5bN837R0dEEBwcze/Zs6tWrd9vtFxF5mF3fUiM9PYvU1IxcW2zk5BhJTr5KfHwKXl6d8PLqZD43cGBffHw607RpC86cOc3QoUP54IPlVK5chYMHvyczMwsHByeef/5FPvzwfZo188Ta2pq1a9fh6eml7V/yoK1xxBKKG7GE4kbyS1tx3QP29vZUqVKF6OhoAA4fPkxCQoJ5+vaWLVswmUycOXOGmJgYateubS7r6urKDz/8YJ4qHhoayvbt2/O8V0xMDMHBwSxZskSJtYjIfaJSpcoEBo5lzJiR9OzZnSVLFjF16iyKFi1Kw4aNaNy4KX369MDPrwvVqz9Fq1ZtCrrJIiIi8oAxmArxZp7XVwvfsWMHx44dIzg4mOTkZGxtbQkKCsLd3Z3AwECSkpJISEggMzOTYcOG0axZs1wLmm3dupV58+ZhNBpxc3MjJCQkz+f5+vXrx8GDB83TyAEGDx5M8+bN70mfRUQKk/SMbFIua+eE+4lGksQSihuxhOJG8qugR64LdXJ9KwIDA6lbty7e3t4F3ZRcEhNTMRof6h+N5IN++YglFDdiCcWNWEJxI5ZQ3Eh+FXRy/VA8c32nRUVFsXDhwpues2RPbREREREREXmwPfQj1/crjVxLfuibXbGE4kYsobgRSyhuxBKKG8mvgh65figWNBMRERERERG5mzQtXERE7gqTyURoaAiPP+5C9+6+5OTkEB4+m/3795GTk0O3bq/RoUNHTpw4TkhIkLmc0ZjD8ePHmDx5Go0bN2PMmAD++ON3ihUrDoC7e20GDx4OQETEOj79dCMZGRlUr/4UgYFjsbOzK5D+ioiIyMOt0CbXISEhHDx4kKysLGJjY3FxcQHAz88PHx+fW6pj9erVAHTr1i3f92/evDn29n9NF1iwYAEVK1a85fJ5TTUQyYujY8mCboI8gO5G3KRnZHMk5mdmzZrKTz8dwd//2v+/mzZFcPp0LMuWreXq1au8+ebrPPlkDWrWfIalS1eZy4eHz+aJJ6rSuHEzAH788QiLFy+nfHnHXPfZvXsHGzasZf78xdjbl2Ts2FGsXbsKX9+ed7xPIiIiIv+l0CbX48ePB/7ajsuShcYsSaoBLl68iK2t7W0tbuY/KZoLF7X9jIg8eCJntici4mM8Pb1wcvprW8I9e3bSrp03NjY2lCpViubNPYiO3kLNms+Yr/nhh0Ps2rWdZcvWAHD27BmuXr3K9OmhnD9/jurVn2LgwKGUKuXA1q2f0bXra5Qq5QDAiBFvk52ddW87KyIiIvL/Hqpnrk+cOIGvry9eXl506dKFmJgY4Np2XKNHj8bHx4eXX36ZjRs3AhAeHm7e7zoyMhJPT0/atGlDYGAgWVl5/wF35MgRTCYTr776Kq+88gpbtmy5630TEbmfDBs2ilat2uQ6duFCHBUqOJnfV6jgxIULF3JdM3fuO/Tt258SJa7N3rl48SLPP1+XgIAxLFmykmLFihEWNgGAU6diuXgxiWHDBtGjR1eWLFmEvb1mcIiIiEjBKLQj1zcTEBBA37598fDw4PDhwwwZMoRt27YBEBcXx5o1a0hMTMTb25sGDRqYy8XFxREWFkZERATOzs4EBASwe/duWrRocdP7ZGZm8tJLLzFq1Cji4uJ49dVXefLJJ81T00VECrvr082LFrXF3r4Ijo4lsbIyUKZMcfO5kiWLUqyYnfn9wYMHSU29TPfunbCyuvbdb5Mm9WnSpL653oCAYTRs2BAHhyKYTEYOH/6e+fPnY2dnR2BgIMuXv8+YMWPucW8fPnoMRSyhuBFLKG4kvwoyZh6a5PrKlSvExsbi4eEBgJubGw4ODhw/fhwAb29vbG1tcXZ2xt3dnQMHDpjLHjp0CHd3d5ydr01vnD59+r/eq0WLFubEu3LlyrRs2ZK9e/cquRaRh8b1bTDS07NITc0gPj6FcuUq8McfsVSqdO3/wuPHY3FwKGu+dsOGTbRs2ZrExCvmen744RApKZdp2LAxAMnJqRgMBpKSrlKmTDnq129EWpqJtLQMGjduyYcfvq9tW+4ybY0jllDciCUUN5Jf2orrHjGZTPxzS2+TyUROTg4A1tbW5uNGoxEbm7++d/j7a4CkpCSSkpLyvNfOnTs5cuRIrmP/rENE5GHz0kuN+OyzzWRnZ5OSksL27dG89FIT8/nDhw9Su3bdXGWuXr3K7NnTuXz5EgCrVi2jSZPmWFtb06RJM3bu/IKMjHRMJhNffrmLp56qee86JCIiIvI3D03GZ29vT5UqVYiOjjZPC09ISKBatWoAbNmyhVatWnH27FliYmKYPHkyv/zyCwCurq6EhIQQHx+Po6MjoaGh1KtXj06dOt30XmfOnGHt2rXMmzePpKQkduzYwfLly/PV3sVBHrfXYRGRApKekX3T4x06dOTMmTP07Nmd7Ows2rXz5rnnapvPnz4de8OuCvXrN6Bjx6706+eP0WjExaUqI0de27brlVc6cfnyZfz9r23z9eSTNRg58u271zERERGRf2Ew/XM4t5C5vlr4jh07OHbsGMHBwSQnJ2Nra0tQUBDu7u4EBgaSlJREQkICmZmZDBs2jGbNmpkXMxs0aBBbt25l3rx5GI1G3NzcCAkJyTXa/XfZ2dmEhIRw4MABjEYjgwcPxtPTM1/tTkxMxWgs1D8auYM0bUosobgRSyhuxBKKG7GE4kbyq6CnhRf65PpWBAYGUrduXby9vQu6KWZKriU/9MtHLKG4EUsobsQSihuxhOJG8qugk+uHZlr4nRYVFcXChQtveu529rcWERERERGRB49Gru9TGrmW/NA3u2IJxY1YQnEjllDciCUUN5JfBT1y/dCsFi4icrt2795Jjx5d6dmzO4MGvcGZM6fN5+LiztOhQ2uSk5PNx06diqV//9689lon+vTx4+TJP3PVl5mZydCh/dm584t71AMRERERuVse6GnhISEhHDx4kKysLGJjY837SPv5+eHj43NLdaxevRqAbt26WdSGuLg4fHx82Lt3r/lYZGQk8+fPJysri549e/Lqq6/mu968vg0RyYujY8mCbkKhlZ6RTUL8RSZOHMvSpaupXLkKa9eu5J13pjN9+hy2bPmUxYsXkpAQn6vchAlBdOrUHQ+PVuzb9xVjxoxk+fK1GAwGfvwxhpkzp3Dy5Enat79/1nsQEREREcs80Mn1+PHjgb9WBLfkWWdLk2qA3bt3ExoaSnz8X39Qx8XFMXv2bCIiIrCzs6Nr167Uq1ePqlWr5qtu/0nRXLiYZnHbROTOiZzZnpwcIyaTidTUVADS0tKws7MjISGeL7/czfTpc/D17WwuEx9/gZMnT9KixbVt9erXb8DMmVM4evQ3qlevwbp1a+jTpz+rVi0rkD6JiIiIyJ31QCfXN3PixAnGjRtHcnIyxYsXZ8yYMdSqVYvAwEAMBgNHjx4lNTWVfv360aFDh1zbbV0fcTYYDLi6ujJx4kRsbW3zvNf69esJDw/Hy8vLfOzrr7/mhRdeoHTp0gC8/PLLbN26lYEDB97VfovI3VW8eHFGjBhNv369KFXKAaPRyPz5iylf3pHQ0Ok3XB8XF0f58uWxsvrr6RtHxwrEx8dRvXoNQkJCAZRci4iIiBQShS65DggIoG/fvnh4eHD48GGGDBnCtm3bgGt/7K5Zs4bExES8vb1p0KCBuVxcXBxhYWFERETg7OxMQEAAu3fvpkWLFnne63pi/ncXLlzA0dHR/L5ChQrExMTcwR6KSEFISjrL8uVLiIqK4tFHH2XZsmWMHx/Ipk2bMBgM5uvKlStB2bIlcXAoirW1Va7p+ra21pQpY5/rmJ2dDaVKFSuwaf16nEAsobgRSyhuxBKKG8mvgoyZQpVcX7lyhdjYWDw8rk3DdHNzw8HBgePHjwPg7e2Nra0tzs7OuLu7c+DAAXPZQ4cO4e7ujrOzMwDTp984EnUrbrb4+t//8BaRB9PWrdupWdOVYsXKEB+fgodHO8LCwvjjj9PmmSoAiYlXyMmxpUiRUsTHx3PhwmXz/wHnzp3Hzi73KpaZmdlcvpxWIKuhahVWsYTiRiyhuBFLKG4kv7Ra+B1kMpluSG5NJhM5OTkAWFtbm48bjUZsbP76buHvrwGSkpJISkrKdxucnJxISEgwv79w4QIVKlTIdz0icn+pXr0Ghw8fJCkpEYAvv9xFxYqP5Eqs/65CBSceeaQy27dHA/Dtt/swGAy4uORv/QUREREReTAUquTa3t6eKlWqEB197Y/Zw4cPk5CQQLVq1QDYsmULJpOJM2fOEBMTQ+3atc1lXV1d+eGHH8yLk4WGhrJ9+/Z8t+HFF19k3759JCUlkZaWRnR0NI0aNboDvRORglS7dh26dfNl0KA36NGjGxs2fExY2Mx/LRMSEsrGjRvw9e3MokXzmDhxaq5nsEVERESk8LBoWnhWVta/LvRVkKZPn05wcDDh4eHY2toSHh6OnZ0dAOnp6fj4+JCZmcmECRMoU6aMuZyTkxNjxozB398fo9GIm5sb3t753x7HycmJt956Cz8/P7KysujYsSO1atXKdz2LgzzyXUZE7o70jGwAfHw64+PTOc/r9u79Ptf7KlUeZe7cRf9a93+dFxEREZEHg8F0s4eE/+H7779n//799O7dmy5dunD8+HHCwsLw9PS8F228IwIDA6lbt65FCXNBSExMxWj8zx+NCKBnksQyihuxhOJGLKG4EUsobiS/CvqZ61sauZ4+fTpDhgzhiy++oHz58oSHhzN06NAHKrm2RFRUFAsXLrzpOUv21BYREREREZHC6ZaS65ycHF588UWCgoJo0aIFlStXxmg03u223VFTpkzJdxlPT89C/wWCiIiIiIiI3L5bWlnHaDQSExPDrl27aNCgAUePHiUrK+tut01ERERERETkgXBLI9dvvvkmw4cPp2PHjlSuXJlmzZoxZsyYu902EZG7asuWT1m7dpX5/ZUrqVy4EEetWm6kpPz1vM65c2dwc3Nn6tTZ/PLLT7z77kzS0tIxGnN49dUevPxy7hkue/bsYtKk8URH775nfRERERGRgnVLC5r9U05OTq49o+83ISEhHDx4kKysLGJjY3FxcQHAz88PHx+fW6pj9erVAHTr1i3f91+yZAkff/wxJpOJ4cOH4+Ghlb9F7kfpGdmkXE4DIDs7mwED+tC6dVs6dPjr/4lffvmJoKBRzJv3ARUqOOHj05bRo8dRp049LlyIo1ev15g/fzFVqjwKwKlTsYwYMZikpEQ+//zLAunXrdJCMWIJxY1YQnEjllDcSH49EAuaxcfHM2bMGE6ePMnKlSsZNWoUYWFhVKhQ4Y429E4ZP348AKdPn8bPz8+ixccsSaoBYmJi2Lx5M5s2bSI1NZUuXbpQt25dSpcuna96/CdFc+FimkVtEJFbEzmzPdf/+12xYillypTJlVhnZWUxeXIwgwcPx8nJmYyMDHr16kOdOvUAqFDBidKlSxMff4EqVR4lPT2dCRPGMmjQW4SEBBVAj0RERESkoNzSM9chISG0aNGCIkWK4ODgQI0aNQgKerD+cDxx4gS+vr54eXnRpUsXYmJigGtbdI0ePRofHx9efvllNm7cCEB4eDjh4eEAREZG4unpSZs2bQgMDPzX58337NlDy5YtKVKkCOXKlaNu3brs2rXrbndPRG5DcnIya9asZPDg4bmOf/rpJsqVc6Rx46YAFClShLZtO5jPb9oUwdWrV3n66WcAmD59Mu3be+PiUu2etV1ERERE7g+3NHJ95swZOnfuzKpVq7C1tSUgIAAvL6+73bY7KiAggL59++Lh4cHhw4cZMmQI27ZtAyAuLo41a9aQmJiIt7c3DRo0MJeLi4sjLCyMiIgInJ2dCQgIYPfu3bRo0eKm97lw4QKurq7m946Ojpw/f/7udk5ELOboWJING1bSsmULnn22Rq5zGzasYcKECTg6lryh3KJFi1i2bBkffPABlSs7snLlSkqUKMbrr7/G6dOnMRgMNy13v3kQ2ij3H8WNWEJxI5ZQ3Eh+FWTM3FJybTAYcm29lZqa+kBtxXXlyhViY2PNzz67ubnh4ODA8ePHAfD29sbW1hZnZ2fc3d05cOCAueyhQ4dwd3fH2dkZuLbn97+52SPsVla3NEFARApAfHwKmzd/ytChI3I9o3P06K9kZGTx+ONP5TqemZnJ5MnB/PnnCebNW0y5co8QH5/CunXrSU9Pp00bL7Kzs8yvZ8yYQ/nyjgXRtf+kZ9nEEoobsYTiRiyhuJH8eiCeufbw8GDEiBGkpKSwZs0a1q1bR+vWre9oI+8mk8l0Q9JrMpnIyckByLU4m9FoxMbmr4/l768BkpKSAChbtuxN7+Xk5ER8fLz5fXx8PI8//vjtdUBE7prLly9z5swpXF2fzXX88OGD1K79PAaDIdfxsWNHYTQaWbBgCcWKFTMff//9ZebX586dxc+vC0uXrkJEREREHg63vBXXxo0bMRqNfP3113Tp0oVOnTrd7bbdMfb29lSpUoXo6GjztPCEhASqVbv2XOSWLVto1aoVZ8+eJSYmhsmTJ/PLL78A4OrqSkhICPHx8Tg6OhIaGkq9evXy7H+jRo0YN24cr7/+OmlpaXzzzTcMGTIk321eHKQVxkXutvSMbM6cOUW5cuVv+CLt1KlTODtXzHUsJuYwX331JVWqPEq/fv7m4/36DaJevfr3pM0iIiIicn+6peR65MiRTJs2jQ4dOtzl5tw906dPJzg4mPDwcGxtbQkPD8fOzg6A9PR0fHx8yMzMZMKECZQpU8ZczsnJiTFjxuDv74/RaMTNzQ1vb+8871OrVi3atWtHx44dyc7OZvDgwTg5OeW7vYmJqRiN+d4lTR5SmjZluaeeepq1azfecHz48FE3HKtVy429e7//zzorVnzkvt+GS0RERETurFva57pdu3Zs2rTphumRhUFgYCB169b914S5ICi5lvxQci2WUNyIJRQ3YgnFjVhCcSP59UA8c+3o6EibNm149tlnKVGihPn4g7Yd150SFRXFwoULb3rOkj21RURERERE5MF2S8n1c889x3PPPXe321IgpkyZku8ynp6eeHp63oXWiIiIiIiIyIPolpLrgQMH3u12iIjccVu2fMratX+t2H3lSioXLsTxySdRlC1bDoC33w6gfPnyDBt27RnrP/74nZkzw0hLS8dggL59B1C/fgMAdu/eyZIlCzEYrChZsiSBgWOpVKnyve+YiIiIiNx3bim59vLyuunxyMjIO9qY/AoJCeHgwYNkZWURGxuLi4sLAH5+fvj4+NxSHatXrwagW7duFrUhLi4OHx8f9u7daz62Y8cO5s6dy9WrV2nYsKFF0+fzmscvkhdHx5IF3YT7TucunWjdui0A2dnZDBjQh1df7WFOrFeu/IiYmEM0a9bSXGbixLH4+79Jo0ZNOH78D954oxdRUdsxGnOYOHEsS5eupnLlKqxdu5J33pnO9OlzCqRvIiIiInJ/uaXkeuzYsebXWVlZfPHFF1SoUOGuNepWjR8/HoDTp0/j5+dn0fPOlibVALt37yY0NDTXvtanTp1i/PjxrFu3jnLlytGjRw92795N48aN81W3/6RoLlxMs7htIgKRM9tzfUmLFSuWUqZMGTp0uPbF28GD3/Ptt/to396HlJTL5jKLF6/A2toagDNnTlOyZEmsrKzIysrCZDKRmpoKQFpamnnHARERERGRW0qu69atm+v9iy++SNeuXenXr99dadTtOHHiBOPGjSM5OZnixYszZswYatWqRWBgIAaDgaNHj5Kamkq/fv3o0KED4eHhAAwaNIjIyEjmz5+PwWDA1dWViRMnYmtrm+e91q9fT3h4eK6R/c8//xxPT0+cnZ0BmD17NkWKFLm7nRaRf5WcnMyaNStZsmQFAAkJ8cyZM4OZM+eyadOGXNfa2NhgMpno3Lk958+fY8iQ4VhbW1O8eHFGjBhNv369KFXKAaPRyPz5iwuiOyIiIiJyH7ql5PqfLl68yIULF+50W+6IgIAA+vbti4eHB4cPH2bIkCFs27YNuDaFe82aNSQmJuLt7U2DBg3M5eLi4ggLCyMiIgJnZ2cCAgLYvXs3LVq0yPNe1xPzvzt58iS2trb4+/sTHx9P06ZNGTp06B3vp4jcGkfHkmzYsJKWLVvw7LM1yMrKYujQsYwdG8RTTz3OF18UITPT7oZp9Tt37uDUqVO8+uqrPPvs05QtW5bly5cQFRXFo48+yrJlyxg/PvCB36ZQjxOIJRQ3YgnFjVhCcSP5VZAxY9Ez12fPnqVz5853pUG348qVK8TGxuLh4QGAm5sbDg4OHD9+HABvb29sbW1xdnbG3d2dAwcOmMseOnQId3d384jz9OnTLWpDTk4O33//PcuXL6d48eL079+fTz755L7bR1vkYREfn8LmzZ8ydOgI4uNT+PHHGGJjTzFpUigASUmJGI05XLqUyvDhgezevYNmzVpiZWVF0aKlcXevw3ffHcZkMlGzpivFipUhPj4FD492hIWF8ccfpyldunTBdtJC2j9ULKG4EUsobsQSihvJrwdin+u/P3NtMBgoW7asefGw+4nJZMJkMt1wLCcnB8D8HCWA0WjExuav7v/9NUBSUhIAZcuWzVcbypcvT/369c3lmjdvTkxMjJJrkQJy+fJlzpw5havrswA880wtIiI+M59fvHghly4lm1cLf//9+RiNJjw8WpGQEM/Bg9/j49OZK1euEBHxMUlJiZQtW44vv9xFxYqPPLCJtYiIiIjcWVa3ctHGjRupW7cudevWpU6dOri4uDBo0KC73bZ8s7e3p0qVKkRHRwNw+PBhEhISqFatGgBbtmzBZDJx5swZYmJiqF27trmsq6srP/zwg3lxstDQULZv357vNjRt2pS9e/dy+fJlcnJy+PLLL3n66afvQO9ExBJnzpyiXLnyN3yBlpfQ0Bls2rSBnj27M3LkUPr3H0KNGjWpXbsO3br5MmjQG/To0Y0NGz4mLGzmXW69iIiIiDwo/vWvzfHjxxMXF8eBAwfMI7lwbUub61Ot7zfTp08nODiY8PBwbG1tCQ8PN6/om56ejo+PD5mZmUyYMIEyZcqYyzk5OTFmzBj8/f0xGo24ublZNNr87LPP0rt3b7p3705WVhYNGjS45W3B/m5xkEe+y4hIbukZ2Tz11NOsXbsxz2v8/d/I9d7FpSrvvff+Ta/18emMj8/990iMiIiIiBQ8g+mf86j/5siRI/z++++Eh4czePBg83Fra2uee+45qlSpck8aeScEBgZSt27dB2Z6dmJiKkZjnj8akVz0TJJYQnEjllDciCUUN2IJxY3k1339zLWrqyuurq68+OKL5oW+HiZRUVEsXLjwpucs2VNbRERERERECqd/Hbm+7tChQyxatIirV69iMpkwGo2cPn2aXbt23YMmPpw0ci35oW92xRKKG7GE4kYsobgRSyhuJL8KeuT6lhY0CwoK4rnnniM1NRUvLy/s7e3N212JiIiIiIiIPOxuaflcg8FA3759uXjxIk888QTt2rWjW7dud7ttIiI32LLlU9auXWV+f+VKKhcuxPHJJ1EsW/Yh+/fvIycnh27dXqNDh44AHDz4Pe+9N4fs7GyKFCnC0KEjqFnzGXMdmZmZjBw5lPbtvWnatMU975OIiIiIPPhuKbkuUaIEAI8++ii///47tWvXNu8dXZBCQkI4ePAgWVlZxMbGmvfe9vPzu+UVulevXg2Q7y8LcnJymDBhAgcOHMBkMtGpUyd69uwJwJw5c9i2bRsGg4GOHTvy+uuv56tuIM+pBiJ5cXQsWdBNuOvSM7Jp3botrVu3Ba7tXDBgQB9efbUHu3bt4PTpWJYtW8vVq1d5883XefLJGlSrVp1x40Yza1Y4Tz5Zg6+++pKJE8exenUEAD/+GMPMmVM4efIk7ds/GAseioiIiMj955aS61q1ajF06FCGDBnCG2+8wZ9//om1tfXdbtt/Gj9+PACnT5/Gz8/PokXGLB2Bj4iIIDk5mc2bN5Oenk7Hjh2pU6cOV65c4ZtvvmHz5s1kZ2fj6elJ48aNeeKJJ/JVv/+kaC5cTLOobSKFVeTM9vz9KZoVK5ZSpkwZOnTwYejQ/rRr542NjQ2lSpWieXMPoqO3ULPmM2zcuAUbGxtMJhNnz57BwaG0uY5169bQp8//tXfnYVFX/f/HnwMMJqG4hOKC3UWk5Y0SmuataZmiuaSCWZqQaZrmVhqKSyqgYOKWWGalqWlguWQkJmWJuaTlRn5vzbVwC1lEhdhnfn/4a+5MMGdEEXw9rqvrYs58zvmcA++m3nPO55xX+eSTZbd8PCIiIiJSflxXcj1+/Hj279/Pfffdx/jx49m+fTszZ8682X2zyYkTJ5g0aRIZGRk4OTkxYcIEGjVqRHBwMAaDgcOHD5OZmcmQIUPo3r07UVFRAAwfPpzY2FgWLFiAwWDAy8uLsLAwjEZjkffx9PTE29sbOzs7nJyccHd35+zZs7Rr145ly5bh4OBAcnIyhYWFODk53cpfgcgdISMjg5iYFSxevByAc+eSqVGjpuX9GjVqcuzYUQAcHBxIT0+jf/++XLiQQUhIhOW6kJBwACXXIiIiInJDrvuZazs7O2JiYvDz88PFxcXqmdhbJSgoiEGDBuHr68u+ffsYOXIkGzduBCA5OZmYmBjS0tLw8/OjZcuWlnrJyclERESwZs0a3NzcCAoKIiEhgXbtin7+0tvb2/Lznj17SExMZMaMGQAYjUbmzZvH4sWL6dixIzVr1iyyDRGx3p/L31evXkH79u1o3LgBcHnnxqpVnSzvV6p0FxUrOlpeu7pWYtu2rfzf//0f/fr1w8fn39x3332Wdh0dHahcueIdsbz+r+608UrJUNyILRQ3YgvFjVirNGPmupLr1atXs3jxYnJzc2nfvj2vvvoqr7/+Or169brZ/bNKVlYWSUlJlp3Mvb29cXFx4fjx4wD4+flhNBpxc3PDx8eH3bt3W+ru3bsXHx8fy3nekZGR13XPXbt2MWrUKGbOnImLi4ulfMSIEQwcOJDBgwfz6aef8txzz5XUMEXuaH8er/DFF1/y2mtvWF5Xr16Do0eTqFPn8t4Lx48n4eJSjRMnzrJ794+0afMkADVq1OP++x/gp5/24+x8j6XdvLwCLl7MvqOO/NARJ2ILxY3YQnEjtlDciLXKxFFcy5cvZ+XKlTg7O1O9enXWrFnD0qVLS7STJcFsNvP3Y7vNZrNl87W/PiduMplwcPjfdwt//RkgPT2d9PT0a94vPj6e1157jVmzZllmwY8dO8bBgwcBqFixIr6+vvzyyy+2D0pErnLx4kVOnz6Jl1djS9njj7dm/frLex1cunSJTZviefzxJ7CzsyMiIpTExH0AHD9+jKSk367YLVxERERE5EZd18y1nZ0dzs7/y85r1ap1W2xo9nfOzs64u7sTHx9vWRaempqKp6cnABs2bKBjx46cOXOGxMREpk2bZkmEvby8CAkJISUlBVdXV8LDw2nevDnPPvtskfdKTExkypQpLF68mAYNGljKT506xbx58yy7kG/atOm6dy7/q0UTdY64yN/l5BYAcPr0SapXv+eKL8W6d+/J6dOn6devDwUF+TzzjB+PPNIEgIiImcybN5uCggKMRiOTJ0+94vlsEREREZEbdV3JdZUqVTh48CAGgwGAL7744ool0LeTyMhIpkyZQlRUFEajkaioKBwdHQHIycnB39+fvLw8QkNDqVq1qqVezZo1mTBhAgMGDMBkMuHt7Y2fX/HH8ixYsIDCwkLGjh1rKRsxYgRPPfUU+/fvp3v37tjb2+Pr60vnzp2tHkdaWiYmk/mfLxThzls29dBDDVm58vMryhwcHBg5cnSR1z/ySBM+/PDaG5bNn/9+SXVPRERERO5ABvPf11EX4dixY4wcOZKkpCQqV65MhQoVePfdd6lfv/6t6GOJCA4OplmzZtdMmG8nSq7FGndaci0lQ3EjtlDciC0UN2ILxY1Yq7Sfub6umWsPDw/WrVvHr7/+SmFhIffdd1+xR1SVJ3FxcSxcuLDI92w5U1tERERERETKp2vOXL/55puEhYUBlzf4qlat2i3r2J1OM9diDX2zK7ZQ3IgtFDdiC8WN2EJxI9a6rWeuDxw4YPl5wIABrF27tmR7JiJSjGPHjjJnzgyysjKxs7MnKGg8X331Jfv27bVck5p6jurV72Hp0hiOHDnM7NnTyczM5O67nRk4cAhNmjwKQELCdyxevBCDwY5KlSoRHPwmderULa2hiYiIiEg5dM3k+q+T2tfxaPYVdu7cyeDBg6lXrx5ms5n8/HyeeeYZhgwZYnUni3teOi8vj3feeYdvv/0WOzs7KlSowGuvvcZ//vOfa7Y3btw4hg0bRp06dazuy61S3LchIsVxda1U2l0oETm5BaScO8+oUUMJDn6TFi1a8f33mwkNncgnn6y2XHf27BmGDh3IxIkhAIwbN5qXXhpI587PkJaWyrBhg5g//32cnZ0JC3uTJUuiqVvXnZUrVzB3biSRkW+XzgBFREREpFy6rmeuActO4db497//zccffwxAVlYWnTp1on379jzwwANWt1WUcePG4ejoyKpVq6hQoQK//PIL/fv3Z+nSpde8x86dOxk6dGiJ9OFmGTA1nnPns0u7GyK3XOysbuza9QO1a9elRYtWALRq1YZata78Muytt6by3HN98PSsT0ZGBufOJdOx4+Wd+atXvwcPD0927tzBE088hdlsJjMzE4Ds7GzLCQIiIiIiIiXlmsm1yWTiwoULmM1mCgsLLT//qUqVKtd9o5ycHOzt7alUqRL79u1j2rRp5ObmUrVqVUJDQ7n33ns5ceIEkyZNIiMjAycnJyZMmECjRo0sbWRnZ9O/f3+6dOlCq1at+Pbbb9m2bRsVKlQAoH79+syePZu77roLgDlz5rBjxw4uXLhA1apViYqKYu3atZw7d45BgwaxYsUKTp48SUREBDk5OVStWpWQkBDc3d05fPgwwcHBFBYW0rRpU7Zs2cLXX39NamoqEyZM4MyZMzg4OPD666/TunVroqKi2LdvH2fPnuX5559n8eLFlhn1Xbt28f777/Phhx9a87cRuWOdPPkb1atXJyIilKNHj+DsXIlXXx1heX/Hjm2cO5dMz57PA5c/i2rVqs2GDV/SpUs3Tp8+RWLiPurXb4CTkxNvvDGOIUP6U7myCyaTiQULFpXW0ERERESknLpmcn348GEee+wxS0LdvHlzy3sGg4GDBw9es/EDBw7QrVs3TCYTSUlJPP3001StWpXevXszd+5cGjVqxIYNGxg1ahSrV68mKCiIQYMG4evry759+xg5ciQbN24EID8/n2HDhtGhQwdeeOEFvvrqKx544AGcnJyuuOefffztt984fvw4MTEx2NnZMWbMGGJjYxk0aBAxMTG8//773H333UycOJH33nuP2rVr8/333/Pmm2+yZMkSgoODGTlyJG3atGHJkiUUFhYCEBYWxmOPPcZLL73EyZMn6d27N59//jlweZl6XFwcAPHx8ezcuZMWLVqwdu3aMnMEmMjtoEIFe374YTvLli2jcePGfPPNN4wd+xrfffcdjo6OfP75pwwZMhg3tyqWOu+/v5C33nqLNWtWUr9+fZ588gmqVHEmPf0MH3+8mLi4OOrVq8eyZcuYPDmYdevW2bQip7wpL48TyK2luBFbKG7EFoobsVZpxsw1k+tDhw7dUON/XxY+ePBgPvjgAypXrmyZkX766aeZNGkSly5dIikpCV9fXwC8vb1xcXHh+PHjALz99tvY2dkxf/58AOzs7K75HPi9997L2LFj+eyzzzhx4gT79u2jXr16V1zz66+/cvLkySueA8/MzCQjI4PTp0/Tpk0bAPz9/Vm2bBkAP/zwA1OnTgXA3d2dxo0bs3//foArZtn9/f354osv8Pb25ocffiAkJMTG36LInadixcrUq3cvtWvfT0rKJRo3bk5BQQH79x/CxaUK+/btY8qU6VfsBpmWdonQ0Bk4OFz+WBs9egRNm7bgq6828fDDXlSsWJWUlEv4+j5DREQER4+esmr1TXmkXVjFFoobsYXiRmyhuBFrlfZu4XY39c5/cffdd9OuXTt27tx51Xtms5lLly5dlSz/uRwdoHPnzrRp04Z58+YBlxP3Y8eOkZOTc0WdJUuWsH79eg4cOMCAAQMwmUx06NCBdu3aXdW+yWSibt26rFu3jnXr1rFmzRo++eQT7O3ti03cr9XHP5ejA3Ts2JFt27axceNGWrdurWc8Razw2GP/4ezZsxw6dHl1zL59ewADtWrV5uef99OgQUMqVqx4RZ0ZM8L5/vvNAPz8835OnDhG06bNqV+/Afv27SE9PQ2A77/fTK1ate/4xFpEREREStYtS64LCwvZtWsXjRs3JiMjg8TERADi4uKoXbs2tWvXxt3dnfj4eAD27dtHamoqnp6eADz00EMEBQURGxvLwYMHqV27Nk888QRhYWHk5uYC8N///pcPP/wQT09PfvzxR5o1a0bv3r154IEH2LZtmyUJtre3p7CwkPvvv58LFy7w008/AbB69WreeOMNKlWqRL169UhISAAgNjbWMo7HHnuMVatWAXDy5En27NmDt7f3VeOtWLEirVu3Zvbs2VoSLmKl6tXvISJiJrNmTScgoBfz5s1m2rRIKlSowKlTSdSqVeuqOmPGjCc6ejmBgc8xf/5cwsNnUrFiRZo0eZTevQMYPvwVXnyxN6tXf0pExKxSGJWIiIiIlGcGs7VnbF2nvx7FBZc3I/Py8iIsLIxffvmF8PBwsrOzcXFxITQ0FA8PD44dO8aUKVPIyMjAaDQyceJEfHx8rjiKa+3atSxfvpxPP/2UvLw8Zs6cybZt23B0dKRixYq89tprtGjRguTkZIYNG0ZOTg5Go5EHHngAk8nEzJkzmTZtGlu2bOHDDz8kNTXVsrmas7Mzb731FvXq1ePYsWOMHz+evLw86tevT2JiInFxcSQnJzNp0iTOnDkDwMiRI2nXrh1RUVEADB8+3PI72LFjB2FhYZbnsEXkn+XkFnDponbKvxW03E5sobgRWyhuxBaKG7FWaS8Lv2nJdVk3f/58evXqRY0aNYiPjyc2NtaSQF+PwsJC5syZQ/Xq1XnppZesvn9aWiYmk/40cn30Hx+xheJGbKG4EVsobsQWihuxVmkn19d9zvWdpnbt2vTv3x8HBwcqV67MtGnTrKrv7+9P1apVWbBgwU3qoYiIiIiIiNwulFwXw8/P74aelf7zeC4REREREREp/5Rci0ipOnbsKHPmzCArKxM7O3uCgsbToMFDbN68iWXLPiI/Pw83t1pMnBiCi0sVcnJymD49jCNHfsFkMjFkyAhat37iijY//PA9Ll68wKhRY0tnUCIiIiJyx7llu4XfDCEhIXTr1o1OnTrx73//m27dutGtWzdWr1593W1ER0cTHR1tcx+Sk5Np1arVFWVvv/02nTp1onPnznz00Uc2ty1S3uXk5DBq1FBeeCGQjz76hH79BhAaOpFDh/7LnDkzmDZtBh9//Cnu7vV4//13AVi8eCEVKzqxYsUq5sx5l1mzpnPuXDIA584lM3HiGKKjPy7NYYmIiIjIHahMz1xPnjwZgFOnThEYGMi6deusbqN379423z8hIYHw8HBSUlIsZbt27eKHH37giy++oKCggE6dOtGmTRvuv/9+q9ou7iF5keK4ulYq7S5YLW7DdmrXrkuLFpe/oGrVqg21atVh/fov6Ny5G7Vq1Qagf/9XuHAhA4AtWzYzefJUANzc3GjW7DG+/fZrnn++L19+uY5GjR7h3nvv49Kli6UyJhERERG5M5Xp5LooJ06cYNKkSWRkZODk5MSECRNo1KgRwcHBGAwGDh8+TGZmJkOGDKF79+5XHKEVGxvLggULMBgMlmPDjEZjsfdatWoVUVFRdO3a1VLWrFkzli1bhoODA8nJyRQWFuLk5GT1OAZMjefceR1HJOXbMw3OUb16dSIiQjl69AjOzpV49dURnDz5Gx4engQHj+Ls2bN4eDzA8OGjgMuz0zVq1LS04epag5SUcwD07z8IgEWLFt76wYiIiIjIHa1MLwsvSlBQEAEBAcTGxjJu3DhGjhxJXl4ecHkJd0xMDEuXLmXGjBlXzDgnJycTERHB4sWLWb9+PYWFhSQkJFzzXlFRUTz44INXlRuNRubNm0fnzp1p0aIFNWvWLKK2iBQUFLBjxzaeecaPRYs+pmfPXgQFjSQ3N5dt274nKGg8H320gmrVqjNjxuXZapPJdFU7dnb2t7rrIiIiIiJXKFcz11lZWSQlJeHr6wuAt7c3Li4uHD9+HLi8A7jRaMTNzQ0fHx92795tqbt37158fHxwc3MDIDIy8ob6MmLECAYOHMjgwYP59NNPee65526oPZHyqEaNGnh4ePDEEy0A8PPryowZ03Bzq0Hjxl40aHAfAH37Ps+LL76Iq2slateujcmUbVkGn5mZQYMGDa5YFn/33RXIy3Msk0vlbzX9jsQWihuxheJGbKG4EWuVZsyUq+TabDZjNpuvKissLATA3v5/s1smkwkHh/8N/68/A6SnpwNQrVo1q/pw7Ngx8vLyeOihh6hYsSK+vr788ssvVrUhcqdo3bo1ERHT+f77XTRo8BD79u3BbIY2bdrzzjtzefbZvri4VOHzz7+kfv2HSEm5RIsWj7N06XLeeGMc584lk5CQwHPPBZKScsnSblZWLtnZeVeUydVcXSvpdyRWU9yILRQ3YgvFjVjrVsSMnZ2h2P2xytWycGdnZ9zd3YmPjwdg3759pKam4unpCcCGDRswm82cPn2axMREmjRpYqnr5eXF/v37LUvFw8PD2bRpk9V9OHXqFBMnTiQvL4+8vDw2bdp0xX1E5H9cXV2JiJjJrFnTCQjoxbx5s5k2LZJWrVrTq1cfhg0bRN++z/Lzz4mMGTMBgAEDXiE7+w/69u3Fa6+9yquvjqROnbqlPBIRERERudOVq5lruLyce8qUKURFRWE0GomKisLR0RG4fOyPv78/eXl5hIaGUrVqVUu9mjVrMmHCBAYMGIDJZMLb2xs/Pz+r79+mTRv2799P9+7dsbe3x9fXl86dO1vdzqKJvlbXESlrcnIL8Pb24YMPll71Xo8ePenRo+dV5U5OTrz5Ztg12x0w4JUS66OIiIiIyPUwmP++jrqcCg4OplmzZjYlzKUhLS0Tk+mO+NNICdCyKbGF4kZsobgRWyhuxBaKG7FWaS8LL3cz1yUpLi6OhQuLPtLHljO1RUREREREpHy6Y2auyxrNXIs19M2u2EJxI7ZQ3IgtFDdiC8WNWKu0Z67L1YZmIiIiIiIiIqVBy8JFpMQdO3aUOXNmkJWViZ2dPUFB42nQ4CEALl26xLBhAxk3bhINGjwMwJEjh5k9ezqZmZncfbczAwcOoUmTRwFISPiOxYsXYjDYUalSJYKD39Tu4CIiIiJy2ymXyXVISAh79uwhPz+fpKQkPDw8AAgMDMTf3/+62oiOjgagd+/eVt27oKCAkJAQ9u7di8FgYNCgQXTt2tW6AUCxSw1EiuPqWqm0u0BObgEp584zatRQgoPfpEWLVnz//WZCQyfyySer2bFjK2+/PZvffz9zRb1x40bz0ksD6dz5GdLSUhk2bBDz57+Ps7MzYWFvsmRJNHXrurNy5Qrmzo0kMvLtUhmfiIiIiEhxymVyPXnyZODymdOBgYE2bT5mbVL9p9jYWLKysvjyyy9JT0/n6aef5sknn8TZ2bpkecDUeM6dz7apDyKlJXZWN3bt+oHatevSokUrAFq1akOtWnUA+OyzlUycOIUpUyZY6mRkZHDuXDIdO14+sq569Xvw8PBk584dPPHEU5jNZjIzMwHIzs62HK0nIiIiInI7KZfJdVFOnDjBpEmTyMjIwMnJiQkTJtCoUSOCg4MxGAwcPnyYzMxMhgwZQvfu3YmKigJg+PDhxMbGsmDBAgwGA15eXoSFhWE0Gou8T48ePSwz1efOncNoNBZ7rUh5dPLkb1SvXp2IiFCOHj2Cs3MlXn11BACzZ0dddX2VKlWoVas2GzZ8SZcu3Th9+hSJifuoX78BTk5OvPHGOIYM6U/lyi6YTCYWLFh0q4ckIiIiIvKP7pjkOigoiEGDBuHr68u+ffsYOXIkGzduBCA5OZmYmBjS0tLw8/OjZcuWlnrJyclERESwZs0a3NzcCAoKIiEhgXbt2hV7LwcHByZMmMC6desYNGgQFSpUuOnjE7ldVKhgzw8/bGfZsmU0btyYb775hrFjX+O7776zzDrb29tRpYqTZSn7++8v5K233mLNmpXUr1+fJ598gipVnElPP8PHHy8mLi6OevXqsWzZMiZPDmbdunUYDIbSG2Q5cjs8TiBlj+JGbKG4EVsobsRapRkzd0RynZWVRVJSEr6+vgB4e3vj4uLC8ePHAfDz88NoNOLm5oaPjw+7d++21N27dy8+Pj64ubkBEBkZeV33nDZtGm+88QYBAQH4+PjQqlWrEh6VyO2pYsXK1Kt3L7Vr309KyiUaN25OQUEB+/cf4l//ug+AwkITGRl/WI5KSEu7RGjoDBwcLn8kjR49gqZNW/DVV5t4+GEvKlasSkrKJXx9nyEiIoKjR09RpUqV0hpiuaEjTsQWihuxheJGbKG4EWvpKK5bwGw28/fjvM1mM4WFhQDY29tbyk0mk+V/8IErfgZIT08nPT292HsdOHCAX3/9FYCqVavy+OOP88svv9zoEETKjMce+w9nz57l0KGDAOzbtwcwUKtW7WLrzJgRzvffbwbg55/3c+LEMZo2bU79+g3Yt28P6elpAHz//WZq1aqtxFpEREREbjt3RHLt7OyMu7s78fHxAOzbt4/U1FQ8PT0B2LBhA2azmdOnT5OYmEiTJk0sdb28vNi/fz8pKSkAhIeHs2nTpmLvtX//fiIjIzGZTGRmZrJ161Z8fHxu4uhEbi/Vq99DRMRMZs2aTkBAL+bNm820aZHXfDxizJjxREcvJzDwOebPn0t4+EwqVqxIkyaP0rt3AMOHv8KLL/Zm9epPiYiYdQtHIyIiIiJyfQzmv0/pliN/7hb+7bffcuzYMaZMmUJGRgZGo5GJEyfi4+NDcHAw6enppKamkpeXx6hRo2jbtu0VG5p99dVXvPvuu5hMJry9vQkJCblitvuvCgsLCQkJYffu3djZ2fHCCy/w/PPP38phi5SanNwCLl3ULvdlhZbbiS0UN2ILxY3YQnEj1irtZeHlOrm+HsHBwTRr1gw/P7/S7soV0tIyMZnu6D+NWEH/8RFbKG7EFoobsYXiRmyhuBFrlXZyfUdsaFbS4uLiWLhwYZHv2XKmtoiIiIiIiJRtd/zM9e1KM9diDX2zK7ZQ3IgtFDdiC8WN2EJxI9bSzLWIlBlRUXP47rtvqFzZBYB69e4lNDSCRYsW8u23X2NnZ0f9+g8RFDSeChUqcPToEWbNiiA7OweDAQYNGkqLFi2v2ZaIiIiISFlUppPrkJAQ9uzZQ35+PklJSXh4eAAQGBiIv7//dbURHR0NQO/eva26d2FhIaGhoezevRuz2cyzzz5Lv379AJg/fz4bNmwAoE2bNowZM8aqtkVuVwcOJBISEo6XV2NL2Z49P7FpUzwffbQCR8cKjB8fxOrVK+nTJ5CwsDcZMGAwrVs/wfHjR3nllf7ExW3CaDQW2ZaIiIiISFlVppPryZMnA//bFdyW552tTar/tGbNGjIyMvjiiy/IycmhZ8+ePProo1y4cIGtW7eydu1aDAYDL7/8Ml9//TXt27e3qv3ilhqIFMfVtdJNazsnt4C01AscOfIL0dHLmTlzOnXr1mX48NGYTCby8vLIzc3Fzs6evLw8HB0dAVi0aLllZ/3Tp09RqVIl7OzsyMvLK7ItNze3mzYGEREREZGbqUwn10U5ceIEkyZNIiMjAycnJyZMmECjRo0IDg7GYDBw+PBhMjMzGTJkCN27d7/iyK3Y2FgWLFiAwWDAy8uLsLAwjEZjkffx9PTE29sbOzs7nJyccHd35+zZs9x7770EBwdbkgsPDw/OnDlj9TgGTI3n3HkdaSS3h9hZ3UhNTcHHpymDBw/F3f1eoqM/Zty4USxevIJHH22Ov38XHByM1Kt3L926XV454uDggNlsplevbvz++1lGjhyNvb09ycm/F9uWwWAo5dGKiIiIiFjPrrQ7UNKCgoIICAggNjaWcePGMXLkSPLy8gBITk4mJiaGpUuXMmPGDFJSUiz1kpOTiYiIYPHixaxfv57CwkISEhKKvY+3tzeenp4A7Nmzh8TERB599FFL0g3w66+/EhcXR5s2bW7egEVukdq16zBz5jzq1fsXBoOB3r0DOH36NOvXr+PMmTOsW/cV69Z9Ra1atZk/f46lnsFg4NNP1xETs5bly5eye/ePxbZ19qz1X0SJiIiIiNwOytXMdVZWFklJSfj6+gKXE2AXFxeOHz8OgJ+fH0ajETc3N3x8fNi9e7el7t69e/Hx8bEsS42MjLyue+7atYtRo0Yxc+ZMXFxcLOVHjhzhlVdeYezYsfzrX/8qoRGKlJ60tNMcOnSI7t27A3D5oAEzCQmb8Pfvzr33Xv53JzDwBcLCwnBxqcDXX3/N008/jZ2dHa6uDWjVqiVnzvzKv/5Vu8i2atasclOXt8vV9PsWWyhuxBaKG7GF4kasVZoxU66Sa7PZzN9PFjObzRQWFgJYnv0EMJlMODj8b/h//RkgPT0dgGrVqhV7v/j4eKZMmcKcOXNo3ry5pXz37t2MGDGC8ePH07lzZ9sHJHIbuXAhm7Cwqdx3XwNq167DmjWf4eHxAPXrN+TLLzfwn/+0xd7eni++WE/9+g9z4UIus2bNJiPjD3x9O5KamsL27Tvo3LlHsW3Z29+tIzduIR1xIrZQ3IgtFDdiC8WNWEtHcZUgZ2dn3N3diY+Px9fXl3379pGammpZvr1hwwY6duzImTNnSExMZNq0aRw8eBAALy8vQkJCSElJwdXVlfDwcJo3b86zzz5b5L0SExOZMmUKixcvpkGDBpbys2fPMnToUObMmUOLFi1u/qBFbpH773+A118PYuzY1zGZTLi61mDy5HCqVq1KVNQc+vbthaOjkQceeJBRo8YCEB4+k9mz3+KTT5ZhZ2fg1VdH0qDBwwBFtiUiIiIiUlYZzH+f6i2D/twt/Ntvv+XYsWNMmTKFjIwMjEYjEydOxMfHh+DgYNLT00lNTSUvL49Ro0bRtm3bKzY0++qrr3j33XcxmUx4e3sTEhJyxWz3Xw0ZMoQ9e/ZcsbvxiBEj2LFjB6tXr6ZevXqW8ueff97mXclFbgc5uQVcuqgN9sobzQiILRQ3YgvFjdhCcSPWKu2Z63KRXF+P4OBgmjVrhp+fX2l35bqkpWViMt0RfxopAfqPj9hCcSO2UNyILRQ3YgvFjVirtJPrcrUsvKTFxcWxcOHCIt+z5UxtERERERERKZ/umJnrskYz12INfbMrtlDciC0UN2ILxY3YQnEj1irtmetyd861iIiIiIiIyK2mZeEict2ioubw3XffULny5TPd69W7l9DQCBYtWsi3336NnZ0d9es/RFDQeCpUqMCpUyeZOTOCjIwMCgry6dy5G7179wVg//59zJs3i8LCQhwdHRk1aoxlJ3ERERERkbKmzCfXISEh7Nmzh/z8fJKSkvDw8AAgMDAQf3//62ojOjoawOYdvZOTk/H392fr1q2Wsvz8fF5++WVeffXVK87Avl7FLTUQKY6ra6Wb1vafu4UfOJBISEg4Xl6NLe/t2fMTmzbF89FHK3B0rMD48UGsXr2SPn0CmTZtCp06daVr1+5kZmby8suBPPhgfZo0eZSwsDcZN24STZo8SkLCd0ydOoXlyz+9aWMQEREREbmZynxyPXnyZOB/x3HZstHYjRyTlZCQQHh4OCkpKZay48ePM378eP773//a3O6AqfGcO6+jj+T2EDurG2l5eRw58gvR0cuZOXM6devWZfjw0ZhMJvLy8sjNzcXOzp68vDwcHR0B6NKlG0895QtcPoe+bt26/P77WQBMJhOXLl0E4I8/six1RERERETKojKfXBflxIkTTJo0iYyMDJycnJgwYQKNGjUiODgYg8HA4cOHyczMZMiQIXTv3v2Ks65jY2NZsGABBoMBLy8vwsLCMBqNxd5r1apVREVF0bVr1yvKXn75ZZYuXXrTxypyq6SmpuDj05TBg4fi7n4v0dEfM27cKBYvXsGjjzbH378LDg5G6tW7l27dLq8a6dz5GUv9H37YzoEDiQQHTwJg3Lg3GTfuDd5+exaZmZeYM+edUhmXiIiIiEhJKJfJdVBQEIMGDcLX15d9+/YxcuRINm7cCFxewh0TE0NaWhp+fn60bNnSUi85OZmIiAjWrFmDm5sbQUFBJCQk0K5du2Lv9Wdi/ldjxowBUHIt5Urjxg1YuvQjy+sRI15l6dJFJCRsJDU1ma1bt+Lo6Mi4ceNYtOgd3nzzTcu1a9euZfr06URFRfHQQ/eRmprKzJkRLF++HC8vL7755hsmTQpm48aNODk5lcbw7lg383ECKb8UN2ILxY3YQnEj1irNmCl3yXVWVhZJSUn4+l5eiurt7Y2LiwvHjx8HwM/PD6PRiJubGz4+PuzevdtSd+/evfj4+ODm5gZAZGTkrR+AyG1qx449HD16mI4dOwNgNpsxmcysXfsFvr4dyc42k52di69vF+bMmUFKyiXMZjPz589l8+ZNzJnzDp6e9UlJucR3332Pq2tN3Nz+RUrKJRo3bo6dnT27d/+sTc1uIR1xIrZQ3IgtFDdiC8WNWEtHcZUws9nM34/uNpvNFBYWAmBvb28pN5lMODj87/uFv/4MkJ6eTnp6+k3srUjZYWdnYO7cmZw5cxqAtWtX8cADD+Dl1YiEhO8oKCjAbDazZct3NGzoBcDbb89k//69fPjhx3h61re05eHhyfHjx0hK+g2A//u/A+Tk5ODuXu/WD0xEREREpASUu5lrZ2dn3N3diY+PtywLT01NxdPTE4ANGzbQsWNHzpw5Q2JiItOmTePgwYMAeHl5ERISQkpKCq6uroSHh9O8eXOeffbZ0hySyG3h/vsf4PXXgxg79nVMJhOurjWYPDmcqlWrEhU1h759e+HoaOSBBx5k1KixJCf/zurVn+LmVovXXx9qaefZZ5+nc+dneOONcUycOAaDwUCFCncxbVokd9+tXfJFREREpGwqd8k1XF7OPWXKFKKiojAajURFRVl2Is7JycHf35+8vDxCQ0OpWrWqpV7NmjWZMGECAwYMwGQy4e3tjZ+fX6mMYdFE31K5r0hRcnILAOjQoRMdOnS66v033gi+qszZ2Znvv/+x2Dbbtm1H27bF72cgIiIiIlKWGMx/X0NdjgUHB9OsWbNSS5itkZaWicl0x/xp5AbpmSSxheJGbKG4EVsobsQWihuxVmk/c10uZ65LUlxcHAsXLizyPVvO1BYREREREZHy546auS5LNHMt1tA3u2ILxY3YQnEjtlDciC0UN2ItzVyLSJkQFTWH7777hsqVXQCoV+9eQkMjWLRoId9++zV2dnbUr/8QQUHjqVChAufPn2fq1MkkJ5/FYDAwZswEvLwaA5CQ8B2LFy/EYLCjUqVKBAe/SZ06dUtzeCIiIiIiN6RMJ9chISHs2bOH/Px8kpKS8PDwACAwMBB/f//raiM6OhqA3r1729SH5ORk/P392bp1q6Xs/fffZ/Xq1Tg6OtKpUyeGDBliU9sit5MDBxIJCQm3JMgAe/b8xKZN8Xz00QocHSswfnwQq1evpE+fQGbPfovGjb0JDJzHkSO/EBT0GjExazEYICzsTZYsiaZuXXdWrlzB3LmRREa+XYqjExERERG5MWU6uZ48eTIAp06dIjAw0KZnoG1NqgESEhIIDw8nJSXFUrZ9+3ZiY2NZvXo1FStWZOjQoZZjwaxR3FIDkeK4ula6Ke3m5BaQlnqBI0d+ITp6OTNnTqdu3boMHz4ak8lEXl4eubm52NnZk5eXh6OjIwUFBWzf/j2jRo0FwNOzPnXrurNz53YeffQxzGYzmZmZAGRnZ1t28xcRERERKavKdHJdlBMnTjBp0iQyMjJwcnJiwoQJNGrUiODgYAwGA4cPHyYzM5MhQ4bQvXt3oqKiABg+fDixsbEsWLAAg8GAl5cXYWFhGI3GYu+1atUqoqKi6Nq1q6Xsv//9L61atcLZ+XJy/Pjjj/PNN99YnVwPmBrPufPZNvwGREpW7KxupKam4OPTlMGDh+Lufi/R0R8zbtwoFi9ewaOPNsffvwsODkbq1buXbt38uXAhA7PZfMVRdzVq1OTcuXM4OTnxxhvjGDKkP5Uru2AymViwYFEpjlBERERE5MbZlXYHSlpQUBABAQHExsYybtw4Ro4cSV5eHnB5CXdMTAxLly5lxowZV8w4JycnExERweLFi1m/fj2FhYUkJCRc815RUVE8+OCDV5Q1bNiQrVu3kpGRQW5uLt9++y2pqaklP1CRW6h27TrMnDmPevX+hcFgoHfvAE6fPs369es4c+YM69Z9xbp1X1GrVm3mz59Dcfsk2tnZcezYUZYs+ZDlyz9j3bqvCAzsz4QJY4qtIyIiIiJSFpSrmeusrCySkpIss8Te3t64uLhw/PhxAPz8/DAajbi5ueHj48Pu3bstdffu3YuPjw9ubm4AREZG2tSHFi1a4OfnR0BAAFWqVKFFixbs37//BkcmUrrS0k5z6NAhunfvDvD/E2EzCQmb8Pfvzr33Xv73JjDwBcLCwvD0vPzIhqOjCReXyxugZWSk8cAD9/J//7eHRx9tirf3QwC88kp/oqJm4+BQQLVq1W752O50N+txAinfFDdiC8WN2EJxI9YqzZgpV8m12Wy+avbLbDZTWFgIgL29vaXcZDLh4PC/4f/1Z4D09HQAq/9nPzMzk/bt2/PSSy8B8NFHH+Hu7m5VGyK3mwsXsgkLm8p99zWgdu06rFnzGR4eD1C/fkO+/HID//lPW+zt7fnii/XUr/8w589n06JFSxYtWkZAQD+OHj3CkSNH8fBoSEGBHcuWfcwvv/xKtWrV2bx5E7Vq1aaw0KjjNm4xHXEitlDciC0UN2ILxY1Yq7SP4ipXy8KdnZ1xd3cnPj4egH379pGamoqnpycAGzZswGw2c/r0aRITE2nSpImlrpeXF/v377csFQ8PD2fTpk1W9+HUqVMMHTqUgoICLl26xGeffcbTTz9dAqMTKT333/8Ar78exNixr/PCCz3ZsuU7Jk8OJyDgJWrUqEnfvr148cXnuXjxIsOGvQ7A6NHB/PzzfgICehEaOpE33wzF2dmZJk0epXfvAIYPf4UXX+zN6tWfEhExq5RHKCIiIiJyY8rVzDVcXs49ZcoUoqKiMBqNREVFWXYizsnJwd/fn7y8PEJDQ6/YbKlmzZpMmDCBAQMGYDKZ8Pb2xs/Pz+r7N2jQAF9fX5555hkKCwvp16/fFUn89Vo00boN0ERulpzcAgA6dOhEhw6drnr/jTeCi6xXrVp1ZsyYU+R7/v698PfvVXKdFBEREREpZQbzHbKLUHBwMM2aNbMpYS4NaWmZmEx3xJ9GSoCWTYktFDdiC8WN2EJxI7ZQ3Ii1SntZeLmbuS5JcXFxLFy4sMj3bDlTW0RERERERMqnO2bmuqzRzLVYQ9/sii0UN2ILxY3YQnEjtlDciLVKe+a6XG1oJiIiIiIiIlIatCxcRCyioubw3XffULny5bOp69W7l9DQCPr370teXi4ODkYAfH070qdPIJmZmTzzjC/16v3L0saIEaPw8WnKnj0/8c47b1NQUECFChV47bU3ePjhf5fGsEREREREbroynVyHhISwZ88e8vPzSUpKwsPDA4DAwED8/f2vq43o6GgAevfubVMfkpOT8ff3Z+vWrVe999Zbb3H+/HmmT59udbvFLTUQKY6rayWb6+bkFnDpYjYHDiQSEhKOl1djy3vZ2dmcOXOKL7/85qrz4P/v/36mceNHmDPnnSvK8/PzmTRpHLNnR/Hggw3Ytu17wsImER29xuY+ioiIiIjczsp0cj158mTg8tnSgYGBNm0yZmtSDZCQkEB4eLjlbOy/2rFjB2vXruWJJ56wqe0BU+M5dz7b5r6JWCN2VjfS8vI4cuQXoqOXM3PmdOrWrcvw4aM5c+YUFSs6ERQ0krS0VJo2bcYrrwylQoW7OHAgkYsXLzJkyABycrJ55hk/evToidFo5PPPN+Dg4IDZbObMmdO4uFQp7WGKiIiIiNw05e6Z6xMnThAQEEDXrl157rnnSExMBC4fxTVu3Dj8/f3p0KEDn3/+OQBRUVFERUUBEBsbS6dOnejcuTPBwcHk5+df816rVq2y1P2rjIwM5syZw+DBg0t2cCI3UWpqCj4+TRk8eChLlnxCw4ZejBs3iqysTHx8mjB16lt88MEykpN/5733Ls9U29vb07Ll48yf/z4zZsxl5cpP2LJlMwAODg6kp6fRo0cn3n33bfr0CSzF0YmIiIiI3Fxleua6KEFBQQwaNAhfX1/27dvHyJEj2bhxI3B5CXdMTAxpaWn4+fnRsmVLS73k5GQiIiJYs2YNbm5uBAUFkZCQQLt27Yq9V1GJNcCkSZN4/fXXOXv2bMkOTuQmaty4AUuXfmR5PWLEqyxduohHH/XGz6/rX8qHMXz4cKZOnUJQ0OuWcje3KrzwQm927dqKv//l611dK7Ft21b+7//+j379+uHj82/uu+++Wzco+Uc38jiB3LkUN2ILxY3YQnEj1irNmClXyXVWVhZJSUn4+voC4O3tjYuLC8ePHwfAz88Po9GIm5sbPj4+7N6921J37969+Pj44ObmBkBkZKRNffjss8+oVasWLVq0YM0aPV8qZceOHXs4evQwHTt2BsBsNmMymdm8eRtubrXw9vYB4Pz5LAwGO1JSLrFqVQytWj1h+ffm0qUcCgrMnDhxlt27f6RNmycBqFGjHvff/wA//bQfZ+d7SmeAchUdcSK2UNyILRQ3YgvFjVhLR3GVILPZzN+P7TabzRQWFgKXl7D+yWQyXbE50983akpPTyc9Pd3qPsTFxbFt2za6devGvHnz+PbbbwkPD7e6HZFbzc7OwNy5Mzlz5jQAa9eu4oEHHiA7O5t33plLbm4OhYWFxMSsoG3b9gAkJu4nOnoZABcvXmD9+nU89VR77OzsiIgIJTFxHwDHjx8jKek37RYuIiIiIuVWuZq5dnZ2xt3dnfj4eMuy8NTUVDw9PQHYsGEDHTt25MyZMyQmJjJt2jQOHjwIgJeXFyEhIaSkpODq6kp4eDjNmzfn2WeftaoPH330v2W1a9asYdeuXYwfP97qsSya6Gt1HRFb5eQWcP/9D/D660GMHfs6JpMJV9caTJ4cTo0aNThz5jT9+/elsLCQRx5pyksvDQTg9dfHEBkZTt++vSgoKMDfvxePPvoYABERM5k3bzYFBQUYjUYmT55KjRo1S3OYIiIiIiI3TblKruHycu4pU6YQFRWF0WgkKioKR0dHAHJycvD39ycvL4/Q0FCqVq1qqVezZk0mTJjAgAEDMJlMeHt74+fnV1rDIC0tE5PJ/M8XilByS2A6dOhEhw6driofOnQkQ4eOvKq8atWqhIcX/QjFI4804cMPl91wn0REREREygKD+e/rqMup4OBgmjVrVqoJszWUXIs19EyS2EJxI7ZQ3IgtFDdiC8WNWKu0n7kudzPXJSkuLo6FCxcW+Z4tZ2qLiIiIiIhI+XTHzFyXNZq5Fmvom12xheJGbKG4EVsobsQWihuxlmauReSmioqaw3fffUPlyi4A1Kt3L6GhEQBcunSJYcMGMm7cJBo0eBiAI0cOM3v2dDIzM7n7bmcGDhxCkyaPArB9+1YWLpxPXl4eHh6ejBv3JnffXfSHi4iIiIjInaRMJ9chISHs2bOH/Px8kpKS8PDwACAwMBB/f//raiM6OhqA3r17W3XvwsJCQkND2b17N2azmWeffZZ+/fpdcc1bb73F+fPnmT59ulVti5SkAwcSCQkJx8ur8RXlO3Zs5e23Z/P772euKB83bjQvvTSQzp2fIS0tlWHDBjF//vvY2dkTHh7CggWLcHevx7vvzmPBgvm88UbwrRyOiIiIiMhtqUwn15MnTwbg1KlTBAYG2vQctLVJ9Z/WrFlDRkYGX3zxBTk5OfTs2ZNHH32Uhg0bArBjxw7Wrl3LE088YVP7xS01ECmOq2ulq8ouXvqDI0d+ITp6OTNnTqdu3boMHz4aNzc3PvtsJRMnTmHKlAmW6zMyMjh3LpmOHTsDUL36PXh4eLJz5w4cHBx46KGHcXevB0CPHj3p1683o0ePxWAw3JpBioiIiIjcpsp0cl2UEydOMGnSJDIyMnBycmLChAk0atSI4OBgDAYDhw8fJjMzkyFDhtC9e3eioqIAGD58OLGxsSxYsACDwYCXlxdhYWEYjcYi7+Pp6Ym3tzd2dnY4OTnh7u7O2bNnadiwIRkZGcyZM4fBgwdz6NAhm8YxYGo8585n2/x7EAF49zUffHyaMnjwUNzd7yU6+mPGjRvF4sUrmD076qrrq1SpQq1atdmw4Uu6dOnG6dOnSEzcR/36DQDDFedUu7rWICsriz/+yNLScBERERG549mVdgdKWlBQEAEBAcTGxjJu3DhGjhxJXl4eAMnJycTExLB06VJmzJhBSkqKpV5ycjIREREsXryY9evXU1hYSEJCQrH38fb2xtPTE4A9e/aQmJjIo49efi510qRJvP7661SuXPkmjlTkn7m7uzNz5jzq1fsXBoOB3r0DOH36NGfPnim2zvTps9m8eROBgc+xaNFCWrRoiYODEbPZVOT1dnb2N6v7IiIiIiJlRrmauc7KyiIpKQlfX1/gcgLs4uLC8ePHAfDz88NoNOLm5oaPjw+7d++21N27dy8+Pj64ubkBEBkZeV333LVrF6NGjWLmzJm4uLjw2WefUatWLVq0aMGaNWtKeIQi1jl06BCHDh2ie/fuAFw+HMBMzZpVLMvI7e3tqFLFyfL6/PmKLFr0AQ4Olz8eXn75ZR5+2JOsrCyOHj1kue706dO4uLhQr16NWz4uKTlFPU4g8k8UN2ILxY3YQnEj1irNmClXybXZbObvJ4uZzWYKCwsBsLf/3wybyWSyJA/AFT8DpKenA1CtWrVi7xcfH8+UKVOYM2cOzZs3By6fjZ2SkkK3bt24cOECf/zxB+Hh4YwfP/7GBidiAzs7O8LCpnLffQ2oXbsOa9Z8hofHA9jb3205pqCw0ERGxh+W1+PGTeC55/rw5JPt+Pnn/fzyy2EefLAROTnZRERMZ8+e/8PdvR6LFy+jZcvWOiKjDNMRJ2ILxY3YQnEjtlDciLV0FFcJcnZ2xt3dnfj4eHx9fdm3bx+pqamW5dsbNmygY8eOnDlzhsTERKZNm8bBgwcB8PLyIiQkhJSUFFxdXQkPD6d58+Y8++yzRd4rMTGRKVOmsHjxYho0aGAp/+ijjyw/r1mzhl27dimxllLz4IMP8vrrQYwd+zomkwlX1xpMnhx+zTpjxoxn+vSpfPTRB1Ss6ER4+EwqVqxIxYoVGT9+EhMnjqWgIJ86deoycWLILRqJiIiIiMjtrVwl13B5OfeUKVOIiorCaDQSFRWFo6MjADk5Ofj7+5OXl0doaChVq1a11KtZsyYTJkxgwIABmEwmvL298fPzK/Y+CxYsoLCwkLFjx1rKRowYwVNPPVUi41g00bdE2pE7W05uAR06dKJDh07FXrNqVewVr++//wHef39Jkde2aNGKFi1alWQXRURERETKBYP57+uoy6ng4GCaNWt2zYT5dpKWlonJdEf8aaQEaNmU2EJxI7ZQ3IgtFDdiC8WNWEvLwm9jcXFxLFy4sMj3bDlTW0RERERERMqnO2bmuqzRzLVYQ9/sii0UN2ILxY3YQnEjtlDciLVKe+a63J1zLSIiIiIiInKraVm4SBmzZctmpk6dTHx8AgCbN28iOnoZf/yRjZtbLSZODMHFpQpnz54hMjKC5OSzVKzoRO/eATz1VHsAfv/9d2bPnk5KyjkKCwsZOvQ1mjdvUZrDEhEREREp08plch0SEsKePXvIz88nKSkJDw8PAAIDA/H397+uNqKjowHo3bu31ff/9NNPiY6O5o8//qBnz54MHDjQ6jaKW2ogd6ac3AIuXczm5Mkk3nlnLmazCYBDh/7LnDkz+PTTT6lQwYV582bx/vvvEhQ0nmnTpvDII02YPTuKP/7IYvjwwdSrdy+eng8yduzrdO/uT48ePTl8+BAjRgzhiy82WnbWFxERERER65TL5Hry5MkAnDp1isDAQJs2H7MlqQb46aefWLx4MZ999hl2dnb06NGDJ598kgceeMCqdgZMjefc+Wyb+iDlT+ysbqTk5BAa+ibDh79OSMhEADZu3EDnzt2oW7cuKSmX6N//FS5cyADgl18OMmHCFACcnO7Gx6cpW7Z8B5i5dOkiPXr0BODBBxvw7rsfYmenp0RERERERGxVLpPropw4cYJJkyaRkZGBk5MTEyZMoFGjRgQHB2MwGDh8+DCZmZkMGTKE7t27ExUVBcDw4cOJjY1lwYIFGAwGvLy8CAsLw2g0FnmfDRs20KdPHypVqgTA4sWLqVKlyq0appRjkZHT6NbNDw8PT0vZyZO/4eHhyZAhQ/jtt5N4eDzA8OGjAHj44X8TFxdL//6DyMjIYMeObTRq1JikpCTc3GoRFTWbxMT9ODjY07//K9x/v0dpDU1EREREpMy7Y5LroKAgBg0ahK+vL/v27WPkyJFs3LgRgOTkZGJiYkhLS8PPz4+WLVta6iUnJxMREcGaNWtwc3MjKCiIhIQE2rVrV+R9fvvtNypVqkTfvn25dOkSPXv2JCAg4JaMUcqvFStWcPfdFXnppb6cOnUKg8GAq2sl7Oxg585tLFmyhOrVqxMZGcncudN59913mT17JhEREQwY8AJ16tShXbu25OTk4OTkwM8/7+eVVwYSGjqZxMREBg4cyBdffEHNmjVLe6hyi7m6VirtLkgZpLgRWyhuxBaKG7FWacbMHZFcZ2VlkZSUhK+vLwDe3t64uLhw/PhxAPz8/DAajbi5ueHj48Pu3bstdffu3YuPjw9ubm4AREZGXvNehYWF7Nmzh4ULF1JQUEDfvn3x9PTkscceu0mjkzvB2rVryczMonPnrhQU5JOTk0Pnzl2pUqUKTZo0w9XVlZSUSzzxRAdGjhxCSsolzpxJY/ToCVSsWBGAmTMjqFfvX1SoUAln50o0btyclJRL1Kp1H25utdm5cy8tWrT8h55IeaIjTsQWihuxheJGbKG4EWuV9lFcd0RybTab+ftx3mazmcLCQgDs7e0t5SaTCQeH//1a/vozQHp6OgDVqlUr8l733HMPDRs25O677wbg8ccf5+eff1ZyLTdk1apVlg+Ks2fPEBj4HEuWfMLWrVt45525nD9/HnBgy5bveOihhwFYtGghDz7YgD59AkhK+o3vv0/g/fdfomrVajg6OrJ16xZatWrNb7/9yunTp3jgAc9r9EBERERERK7ljkiunZ2dcXd3Jz4+3rIsPDU1FU/Py8nEhg0b6NixI2fOnCExMZFp06Zx8OBBALy8vAgJCSElJQVXV1fCw8Np3rw5zz77bJH3evLJJ1mxYgV9+vTBZDLxww8/EBQUZHWfF030tX3AUu7k5BYUWd6qVWtSUs4REBBAfn4BNWvWYty4NwEYOnQkYWGT+OqrL7G3t2f8+MnUrHl5Bcbs2fOZM2cGCxfOB2DcuEm4uta4NYMRERERESmH7ojkGi4v554yZQpRUVEYjUaioqIsxw7l5OTg7+9PXl4eoaGhVK1a1VKvZs2aTJgwgQEDBmAymfD29sbPz6/Y+3Tq1ImkpCR69OhBQUEB3bp1o0UL688PTkvLxGQy//OFcsepVas2X3/9veV1jx49GTTopauWwLi61mDevPeKbMPD4wHmz3//pvZTREREROROYjD/fb30HSY4OJhmzZpdM2EuDUquxRp6JklsobgRWyhuxBaKG7GF4kaspWeuy6C4uDgWLlxY5Hu2nKktIiIiIiIiZdsdP3N9u9LMtVhD3+yKLRQ3YgvFjdhCcSO2UNyItTRzLXKH2LJlM1OnTiY+PgGAZcsW89VX6yksLMTX92n69x9EZmYmw4e/ckW948eP8uqrI3j++b4cO3aUOXNmkJWViZ2dPUFB42nQ4KHSGI6IiIiIiPxFmU6uQ0JC2LNnD/n5+SQlJeHh4QFAYGAg/v7+19VGdHQ0AL1797apD8nJyfj7+7N161ZLWWBgIGlpaZZjvEJDQ2ncuLFN7Uv5cPJkEu+8Mxez2QTAjh1b+e67b1i0aDl2dnaMHj2cb7/9hqeeas+SJZ9Y6q1aFcPmzd/Ss+fz5OTkMGrUUIKD36RFi1Z8//1mQkMn8sknq0tnUCIiIiIiYlGmk+vJkycDcOrUKQIDA2163tnWpBogISGB8PBwUlJSLGVms5njx4+zefPmq87ItkZxSw2kbMnJLSDl3HlCQ99k+PDXCQmZCFyexW7fviMVK1YEoFOnrsTHx/HUU+0tdU+dOsnSpYv54INlODg4sH37VmrXrkuLFq0AaNWqDbVq1bn1gxIRERERkauU6eS6KCdOnGDSpElkZGTg5OTEhAkTaNSoEcHBwRgMBg4fPkxmZiZDhgyhe/fuREVFATB8+HBiY2NZsGABBoMBLy8vwsLCMBqNxd5r1apVREVF0bVrV0vZ8ePHMRgMDBw4kLS0NHr16kXfvn2tHseAqfGcO59t/S9Abiuxs7oRGTmNbt388PDwtJQnJyfTpMmjlteurjVISTl3Rd33338Xf/9euLldPpv65MnfqF69OhERoRw9egRn50q8+uqIWzMQERERERG5JrvS7kBJCwoKIiAggNjYWMaNG8fIkSPJy8sDLic0MTExLF26lBkzZlwx45ycnExERASLFy9m/frLz8EmJCRc815RUVE8+OCDV5RdvHiRFi1a8M4777BkyRJiYmLYtm1byQ9UyoQVK1Zgb+9Aly7drij/c3n4X9nZ2Vt+Tk7+nV27dtCr1/9WVhQUFLBjxzaeecaPRYs+pmfPXgQF/S++RURERESk9JSrmeusrCySkpLw9fUFwNvbGxcXF44fPw6An58fRqMRNzc3fHx82L17t6Xu3r178fHxscwSRkZG2tSHRx55hEceeQQAJycnevbsSUJCAi1btryRoUkZtXbtWnJycnj55b7k5+eTm5vLyy/35eGHHyY3NxNX10oA5OZeom7d2pbX69evxtfXl3vvdbO0dd997nh4ePDEEy0A8PPryowZ08jJyQCqW+qKWENxI7ZQ3IgtFDdiC8WNWKs0Y6ZcJddms5m/nyxmNpspLCwEwN7+fzODJpPpimei//58dHp6OgDVqlWzqg8//fQT+fn5tGjRwnL/G3n2Wsq2VatWWY4DOHv2DIGBz/Hhh8vZunULH330AW3bdsLe3p6VKz+jU6eulmu3bt3OE088dcVRAg0b+nDy5HS+/34XDRo8xL59ezCboUIFFwAdVSFW0xEnYgvFjdhCcSO2UNyItUr7KK5ytSzc2dkZd3d34uPjAdi3bx+pqal4el5+1nXDhg2YzWZOnz5NYmIiTZo0sdT18vJi//79lqXi4eHhbNq0yeo+XLp0iRkzZpCbm0tmZiZr166lffv2/1xR7iitWrWmTZsnGTjwRQIDn6N+/Yfo2LGz5f2TJ0/i5lb7ijrVq99DRMRMZs2aTkBAL+bNm820aZFUqFDhVndfRERERET+ptxNqUZGRjJlyhSioqIwGo1ERUXh6OgIQE5ODv7+/uTl5REaGkrVqlUt9WrWrMmECRMYMGAAJpMJb29v/Pz8rL7/k08+yf79++nevTsmk4k+ffpYlolbY9FEX6vryO0nJ7fA8nOtWrX5+uvvLa8DA/sTGNi/yHrLl39aZLm3tw8ffLC0ZDspIiIiIiI3zGD++zrqcio4OJhmzZrZlDCXhrS0TEymO+JPIyVAy6bEFoobsYXiRmyhuBFbKG7EWqW9LLzczVyXpLi4OBYuXFjke7acqS0iIiIiIiLl0x0zc13WaOZarKFvdsUWihuxheJGbKG4EVsobsRapT1zXa42NBMREREREREpDUquRW6yLVs24+vbxvJ62bLF9Onjz3PPdWfRooWW4+POnz/P6NEj6Nv3WQICevHzz/stdY4dO8qwYYN46aU+DBgQwKFDB2/5OEREREREpHjl8pnrkJAQ9uzZQ35+PklJSXh4eAAQGBiIv7//dbURHR0NQO/eva269/z58/n6668tr0+cOMHIkSMZMGCAVe0Ut9RAyoac3AIuXczm5Mkk3nlnLmazCYAdO7by3XffsGjRcuzs7Bg9ejjffvsNTz3Vntmz36JxY28CA+dx5MgvBAW9RkzMWgBGjRpKcPCbtGjRiu+/30xo6EQ++WR16Q1QRERERESuUC6T68mTJwNw6tQpAgMDbdp8zNqk+k/Dhg1j2LBhAGzfvp0ZM2bQt29fq9sZMDWec+ezbeqDlL7YWd1IyckhNPRNhg9/nZCQicDlWez27TtSsWJFADp16kp8fBxt2jzJ9u3fM2rUWAA8PetTt647O3dux2Cwo3bturRo0QqAVq3aUKtWndIZmIiIiIiIFKlcJtdFOXHiBJMmTSIjIwMnJycmTJhAo0aNCA4OxmAwcPjwYTIzMxkyZAjdu3cnKioKgOHDhxMbG8uCBQswGAx4eXkRFhaG0Wi85v3y8vIICQlhxowZVKhQ4VYMUW4zkZHT6NbNDw8PT0tZcnIyTZo8annt6lqDlJRzXLiQgdlsvuLs9Ro1anLu3Dny8nKpXr06ERGhHD16BGfnSrz66ohbOhYREREREbm2Oya5DgoKYtCgQfj6+rJv3z5GjhzJxo0bgcsJT0xMDGlpafj5+dGyZUtLveTkZCIiIlizZg1ubm4EBQWRkJBAu3btrnm/devWUb9+fRo3bnxTxyW3pxUrVnD33RV56aW+nDp1CoPBgKtrJYxGOypXroirayUAqlRxwtHRSNWqTgCWcoAKFRxwcXHi4sUCfvhhO8uWLaNx48Z88803jB37Gt999x2Ojo6W6/9aV+R6KW7EFoobsYXiRmyhuBFrlWbM3BHJdVZWFklJSfj6+gLg7e2Ni4sLx48fB8DPzw+j0Yibmxs+Pj7s3r3bUnfv3r34+Pjg5uYGQGRk5HXdMyYmhokTJ5bwSKSsWLt2LZmZWXTu3JWCgnxycnLo3LkrDz5YnxMnTlqOCDh69DeqVr0Hk+lyknzs2GkqV64MwKlTZ/jPf9qQnw/16t1L7dr3k5JyicaNm1NQUMD+/Yf417/uA3RUhdhGcSO2UNyILRQ3YgvFjVhLR3HdAmazmb8f5202myksLATA3t7eUm4ymXBw+N93Dn/9GSA9PZ309PRr3i85OZnz58/zyCOP3GjXpYxatWoVH3/8KUuWfEJk5NtUqFCBJUs+oXXrJ4mP/4rs7Gzy8vKIi4uldesncHBwoEWLlqxbtwaAo0eP8OuvJ3jkkaY89th/OHv2rGWH8H379gAGatWqXYojFBERERGRv7ojZq6dnZ1xd3cnPj7esiw8NTUVT8/Lz8Ju2LCBjh07cubMGRITE5k2bRoHD15OZLy8vAgJCSElJQVXV1fCw8Np3rw5zz77bLH3+3O2+0Ysmuh7Q/WldOXkFhRZ3qpVa44fP8rAgS9SUJBPq1Zt6NixMwCjRwczffpUAgJ6YTAYePPNUJydnXF2diYiYiazZk0nJycbo9GRadMi9Sy/iIiIiMht5I5IruHycu4pU6YQFRWF0WgkKirK8rxqTk4O/v7+5OXlERoaesWmUjVr1mTChAkMGDAAk8mEt7c3fn5+17zXyZMnLcvIbZWWlonJZP7nC+W2V6tWbb7++nvL68DA/gQG9r/qumrVqjNjxpwi2/D29uGDD5betD6KiIiIiMiNMZj/vl76DhMcHEyzZs3+MWG+1ZRcizX0TJLYQnEjtlDciC0UN2ILxY1Yq7Sfub5jZq5LUlxcHAsXLizyPVvO1BYREREREZGy7Y6fub5daeZarKFvdsUWihuxheJGbKG4EVsobsRapT1zfUfsFi5yq61evZK+fXsRENCL4OBRnD+fzsWLF5g0aRy9e/vRv/8LrFoVc1W9M2dO8/TTbTl06L9Xvbdly2Z8fdvciu6LiIiIiIiVyuWy8JCQEPbs2UN+fj5JSUl4eHgAEBgYiL+//3W1ER0dDUDv3r1t6kNycjL+/v5s3brVpvpSdh06dJDo6OUsWRKNs7Mz8+fP5YMPFpCXl0fFihVZvvwzTCYT48aNplatOrRs+TgAubm5hIW9SUFB/lVtnjyZxDvvzMVsNt3q4YiIiIiIyHUol8n15MmTATh16hSBgYE2PQdta1INkJCQQHh4OCkpKTa3UdxSA7n9VarsQ0zMWhwcHMjNzSUl5Ry1a9dh69YEXn99DPb29tjb29OiRSs2b95kSa5nz36Lp5/uyrJli69oLycnh9DQNxk+/HVCQiaWxpBEREREROQflMvkuignTpxg0qRJZGRk4OTkxIQJE2jUqBHBwcEYDAYOHz5MZmYmQ4YMoXv37kRFRQEwfPhwYmNjWbBgAQaDAS8vL8LCwjAajcXea9WqVURFRdG1a1eb+ztgajznzmfbXF9KT+ysbjg4OLBly2beeisMo9GRl18eTHp6Ghs3xtGokTd5eXkkJHyLg8PlfwVjYz+noKCAZ57pcVVyHRk5jW7d/PDw8CyN4YiIiIiIyHW4Y565DgoKIiAggNjYWMaNG8fIkSPJy8sDLi/hjomJYenSpcyYMeOKGefk5GQiIiJYvHgx69evp7CwkISEhGveKyoqigcffPCmjkduf61bP8H69Zvo338Qo0YNZ+jQkRgMBl56qQ/jx7/Bo482x8HByC+/HOLzz1cTFDT+qjbWrPkMe3sHunTpVgojEBERERGR63VHzFxnZWWRlJSEr68vAN7e3ri4uHD8+HEA/Pz8MBqNuLm54ePjw+7duy119+7di4+PD25ubgBERkbe+gFImfLbb7+RkpJC06ZNAejX7wVmzozgrrsMvPnmeKpUqQLA+++/j6fn/SQkxJObm82wYS8DkJaWytSpkxgzZgxffx1HTk4OL7/cl/z8fHJzc3n55b68//771KxZ84r7urpWuqXjlPJBcSO2UNyILRQ3YgvFjVirNGPmjkiuzWYzfz9xzGw2U1hYCIC9vb2l3GQyWZbqAlf8DJCeng5AtWrVblZ3pYxLSUnhtdde56OPPqFKlSps2PAl993nwUcffUxWViajRo0lPT2NmJiVTJkyjYceasigQSMs9Xv27MrEiaE0aPAwCxY0s5SfPXuGwMDn+PDD5f//Pv87ZkBHVYgtFDdiC8WN2EJxI7ZQ3Ii1dBTXLeDs7Iy7uzvx8fEA7Nu3j9TUVDw9Lz/DumHDBsxmM6dPnyYxMZEmTZpY6np5ebF//37LUvHw8HA2bdp06wchZUbTpk0JDOzP8OGD6NevD5s2xRMRMZOAgH6kpJwjIKAXI0YMoX//QTz0UMPS7q6IiIiIiJSAO2LmGi4v554yZQpRUVEYjUaioqJwdHQELu/G7O/vT15eHqGhoVStWtVSr2bNmkyYMIEBAwZgMpnw9vbGz8/vpvd30UTfm34PuTlycgvo0aMnPXr0vOq9iIhZ/1h/1arYIstr1arN119/f8P9ExERERGRkmcw/3299B0mODiYZs2a3ZKE2RppaZmYTHf0n0asoGVTYgvFjdhCcSO2UNyILRQ3Yq3SXhZ+x8xcl6S4uDgWLlxY5Hu2nKktIiIiIiIiZdsdP3N9u9LMtVhD3+yKLRQ3YgvFjdhCcSO2UNyItUp75vqO2NBMRERERERE5GbSsnCRErR69UrWrl2NwQB16tRl7NiJVK7swpw5M9i3bw8Ajz3WkqFDR2IwGLh48QJz5kTy66/Hyc3NJTCwPx07dgYgIeE7Fi9eiMFgR6VKlQgOfpM6deqW5vBERERERKQYZTq5DgkJYc+ePeTn55OUlISHhwcAgYGB+Pv7X1cb0dHRAPTu3dumPiQnJ+Pv78/WrVstZfPnz2fDhg0AtGnThjFjxljdbnFLDeT2lJNbwI+79hAdvZwlS6JxdnZm/vy5fPDBAv7970YkJf3G0qUxmM1mBg/uz3ffbaJt23ZMmzaFe++9j8mTp3LuXDKBgc/j49MUFxcXwsLeZMmSaOrWdWflyhXMnRtJZOTbpT1UEREREREpQplOridPngzAqVOnCAwMtGkzMVuTaoCEhATCw8MtZ2ADbN++na1bt7J27VoMBgMvv/wyX3/9Ne3bt7eq7QFT4zl3PtvmvsmtFTurGw0aPERMzFocHBzIzc0lJeUctWvXwWQqJDs7m/z8fEwmE/n5+Tg6OnLx4gV+/HEXISERANSoUZP3319C5couFBaaMJvNZGZmApCdnW05Ok5ERERERG4/ZTq5LsqJEyeYNGkSGRkZODk5MWHCBBo1akRwcDAGg4HDhw+TmZnJkCFD6N69O1FRUQAMHz6c2NhYFixYgMFgwMvLi7CwMIxGY7H3WrVqFVFRUXTt2tVS5urqSnBwsCUR8vDw4MyZMzd30HLbcHBwYMuWzbz1VhhGoyMvvzyY2rXr8O23m+je/WkKCwtp1qw5rVq15r//PUD16vcQE7OcnTu3k5eXT+/efalX714A3nhjHEOG9KdyZRdMJhMLFiwq5dGJiIiIiEhxyl1yHRQUxKBBg/D19WXfvn2MHDmSjRs3ApeXcMfExJCWloafnx8tW7a01EtOTiYiIoI1a9bg5uZGUFAQCQkJtGvXrth7/ZmY/5Wnp6fl519//ZW4uDhiYmJKcIRyu3J1rQSAv39X/P278umnnxIUNIJnnnkGNzdXFi3aTm5uLq+++iqxsZ/RuHFjzp49Tc2a1Vm16jN+++03XnjhBby8GmA0Gvn448XExcVRr149li1bxuTJwaxbtw6DwXDN+4tYQ3EjtlDciC0UN2ILxY1YqzRjplwl11lZWSQlJeHr6wuAt7c3Li4uHD9+HAA/Pz+MRiNubm74+Piwe/duS929e/fi4+ODm5sbAJGRkTfUlyNHjvDKK68wduxY/vWvf91QW1I27N37X9LS0mjc2BuA1q19mTx5MuvXxzF6dDAXLuQC0K7d02zevIkmTf7z/69rT0rKJZycqtGwYSO2b/+R7OxsHn7Yi4oVq5KScglf32eIiIjg6NFTVKlS5ap766gKsYXiRmyhuBFbKG7EFoobsZaO4ipBZrOZvx/bbTabKSwsBMDe3t5SbjKZcHD433cLf/0ZID09nfT0dJv6sXv3bvr168fo0aPp0aOHTW1I2ZOWlsqUKePJyMgAID5+A/fd50GDBg/z7bdfA1BQUMDWrVt4+OF/U7t2HR58sAEbNnwJQHp6GgcOJNKgwUPUr9+Affv2kJ6eBsD332+mVq3aRSbWIiIiIiJS+srVzLWzszPu7u7Ex8dbloWnpqZalmpv2LCBjh07cubMGRITE5k2bRoHDx4EwMvLi5CQEFJSUnB1dSU8PJzmzZvz7LPPWtWHs2fPMnToUObMmUOLFi1sHsuiib4215VbLye3gMaNHyEwsD/Dhw/C3t6Be+65h4iImdx9993MmRNJnz7+2NnZ07Tpo/Tt2w+A8PCZzJ79Fp9/vgaz2US/fi/z0EMNAejdO4Dhw1/BwcFI5cqViYiYVYojFBERERGRaylXyTVcXs49ZcoUoqKiMBqNREVFWTYXy8nJwd/fn7y8PEJDQ6lataqlXs2aNZkwYQIDBgzAZDLh7e2Nn5+f1fdftGgRubm5TJ8+3VL2/PPPW70reVpaJiaT+Z8vlNtKjx496dGj51XlU6ZMK/J6Nzc3ZsyYU+R7/v698PfvVaL9ExERERGRm8Ng/vs66nIqODiYZs2a2ZQwlwYl12INPZMktlDciC0UN2ILxY3YQnEj1irtZ67L3cx1SYqLi2PhwoVFvmfLmdoiIiIiIiJSPt0xM9dljWauxRr6ZldsobgRWyhuxBaKG7GF4kaspZlrkVK0evVK1q5djcEAderUZezYiVStWg2A5OTfeeWVl1iyJNqyS/fWrVuYNm0KNWu6Wdp4990PcHK6m1WrYli9+lMqVLiLe+/9F6NHj6VyZZfSGJaIiIiIiNxi5eoorqIcPnyY+vXrs3Hjxn+8duXKlXz55eVjkYKDg1mzZs3N7p6UokOHDhIdvZz33lvMxx9/St269fjggwUAbNjwJUOHDiQ1NeWKOgcOJNK7d1+WLPnE8o+T093s2fMTK1Ys4+23F7BkySe0aNGSGTOK3sRMRERERETKn3I/c71mzRo6dOhATEwMHTp0uOa1e/fupVmzZreoZ9dW3FIDKRk5uQU0aPAQMTFrcXBwIDc3l5SUc9SuXYfU1BS+/z6ByMi3CQi4crfuAwcSsbd3YPPmb7nrrrsYNOhVvL19OHToIE2bNqNGjZoAtGnTlrfemkp+fj5Go7E0higiIiIiIrdQuU6uCwoK+OKLL1ixYgXPP/88SUlJ1KtXj7Zt27Js2TLq1q3Lzp07mT9/PkOGDOHbb7/lhx9+wNXVFYDNmzfzySefkJaWxuDBg3nuuefIzs5m4sSJ/PLLLxgMBgYMGED37t1Zs2YNa9euJSMjgyeffBJPT08+/PBD7O3tqVu3LpGRkVSoUOG6+z5gajznzmffrF/NHS92VjcuAQ4ODmzZspm33grDaHTk5ZcHc889roSHRxZZr3JlFzp06ESbNk+yf/8+xo0bzZIln/Dwww1ZtSqG338/i5tbLeLiviA/P58LFy5wzz333NrBiYiIiIjILVeuk+vNmzdTu3Zt7rvvPtq1a0dMTAxjxowp8tr//Oc/tG3blmbNmvH444+zfv168vLy+Oyzzzhy5AiBgYE899xzREVFUbVqVb788kvS09N59tlnadCgAQDJycnExcXh4ODAU089xaeffkr16tWZM2cOx48f56GHHrqVw5fr1Lr1E7Ru/QRffLGWUaOGs3LlWuzsin5i4q9Jd+PG3vz734348ceddO78DP37D2T8+DcwGOzo3PkZKld2wWgs1/+KiYiIiIjI/1eu/89/zZo1dOnSBYBOnTrxxhtv8Nprr113/aeeegqDwYCnpyfnz58H4IcffiA8PByAatWq8dRTT7Fr1y6cnZ15+OGHcXC4/Ct98skn6d27N0899RQdOnRQYn0b+uOPdFJSUmjatCkA/fq9wMyZETg6mqha9X8bkVWvfjfVqlXi4sWLfPLJJ7zyyisYDAYAHB3tqVrVmYoVDbRt25qXXgoAIDU1lcWLF+LhUddy7c3m6lrpltxHyhfFjdhCcSO2UNyILRQ3Yq3SjJlym1ynpaWxZcsWDhw4wLJlyzCbzVy8eJH4+HgA/jyBrKCgoNg27O3tAa5Ijv5+cpnZbKawsBCAu+66y1I+ceJEDh06REJCAkFBQQwbNoxu3bqVzOCkRBw58htTpkzgo48+oUqVKmzY8CX33edBQYHDFVv4p6VlUVhopLDQxMcfL6d6dTeeeOIpDh8+xL59+3njjYn88ssJRo58leXLP+Xuu52ZPXsubdu2JzU185aMRUdViC0UN2ILxY3YQnEjtlDciLV0FNdN8sUXX/DYY4/x4YcfWsqioqJYuXIlVatW5ejRo7i7u7Np0ybL+/b29pZEuTiPPfYYq1atYuLEiaSnp7Np0yaioqL45ZdfLNcUFBTQqVMnPv74Y1555RXy8/M5ePCgkuvbTOPGjxAY2J/hwwdhb+/APffcQ0TEzGKvt7e3Z/r0WcyZE8miRQuxt3cgNDSCKlWqUKVKFfr2fZFBg/phMplo1MibUaOKfgRBRERERETKn3KbXK9Zs4bXX3/9irI+ffrw4YcfMmbMGKZNm8b8+fNp1aqV5f3//Oc/zJ49m0qVil9KMHToUKZMmULXrl0pLCxk8ODBNGzY8Irk2sHBgREjRvDSSy9x1113UblyZd566y2r+r9ooq9V14t1cnIvr1jo0aMnPXr0LPa6rVt/uuJ1gwYPs3DhR0Ve6+//HP7+z5VcJ0VEREREpMwwmP++zlluC2lpmZhM+tPI9dGyKbGF4kZsobgRWyhuxBaKG7FWaS8LL3pLZBERERERERG5bkquRURERERERG6QkmsRERERERGRG1RuNzSTK23Y8CUrV35ieZ2Vlcm5c8msXPk5ixe/z6FD/8VkMvPwww0ZPXosFSrcRW5uDu+88zY//7yf7OwcnnmmO336BJbiKERERERERG5PSq6LcOrUKTp27IiHhwcGg4H8/Hxq1KhBREQEbm5uluuSk5OZOHEiH3zwQbFtJSYmsnHjRoKCgqzqQ3EPydsiJ7eAp5/uwtNPdwEuHxU2dOhAXnjhRb74Yi2FhYUsWRKN2WwmNPRNPv54CS+/PJgFC6K4ePEiH374MdnZ2fTr15tGjR7h3//2KrG+iYiIiIiIlAdKrotRo0YN1q1bZ3k9a9YswsLCeOeddyxlNWvWvGZiDXD06FHS0tKsvv+AqfGcO59tdb2ixM7qxl/3zFu+fAlVq1ale3d/du36ATe3WtjZXX5C4MEH63PixHHMZjNffRXHhx8uw97eHmdnZ+bNe49KlSqXSJ9ERERERETKEz1zfZ2aNm3Kr7/+Stu2bXnttdfo0KEDiYmJtG3bFoDg4GCmTp1K7969adu2LatXr+bixYvMmzePb7/9lgULFpTyCC7LyMggJmYFI0aMBqBZs8eoV+9eAH7//SyffhrNk0+2IyPjPNnZf/DTTzsZNmwQ/fr1YevWLdc8A1xEREREROROpZnr65Cfn8+GDRvw8fFh27ZttG7dmrlz53Lq1Kkrrvv999/55JNPOHz4MIGBgfj7+zNixAh27drFkCFDSqn3l7m6Xk6KV69eQfv27WjcuMEV7x84cIDhw4cRGBhA9+6dSE5OprCwkPT0c0RHryA9PZ2AgADq17+fdu3alcYQ5B/8+TcWsYbiRmyhuBFbKG7EFoobsVZpxoyS62KcO3eObt26AZCXl0ejRo0YPXo027Zto3HjxkXWadmyJQaDgQcffJCMjIxb2Nt/9udh6l988SWvvfbGFYerf/PNRmbNeovXXx+Dr29HUlIuUVhoxMHBgdat25OWlgVUoHnzlmzbtpPGjZuX0iikOK6ula74m4pcD8WN2EJxI7ZQ3IgtFDdirVsRM3Z2hmL3x1JyXYy/P3P9VxUqVLhmucFguGn9uhEXL17k9OmTeHn978uB7777hrlzZzJnznwaNHjYUm40GmnZ8nG++mo9w4a9xh9//MGPP+7kxRf7l0bXRUREREREbmtKrm8ye3t7CgoKrK63aKJvifUhJ/fy/U+fPkn16vfg4PC/P/vChe8AZqZPn2op8/JqzOjRYxk7diJvvz2Tvn2fpbCwkPbtO/Lkk1oSLiIiIiIi8ndKrm+yRo0aMX/+fGbOnMkbb7xx3fXS0jIxmcwl2peHHmrIypWfX1EWE7O22OsrV3bhzTfDSrQPIiIiIiIi5ZHBbDaXbAYnJeJmJNdSfumZJLGF4kZsobgRWyhuxBaKG7FWaT9zraO4RERERERERG6QkmsRERERERGRG6Rnrsu41atXsnbtagwGqFOnLmPHTqRq1Wp06dKOe+6pYbmuT58AfH2f5ujRI8yaFUF2dg4GAwwaNJQWLVqW4ghERERERETKPiXXfxMSEsKePXvIz88nKSkJDw8PAAIDA/H39y/l3l3p0KGDREcvZ8mSaJydnZk/fy4ffLCA559/AWfnyixZ8slVdcLC3mTAgMG0bv0Ex48f5ZVX+hMXtwmj0VgKIxARERERESkflFz/zeTJkwE4deoUgYGBxZ51fbMV95D8X1Wq7ENMzFocHBzIzc0lJeUctWvX4eefE7G3t2P48Fe4ePECTzzxFIGB/bG3t2fRouXY29sDcPr0KSpVqoSdnZ4OEBERERERuRFKrq/D3LlzMZlMjBo1CoBx48bx+OOPs2XLFgwGA4cPHyYzM5MhQ4bQvXt3srKyCA0N5ciRIxQWFjJw4EC6dOli1T0HTI3n3Pnsa14TO6sbDg4ObNmymbfeCsNodOTllwezd+9uHn20Oa++OpLc3FzGjBnJ3XffTa9efXBwcMBsNtOrVzd+//0sI0eOtiTbIiIiIiIiYhtNWV4Hf39/vvzyS8xmM3/88Qc7duygXbt2ACQnJxMTE8PSpUuZMWMGKSkpLFiwgIYNG7JmzRpWrFjBe++9x8mTJ29a/1q3foL16zfRv/8gRo0aTpcu3XjttSAcHR2pVKkSzz33Alu2bLZcbzAY+PTTdcTErGX58qXs3v3jTeubiIiIiIjInUAz19fB3d2dOnXq8OOPP3LmzBnatGmDo6MjAH5+fhiNRtzc3PDx8WH37t1s376dnJwcVq9eDcAff/zBkSNHcHd3L9F+/fbbb6SkpNC0aVMA+vV7gZkzI9i+/VseeughGjRoAEClSndRsWIFXFwq8PXXX/P0009jZ2eHq2sDWrVqyZkzv9KxY9sS7Zvceq6ulUq7C1IGKW7EFoobsYXiRmyhuBFrlWbMKLm+Tn/OXp85c4bhw4dbyv+6pNpkMuHg4IDJZCIyMpKGDRsCkJqaiouLS4n3KSUlhddee52PPvqEKlWqsGHDl9x3nweJif9l/foNTJ06g4KCfD76aCm+vk9z4UIus2bNJiPjD3x9O5KamsL27Tvo3LnHTT9sXW4uV9dK+huK1RQ3YgvFjdhCcSO2UNyItW5FzNjZGYrdH0vLwq9Tx44d2bFjB6mpqTRu3NhSvmHDBsxmM6dPnyYxMZEmTZrw2GOPER0dDcC5c+d45plnOHv2bIn3qWnTpgQG9mf48EH069eHTZviiYiYSf/+g6hUqTIvvvg8L77YGy+vxnTt2h2A8PCZrFu3mn79+jBmzGu8+upIGjR4uMT7JiIiIiIicifRzPV1uuuuu/D29ubBBx+8ojwnJwd/f3/y8vIIDQ2latWqDBs2jClTptClSxcKCwsJCgqiXr16Vt1v0UTff7wmJ7eAHj160qNHz6veGz9+cpF1PDwe4J13PrCqLyIiIiIiInJtSq6LUbduXb799lsAzGYzWVlZ/Pe//2XMmDFXXNexY0f8/PyuKHN2dmbmzJk3dP+0tExMJvMNtSEiIiIiIiK3hpaFX4eff/6Ztm3b0qtXL1xdXUu7OyIiIiIiInKb0cz1dWjUqBG7du26qnz69Oml0BsRERERERG53WjmWkREREREROQGKbkWERERERERuUFKrkVERERERERukJJrERERERERkRukDc1uU3Z2htLugpQxihmxheJGbKG4EVsobsQWihux1s2OmWu1bzCbzTpMWUREREREROQGaFm4iIiIiIiIyA1Sci0iIiIiIiJyg5Rci4iIiIiIiNwgJdciIiIiIiIiN0jJtYiIiIiIiMgNUnItIiIiIiIicoOUXIuIiIiIiIjcICXXIiIiIiIiIjdIybWIiIiIiIjIDVJyfRuJjY2lU6dOtG/fnhUrVpR2d+Q2EBgYSOfOnenWrRvdunVj//79xcbJ9u3b6dq1K76+vsyZM8dSfvDgQfz9/enQoQMTJkygoKCgNIYiN1lmZiZdunTh1KlTgPXxcObMGV544QU6duzIkCFDyMrKAuDixYsMGjSIp59+mhdeeIGUlJRbPzi5af4eN+PGjcPX19fymfP1118DJRdPUvbNnz+fzp0707lzZ2bMmAHo80b+WVFxo88b+Sdvv/02nTp1onPnznz00UdAGfi8Mctt4ffffzc/+eST5vPnz5uzsrLMXbt2NR85cqS0uyWlyGQymVu2bGnOz8+3lBUXJ9nZ2eY2bdqYk5KSzPn5+eb+/fubN2/ebDabzebOnTub9+7dazabzeZx48aZV6xYURrDkZto37595i5dupgbNmxoPnnypE3xMGjQIPOXX35pNpvN5vnz55tnzJhhNpvN5pCQEPPChQvNZrPZvHbtWvPIkSNv7eDkpvl73JjNZnOXLl3MycnJV1xXkvEkZdu2bdvMzz33nDk3N9ecl5dnDgwMNMfGxurzRq6pqLiJj4/X541c086dO83PP/+8OT8/35ydnW1+8sknzQcPHrztP280c32b2L59O4899hhVqlTBycmJDh068NVXX5V2t6QUHT9+HIPBwMCBA3nmmWdYvnx5sXGSmJjIvffei7u7Ow4ODnTt2pWvvvqK06dPk5OTg7e3NwB+fn6Kq3Lo008/ZfLkydSoUQPA6njIz8/nxx9/pEOHDleUA2zevJmuXbsC0KVLF7Zs2UJ+fv6tH6SUuL/HzR9//MGZM2d488036dq1K/PmzcNkMpVoPEnZ5urqSnBwMI6OjhiNRjw8PPj111/1eSPXVFTcnDlzRp83ck3NmjVj2bJlODg4kJaWRmFhIRcvXrztP28cbrgFKRHnzp3D1dXV8rpGjRokJiaWYo+ktF28eJEWLVowZcoUcnJyCAwM5Omnny4yToqKn+Tk5KvKXV1dSU5OvqXjkJtv2rRpV7y2Nh7Onz+Ps7MzDg4OV5T/vS0HBwecnZ1JT0+nZs2aN3tYcpP9PW7S0tJ47LHHCA0NxcnJiVdeeYVVq1bh5ORUYvEkZZunp6fl519//ZW4uDgCAgL0eSPXVFTcfPLJJ+zatUufN3JNRqORefPmsXjxYjp27Fgm/v9GM9e3CbPZfFWZwWAohZ7I7eKRRx5hxowZODk5Ua1aNXr27Mm8efOuus5gMBQbP4qrO5O18WBtnNjZ6T8d5ZG7uzvvvPMO1atXp2LFigQEBJCQkHDT40nKniNHjtC/f3/Gjh1LvXr1rnpfnzdSlL/Gzf3336/PG7kuI0aMYMeOHZw9e5Zff/31qvdvt88bfWLdJmrWrElqaqrl9blz5yxL9eTO9NNPP7Fjxw7La7PZTJ06dYqMk+Li5+/lKSkpiqs7gLXxUK1aNTIzMyksLLyiHC5/K/xnnYKCAjIzM6lSpcqtG4zcMr/88gsbN260vDabzTg4OJRoPEnZt3v3bvr168fo0aPp0aOHPm/kuvw9bvR5I//k2LFjHDx4EICKFSvi6+vLzp07b/vPGyXXt4n//Oc/7Nixg/T0dLKzs4mPj6d169al3S0pRZcuXWLGjBnk5uaSmZnJ2rVriYyMLDJOGjduzIkTJ/jtt98oLCzkyy+/pHXr1tSpU4cKFSqwe/duAD7//HPF1R3A2ngwGo00bdqUuLi4K8oB2rRpw+effw5AXFwcTZs2xWg0lsq45OYym82Eh4dz4cIF8vPzWblyJe3bty/ReJKy7ezZswwdOpSZM2fSuXNnQJ838s+Kiht93sg/OXXqFBMnTiQvL4+8vDw2bdrE888/f9t/3hjMRc2XS6mIjY1l4cKF5Ofn07NnTwYOHFjaXZJSNnfuXDZu3IjJZKJPnz68+OKLxcbJjh07iIiIIDc3lzZt2jBu3DgMBgOHDh1i4sSJZGVl8fDDDxMREYGjo2Mpj0xuhrZt27Js2TLq1q1rdTycPn2a4OBg0tLSqFWrFrNnz8bFxYWMjAyCg4M5efIklSpVYubMmdStW7e0hyol6K9xs2LFClasWEFBQQG+vr688cYbgPWfL8XFk5RtU6dOZfXq1VcsBX/++ef517/+pc8bKVZxcWMymfR5I9c0b948vvrqK+zt7fH19WX48OG3/f/fKLkWERERERERuUFaFi4iIiIiIiJyg5Rci4iIiIiIiNwgJdciIiIiIiIiN0jJtYiIiIiIiMgNUnItIiIiIiIicoOUXIuIiNyG6tevT9euXenWrZvlnwkTJtjcXmJiIpMmTSrBHl5p06ZNTJ069aa1X5yTJ08yfPjwW35fERGRv3Mo7Q6IiIhI0ZYuXUq1atVKpK2jR4+SnJxcIm0V5amnnuKpp566ae0X58yZM5w4ceKW31dEROTvdM61iIjIbah+/frs2LGjyOT62LFjTJs2jYyMDAoLCwkICKBnz56YTCbCw8PZv38/WVlZmM1mpk6dSu3atenduzeXLl3C19eX7t27ExYWxpdffgnAzp07La+joqLYt28f586do379+sycOZMFCxYQHx+PyWSiTp06TJ48mZo1a17RpzVr1rBx40YWLlxIQEAADRs25IcffiAtLY3AwEDS0tLYtWsX2dnZzJ07l/r16xMQEICHhwcHDhzg/PnzdOvWjREjRgDwzTffMH/+fAoLC3F2dmbcuHE0atToiv55enry888/k5yczKOPPsqiRYt47733+Oabb8jNzSU7O5uxY8fSvn17oqKiOH36NCkpKZw+fZpq1aoxZ84catasyYkTJ5g0aRLp6enY2dkxZMgQOnXqRHJyMqGhoZw9e5b8/Hw6d+7M4MGDb/4fX0REyiTNXIuIiNymXnzxRezs/vcE1+LFi3FxcWHEiBHMmDGDhg0bcunSJZ577jkeeOABzGYz586dY+XKldjZ2fH+++/zwQcf8N577zFixAg2btxIREQEO3fuvOZ9T58+zZdffomDgwOff/45hw8f5rPPPsPBwYGVK1cyceJEPvjgg39s4/PPP2f//v306tWLBQsWEBwcTHh4OMuXLycsLAy4PPMcHR1NdnY2vXr1wsvLi3r16jF58mRiYmJwd3dnx44dvPrqq3z11VdX9e/PLwYWLVrE6dOn2b59O8uXL+euu+5i/fr1zJs3j/bt2wPw008/8fnnn+Ps7MzgwYNZuXIlI0aMYNSoUfTs2ZMXXniBs2fPEhAQQOvWrQkKCqJfv360bduW3NxcBg4cSL169ejUqdON/FlFRKScUnItIiJymypqWfjRo0dJSkpi/PjxlrKcnBz++9//0qdPH1xcXIiJieHkyZPs3LmTu+++2+r7ent74+Bw+X8RvvvuO37++Wf8/f0BMJlMZGdn/2Mbfya07u7uADz++OMA1KtXj127dlmue+655zAajRiNRjp27MjWrVu5//77eeyxxyx1W7RoQbVq1Thw4MBV/furOnXq8NZbbxEbG8tvv/1mmcH/U7NmzXB2dgbg4Ycf5sKFC2RkZHDo0CGeffZZAGrVqsU333zDH3/8wY8//siFCxd4++23Afjjjz84dOiQkmsRESmSkmsREZEypLCwkMqVK7Nu3TpLWWpqKpUqVWLz5s1MmzaNl156iaeeeor777+fL7744qo2DAYDf30qLD8//4r3nZycLD+bTCZefvll+vTpA0BeXh4XLlz4x346Ojpe8dpoNBZ53V+TZLPZjJ2dHUU9sWY2mykoKLiqf3/1f//3f7z66qv069ePli1b8uijjxISEmJ5/6677rL8/Ofv4M/7GwwGy3vHjx/H1dUVs9lMTEwMFStWBCA9PZ0KFSpcc9wiInLn0m7hIiIiZch9991HhQoVLMn12bNn6dKlCwcOHGDbtm08+eST9OnTBy8vL7755hsKCwsBsLe3tySn1apV48yZM6SlpWE2m/nmm2+KvV+rVq1YtWoVmZmZALz99tuMGTOmxMbzxRdfYDKZuHDhAhs2bKBt27Y89thjbNu2jZMnTwKwY8cOzp49S+PGja+qb29vb/ly4Mcff+Tf//43L730Es2aNWPTpk2W8RfH2dmZhg0b8vnnnwOXf5+9e/cmJycHb29vPvroIwAuXrxI79692bRpU4mNXUREyhfNXIuIiJQhjo6OvPvuu0ybNo0PP/yQgoICRo4cSZMmTahSpQpvvPEGXbt2xd7enqZNm1o2InvkkUeYO3cuQ4cO5Z133uH555/H398fV1dXnnjiiWLv9+yzz5KcnEyvXr0wGAzUqlWL6dOnl9h4cnJy6NmzJ1lZWfTp04cWLVoAMHnyZIYNG0ZhYSF33XUX7733HpUqVbqqvqenJ/b29vTs2ZP33nuP+Ph4OnXqhNFopEWLFly4cMHyxUBxZs2aRUhICB9//DEGg4Fp06bh6urKzJkzCQsLo2vXruTl5dGlSxeeeeaZEhu7iIiUL9otXEREREpFQEAAL7zwAh07diztroiIiNwwLQsXERERERERuUGauRYRERERERG5QZq5FhEREREREblBSq5FREREREREbpCSaxEREREREZEbpORaRERERERE5AYpuRYRERERERG5QUquRURERERERG7Q/wMa0VMxWu9XpwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1130.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(font_scale = 1)\n",
    "lightgbm.plot_importance(model, height=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving-exporting the results to a .csv file\n",
    "pd.DataFrame(predictions, columns = ['Price']).to_csv('submission_lgbm_no_authors_removed.csv', index=False)\n",
    "# 0.75788"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling without the Type: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_df[data_df['Set'] == 'train']\n",
    "y_train = X_train['Price']\n",
    "X_train =  X_train.drop(['Set', 'Price', 'Type'], axis=1)\n",
    "\n",
    "X_test = data_df[data_df['Set'] == 'test']\n",
    "#y_test = X_test['Price']\n",
    "X_test =  X_test.drop(['Set', 'Price', 'Type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in X_train.select_dtypes(include=['object']):\n",
    "    X_test[col_name] = X_test[col_name].astype('category')\n",
    "    X_train[col_name] = X_train[col_name].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 54 candidates, totalling 540 fits\n",
      "[CV 5/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.737 total time=  30.9s\n",
      "[CV 6/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.767 total time=  34.3s\n",
      "[CV 10/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.729 total time=  37.1s\n",
      "[CV 8/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.738 total time=  32.5s\n",
      "[CV 6/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.764 total time=  37.1s\n",
      "[CV 4/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time=  48.7s\n",
      "[CV 2/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.761 total time=  53.2s\n",
      "[CV 10/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.731 total time=  48.0s\n",
      "[CV 8/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.736 total time=  57.6s\n",
      "[CV 6/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.768 total time= 1.5min\n",
      "[CV 4/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.742 total time= 1.4min\n",
      "[CV 2/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.759 total time=  30.3s\n",
      "[CV 8/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.739 total time=  35.8s\n",
      "[CV 6/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.767 total time= 1.1min\n",
      "[CV 4/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.740 total time= 1.2min\n",
      "[CV 2/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.759 total time=  51.4s\n",
      "[CV 9/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  40.4s\n",
      "[CV 7/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.751 total time= 1.2min\n",
      "[CV 5/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.738 total time= 1.3min\n",
      "[CV 3/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.756 total time= 1.1min\n",
      "[CV 8/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.740 total time= 1.2min\n",
      "[CV 8/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.8min\n",
      "[CV 5/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.738 total time= 2.1min\n",
      "[CV 4/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.740 total time=  35.0s\n",
      "[CV 8/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.738 total time=  30.1s\n",
      "[CV 4/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.746 total time=  34.8s\n",
      "[CV 2/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.759 total time=  35.3s\n",
      "[CV 9/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.740 total time=  37.2s\n",
      "[CV 7/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.749 total time=  40.3s\n",
      "[CV 5/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.741 total time=  47.1s\n",
      "[CV 3/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.757 total time=  51.7s\n",
      "[CV 1/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.753 total time=  30.1s\n",
      "[CV 1/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.756 total time=  37.1s\n",
      "[CV 3/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.756 total time=  36.4s\n",
      "[CV 2/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.761 total time=  38.7s\n",
      "[CV 10/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.729 total time=  37.1s\n",
      "[CV 8/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.736 total time=  49.3s\n",
      "[CV 6/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.767 total time=  50.7s\n",
      "[CV 4/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  58.4s\n",
      "[CV 1/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.3min\n",
      "[CV 10/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.731 total time= 1.4min\n",
      "[CV 8/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.738 total time= 1.3min\n",
      "[CV 6/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.766 total time=  32.8s\n",
      "[CV 3/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.1min\n",
      "[CV 1/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.758 total time= 1.1min\n",
      "[CV 9/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.742 total time= 1.2min\n",
      "[CV 8/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.740 total time=  40.9s\n",
      "[CV 6/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.768 total time= 1.2min\n",
      "[CV 4/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.741 total time= 1.3min\n",
      "[CV 2/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.759 total time= 1.1min\n",
      "[CV 7/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.751 total time= 1.1min\n",
      "[CV 5/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.8min\n",
      "[CV 3/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.756 total time= 2.1min\n",
      "[CV 1/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.754 total time=  33.5s\n",
      "[CV 5/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.736 total time=  32.1s\n",
      "[CV 1/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.753 total time=  36.4s\n",
      "[CV 9/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.741 total time=  34.6s\n",
      "[CV 7/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.749 total time=  39.3s\n",
      "[CV 6/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.767 total time=  40.2s\n",
      "[CV 4/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.746 total time=  45.6s\n",
      "[CV 1/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.756 total time=  47.1s\n",
      "[CV 9/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.740 total time=  49.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.741 total time=  29.8s\n",
      "[CV 10/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.728 total time=  29.7s\n",
      "[CV 8/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.736 total time=  34.9s\n",
      "[CV 6/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.767 total time=  34.9s\n",
      "[CV 4/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  36.6s\n",
      "[CV 2/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.761 total time=  46.2s\n",
      "[CV 9/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time=  51.6s\n",
      "[CV 7/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.748 total time=  45.0s\n",
      "[CV 5/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.737 total time=  57.7s\n",
      "[CV 3/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.754 total time= 1.4min\n",
      "[CV 1/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.755 total time= 1.4min\n",
      "[CV 9/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.745 total time= 1.1min\n",
      "[CV 2/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.760 total time= 1.1min\n",
      "[CV 9/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.0min\n",
      "[CV 7/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.750 total time= 1.2min\n",
      "[CV 5/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.740 total time=  42.3s\n",
      "[CV 3/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.2min\n",
      "[CV 1/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.757 total time= 1.3min\n",
      "[CV 9/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.742 total time= 1.3min\n",
      "[CV 10/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.732 total time= 1.1min\n",
      "[CV 7/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.751 total time= 1.8min\n",
      "[CV 4/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.741 total time= 2.1min\n",
      "[CV 2/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.759 total time=  33.6s\n",
      "[CV 6/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.767 total time=  31.0s\n",
      "[CV 2/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.759 total time=  34.9s\n",
      "[CV 10/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.732 total time=  35.3s\n",
      "[CV 8/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.737 total time=  35.7s\n",
      "[CV 5/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.737 total time=  41.2s\n",
      "[CV 3/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time=  46.7s\n",
      "[CV 2/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.758 total time=  48.3s\n",
      "[CV 10/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.730 total time=  50.8s\n",
      "[CV 8/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.736 total time=  30.2s\n",
      "[CV 5/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.736 total time=  34.4s\n",
      "[CV 9/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.743 total time=  35.6s\n",
      "[CV 7/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.749 total time=  33.2s\n",
      "[CV 5/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.737 total time=  37.4s\n",
      "[CV 3/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.754 total time=  48.5s\n",
      "[CV 1/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.755 total time=  51.1s\n",
      "[CV 9/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.745 total time=  45.8s\n",
      "[CV 6/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.764 total time=  58.5s\n",
      "[CV 4/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.5min\n",
      "[CV 2/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.760 total time= 1.4min\n",
      "[CV 10/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.732 total time= 1.2min\n",
      "[CV 4/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.1min\n",
      "[CV 3/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.757 total time= 1.2min\n",
      "[CV 1/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.756 total time=  52.6s\n",
      "[CV 7/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.751 total time=  41.0s\n",
      "[CV 5/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.2min\n",
      "[CV 3/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.756 total time= 1.3min\n",
      "[CV 1/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.757 total time= 1.1min\n",
      "[CV 9/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.742 total time= 1.1min\n",
      "[CV 6/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.768 total time= 1.9min\n",
      "[CV 6/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.771 total time= 2.1min\n",
      "[CV 3/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.757 total time=  33.9s\n",
      "[CV 7/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.749 total time=  31.8s\n",
      "[CV 3/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.756 total time=  34.8s\n",
      "[CV 1/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.756 total time=  37.8s\n",
      "[CV 10/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.730 total time=  37.4s\n",
      "[CV 8/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.738 total time=  39.3s\n",
      "[CV 6/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.764 total time=  48.0s\n",
      "[CV 4/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.743 total time=  49.4s\n",
      "[CV 1/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.755 total time=  59.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.761 total time=  30.1s\n",
      "[CV 3/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.755 total time=  37.2s\n",
      "[CV 4/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.743 total time=  36.1s\n",
      "[CV 1/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.753 total time=  36.1s\n",
      "[CV 9/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  37.6s\n",
      "[CV 7/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.749 total time=  48.0s\n",
      "[CV 3/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.757 total time=  52.2s\n",
      "[CV 1/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.753 total time=  58.6s\n",
      "[CV 9/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  56.9s\n",
      "[CV 7/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.4min\n",
      "[CV 5/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.737 total time= 1.4min\n",
      "[CV 3/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.755 total time=  30.7s\n",
      "[CV 9/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.743 total time=  37.3s\n",
      "[CV 7/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.1min\n",
      "[CV 5/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.738 total time= 1.2min\n",
      "[CV 3/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.756 total time=  49.3s\n",
      "[CV 10/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.732 total time=  42.2s\n",
      "[CV 8/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.2min\n",
      "[CV 6/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.771 total time= 1.3min\n",
      "[CV 4/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.743 total time= 1.1min\n",
      "[CV 2/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.760 total time= 1.9min\n",
      "[CV 10/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.8min\n",
      "[CV 7/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.750 total time= 2.2min\n",
      "[CV 10/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.730 total time=  31.2s\n",
      "[CV 6/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.764 total time=  36.8s\n",
      "[CV 4/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.742 total time=  38.5s\n",
      "[CV 3/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.757 total time=  38.7s\n",
      "[CV 1/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.753 total time=  48.8s\n",
      "[CV 9/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.741 total time=  48.6s\n",
      "[CV 7/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.749 total time=  48.1s\n",
      "[CV 5/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.737 total time= 1.0min\n",
      "[CV 7/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.748 total time=  28.8s\n",
      "[CV 9/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.741 total time=  28.7s\n",
      "[CV 7/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.749 total time=  36.3s\n",
      "[CV 5/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.737 total time=  35.2s\n",
      "[CV 3/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.755 total time=  36.1s\n",
      "[CV 1/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.756 total time=  48.8s\n",
      "[CV 10/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.730 total time=  53.8s\n",
      "[CV 8/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.738 total time=  49.6s\n",
      "[CV 7/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.749 total time=  59.0s\n",
      "[CV 5/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.6min\n",
      "[CV 3/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.757 total time= 1.4min\n",
      "[CV 1/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.756 total time=  30.1s\n",
      "[CV 7/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.751 total time=  34.9s\n",
      "[CV 5/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.1min\n",
      "[CV 2/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.761 total time= 1.2min\n",
      "[CV 10/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.731 total time= 1.1min\n",
      "[CV 1/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.2min\n",
      "[CV 9/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.2min\n",
      "[CV 7/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.750 total time= 1.3min\n",
      "[CV 5/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.740 total time= 1.1min\n",
      "[CV 3/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.9min\n",
      "[CV 1/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.758 total time= 2.1min\n",
      "[CV 9/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.742 total time= 2.0min\n",
      "[CV 7/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.748 total time=  33.7s\n",
      "[CV 5/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.737 total time=  36.5s\n",
      "[CV 1/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.754 total time=  38.2s\n",
      "[CV 9/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  40.0s\n",
      "[CV 7/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.748 total time=  45.6s\n",
      "[CV 5/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.736 total time=  46.4s\n",
      "[CV 3/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.757 total time= 1.0min\n",
      "[CV 1/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.753 total time= 1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.754 total time=  30.3s\n",
      "[CV 4/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.743 total time=  36.5s\n",
      "[CV 2/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.761 total time=  36.3s\n",
      "[CV 9/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.744 total time=  33.6s\n",
      "[CV 7/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.749 total time=  38.3s\n",
      "[CV 6/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.769 total time=  50.5s\n",
      "[CV 5/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.737 total time=  53.3s\n",
      "[CV 3/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.755 total time=  58.9s\n",
      "[CV 2/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.760 total time= 1.2min\n",
      "[CV 9/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.3min\n",
      "[CV 7/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.749 total time= 1.3min\n",
      "[CV 5/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.739 total time=  31.9s\n",
      "[CV 1/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.1min\n",
      "[CV 10/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.732 total time= 1.1min\n",
      "[CV 8/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.738 total time= 1.2min\n",
      "[CV 6/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.766 total time=  43.5s\n",
      "[CV 4/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.2min\n",
      "[CV 2/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.761 total time= 1.3min\n",
      "[CV 10/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.731 total time= 1.2min\n",
      "[CV 1/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.759 total time= 1.9min\n",
      "[CV 9/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.9min\n",
      "[CV 8/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.739 total time= 2.1min\n",
      "[CV 9/10; 19/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 19/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.742 total time=  29.9s\n",
      "[CV 5/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.741 total time=  37.3s\n",
      "[CV 3/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.756 total time=  40.2s\n",
      "[CV 2/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.759 total time=  38.7s\n",
      "[CV 10/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.730 total time=  40.3s\n",
      "[CV 8/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.738 total time=  45.9s\n",
      "[CV 6/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.768 total time=  46.2s\n",
      "[CV 4/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.741 total time= 1.0min\n",
      "[CV 2/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.759 total time= 1.2min\n",
      "[CV 6/10; 1/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 1/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.766 total time=  30.0s\n",
      "[CV 2/10; 2/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 2/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.761 total time=  36.3s\n",
      "[CV 1/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.754 total time=  36.7s\n",
      "[CV 10/10; 3/54] START learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 3/54] END learning_rate=0.02, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.731 total time=  34.1s\n",
      "[CV 8/10; 4/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 4/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.736 total time=  37.6s\n",
      "[CV 5/10; 5/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 5/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.738 total time=  50.5s\n",
      "[CV 4/10; 6/54] START learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 6/54] END learning_rate=0.02, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.743 total time=  53.2s\n",
      "[CV 2/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.761 total time=  58.7s\n",
      "[CV 10/10; 7/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 7/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.729 total time=  57.7s\n",
      "[CV 8/10; 8/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 8/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.736 total time= 1.5min\n",
      "[CV 6/10; 9/54] START learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 9/54] END learning_rate=0.02, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.767 total time= 1.3min\n",
      "[CV 4/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.743 total time=  31.0s\n",
      "[CV 10/10; 10/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 10/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.732 total time=  38.1s\n",
      "[CV 8/10; 11/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 11/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.1min\n",
      "[CV 6/10; 12/54] START learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 12/54] END learning_rate=0.02, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.770 total time= 1.2min\n",
      "[CV 4/10; 13/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 13/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  46.5s\n",
      "[CV 2/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.760 total time= 1.2min\n",
      "[CV 10/10; 14/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 14/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.732 total time= 1.2min\n",
      "[CV 8/10; 15/54] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 15/54] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.738 total time= 1.3min\n",
      "[CV 6/10; 16/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 16/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.767 total time= 1.1min\n",
      "[CV 4/10; 17/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 17/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.9min\n",
      "[CV 2/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.760 total time= 2.1min\n",
      "[CV 10/10; 18/54] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 18/54] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.732 total time= 1.9min\n",
      "[CV 8/10; 20/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 20/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.738 total time=  35.1s\n",
      "[CV 6/10; 21/54] START learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 21/54] END learning_rate=0.035, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.768 total time=  37.0s\n",
      "[CV 4/10; 22/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 22/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.741 total time=  39.1s\n",
      "[CV 2/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.759 total time=  47.1s\n",
      "[CV 10/10; 23/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 23/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.732 total time=  48.3s\n",
      "[CV 8/10; 24/54] START learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 24/54] END learning_rate=0.035, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.738 total time=  48.1s\n",
      "[CV 6/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.766 total time=  59.8s\n",
      "[CV 4/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.746 total time= 1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 8/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.738 total time=  59.5s\n",
      "[CV 6/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.765 total time= 1.2min\n",
      "[CV 4/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.743 total time= 1.2min\n",
      "[CV 2/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.762 total time=  33.3s\n",
      "[CV 6/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.761 total time=  33.9s\n",
      "[CV 3/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.756 total time=  59.8s\n",
      "[CV 1/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.756 total time=  59.4s\n",
      "[CV 9/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.740 total time= 1.0min\n",
      "[CV 10/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.734 total time=  45.1s\n",
      "[CV 7/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.749 total time= 1.2min\n",
      "[CV 5/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.739 total time= 1.3min\n",
      "[CV 3/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.755 total time= 1.0min\n",
      "[CV 8/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.739 total time= 1.0min\n",
      "[CV 6/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.769 total time= 1.8min\n",
      "[CV 5/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.739 total time= 1.9min\n",
      "[CV 4/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.740 total time=  28.6s\n",
      "[CV 8/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.739 total time=  28.4s\n",
      "[CV 3/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.758 total time=  33.1s\n",
      "[CV 1/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.757 total time=  33.7s\n",
      "[CV 9/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.744 total time=  33.0s\n",
      "[CV 6/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.765 total time=  37.8s\n",
      "[CV 4/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time=  45.7s\n",
      "[CV 3/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.758 total time=  46.0s\n",
      "[CV 2/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.759 total time=  57.1s\n",
      "[CV 10/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.729 total time=  60.0s\n",
      "[CV 8/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.735 total time= 1.1min\n",
      "[CV 5/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.737 total time= 1.2min\n",
      "[CV 3/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.752 total time=  36.2s\n",
      "[CV 9/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.743 total time=  34.1s\n",
      "[CV 6/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.769 total time=  59.5s\n",
      "[CV 4/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.744 total time=  59.8s\n",
      "[CV 9/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.743 total time= 1.0min\n",
      "[CV 7/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.748 total time= 1.2min\n",
      "[CV 5/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.736 total time= 1.2min\n",
      "[CV 3/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.755 total time=  33.3s\n",
      "[CV 9/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.741 total time=  35.0s\n",
      "[CV 7/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.749 total time=  57.3s\n",
      "[CV 5/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.738 total time= 1.0min\n",
      "[CV 3/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.756 total time=  45.9s\n",
      "[CV 7/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.749 total time=  44.7s\n",
      "[CV 5/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.2min\n",
      "[CV 3/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.757 total time= 1.3min\n",
      "[CV 1/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.754 total time= 1.0min\n",
      "[CV 7/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.750 total time= 1.0min\n",
      "[CV 5/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.9min\n",
      "[CV 4/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.742 total time= 1.9min\n",
      "[CV 2/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.759 total time=  29.0s\n",
      "[CV 6/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.765 total time=  28.6s\n",
      "[CV 2/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.757 total time=  33.4s\n",
      "[CV 10/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.729 total time=  34.9s\n",
      "[CV 10/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.731 total time=  34.3s\n",
      "[CV 8/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.739 total time=  37.6s\n",
      "[CV 6/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.769 total time=  43.1s\n",
      "[CV 1/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.757 total time=  45.0s\n",
      "[CV 1/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.757 total time=  57.6s\n",
      "[CV 9/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.747 total time=  59.2s\n",
      "[CV 7/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.2min\n",
      "[CV 6/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.767 total time= 1.3min\n",
      "[CV 4/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.742 total time=  37.9s\n",
      "[CV 1/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.754 total time= 1.0min\n",
      "[CV 9/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.740 total time=  59.0s\n",
      "[CV 7/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.752 total time= 1.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 9/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.2min\n",
      "[CV 7/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.750 total time= 1.2min\n",
      "[CV 7/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.749 total time=  34.4s\n",
      "[CV 5/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.741 total time=  59.8s\n",
      "[CV 3/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.757 total time= 1.0min\n",
      "[CV 2/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.763 total time=  48.1s\n",
      "[CV 9/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  46.2s\n",
      "[CV 8/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.2min\n",
      "[CV 6/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.771 total time= 1.2min\n",
      "[CV 2/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.762 total time= 1.1min\n",
      "[CV 10/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.735 total time= 1.1min\n",
      "[CV 8/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.8min\n",
      "[CV 6/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.771 total time= 1.9min\n",
      "[CV 3/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.753 total time=  28.8s\n",
      "[CV 7/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.749 total time=  29.0s\n",
      "[CV 4/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.742 total time=  34.1s\n",
      "[CV 2/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.760 total time=  32.2s\n",
      "[CV 8/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.738 total time=  34.6s\n",
      "[CV 7/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.749 total time=  37.5s\n",
      "[CV 5/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.734 total time=  44.5s\n",
      "[CV 4/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.740 total time=  43.6s\n",
      "[CV 9/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.744 total time=  47.1s\n",
      "[CV 8/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.738 total time=  59.9s\n",
      "[CV 6/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.769 total time= 1.1min\n",
      "[CV 3/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.758 total time= 1.3min\n",
      "[CV 2/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.759 total time=  39.0s\n",
      "[CV 7/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.747 total time=  34.3s\n",
      "[CV 4/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.1min\n",
      "[CV 3/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.755 total time= 1.0min\n",
      "[CV 1/10; 49/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 49/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.753 total time=  45.6s\n",
      "[CV 7/10; 49/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 49/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.747 total time=  43.1s\n",
      "[CV 2/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.758 total time= 1.0min\n",
      "[CV 10/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.730 total time= 1.0min\n",
      "[CV 8/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.2min\n",
      "[CV 6/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.768 total time= 1.2min\n",
      "[CV 4/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.743 total time=  33.6s\n",
      "[CV 10/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.734 total time=  34.3s\n",
      "[CV 8/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.741 total time=  56.6s\n",
      "[CV 6/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.771 total time= 1.0min\n",
      "[CV 4/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  47.5s\n",
      "[CV 1/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.2min\n",
      "[CV 9/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.2min\n",
      "[CV 7/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.751 total time= 1.3min\n",
      "[CV 5/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.738 total time= 1.0min\n",
      "[CV 3/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.8min\n",
      "[CV 1/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.757 total time= 1.9min\n",
      "[CV 9/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.740 total time= 1.8min\n",
      "[CV 6/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.769 total time=  31.7s\n",
      "[CV 4/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.740 total time=  33.7s\n",
      "[CV 2/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 2/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.759 total time=  37.1s\n",
      "[CV 10/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.729 total time=  38.2s\n",
      "[CV 8/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.735 total time=  43.3s\n",
      "[CV 6/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.767 total time=  46.6s\n",
      "[CV 4/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.740 total time=  58.0s\n",
      "[CV 2/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.1min\n",
      "[CV 10/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.730 total time= 1.2min\n",
      "[CV 8/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.737 total time= 1.3min\n",
      "[CV 8/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.737 total time=  35.1s\n",
      "[CV 7/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.749 total time= 1.1min\n",
      "[CV 6/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.770 total time= 1.0min\n",
      "[CV 4/10; 49/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 49/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.742 total time=  47.2s\n",
      "[CV 1/10; 50/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 50/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.754 total time= 1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.2min\n",
      "[CV 1/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.756 total time= 1.2min\n",
      "[CV 9/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.740 total time= 1.2min\n",
      "[CV 1/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.0min\n",
      "[CV 9/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.742 total time=  56.5s\n",
      "[CV 7/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.750 total time= 1.1min\n",
      "[CV 5/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.738 total time=  44.4s\n",
      "[CV 3/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.2min\n",
      "[CV 1/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.757 total time= 1.2min\n",
      "[CV 9/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.740 total time= 1.2min\n",
      "[CV 9/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.742 total time= 1.1min\n",
      "[CV 7/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.8min\n",
      "[CV 3/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.757 total time= 1.9min\n",
      "[CV 1/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.757 total time=  28.9s\n",
      "[CV 5/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.736 total time=  27.5s\n",
      "[CV 1/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.754 total time=  33.5s\n",
      "[CV 9/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.738 total time=  33.1s\n",
      "[CV 7/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.745 total time=  34.8s\n",
      "[CV 5/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.736 total time=  38.4s\n",
      "[CV 3/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.758 total time=  45.6s\n",
      "[CV 2/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.759 total time=  44.6s\n",
      "[CV 10/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.731 total time=  46.1s\n",
      "[CV 7/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.749 total time=  58.7s\n",
      "[CV 5/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.735 total time= 1.2min\n",
      "[CV 4/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.740 total time= 1.2min\n",
      "[CV 1/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.753 total time=  38.2s\n",
      "[CV 5/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.736 total time=  36.1s\n",
      "[CV 5/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.1min\n",
      "[CV 2/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.758 total time= 1.0min\n",
      "[CV 10/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.731 total time= 1.0min\n",
      "[CV 2/10; 50/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 50/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.759 total time= 1.2min\n",
      "[CV 7/10; 25/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 7/10; 25/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.749 total time=  59.5s\n",
      "[CV 5/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.2min\n",
      "[CV 3/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.757 total time= 1.2min\n",
      "[CV 1/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.753 total time=  31.8s\n",
      "[CV 5/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.737 total time=  32.9s\n",
      "[CV 2/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.758 total time=  59.6s\n",
      "[CV 10/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.731 total time=  58.4s\n",
      "[CV 8/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.737 total time= 1.1min\n",
      "[CV 8/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.739 total time=  45.8s\n",
      "[CV 6/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.768 total time= 1.2min\n",
      "[CV 4/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.741 total time= 1.3min\n",
      "[CV 4/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.742 total time= 1.1min\n",
      "[CV 2/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.8min\n",
      "[CV 10/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.8min\n",
      "[CV 8/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.737 total time= 1.9min\n",
      "[CV 10/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.729 total time=  28.5s\n",
      "[CV 8/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.735 total time=  33.0s\n",
      "[CV 6/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.767 total time=  34.4s\n",
      "[CV 4/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.740 total time=  37.5s\n",
      "[CV 2/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.756 total time=  44.4s\n",
      "[CV 10/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.729 total time=  44.7s\n",
      "[CV 8/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.737 total time=  45.8s\n",
      "[CV 6/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.766 total time=  59.8s\n",
      "[CV 4/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.2min\n",
      "[CV 2/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.759 total time= 1.2min\n",
      "[CV 10/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.732 total time= 1.3min\n",
      "[CV 2/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.758 total time=  58.0s\n",
      "[CV 10/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.730 total time=  58.7s\n",
      "[CV 8/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.735 total time= 1.0min\n",
      "[CV 6/10; 49/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 49/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.768 total time=  43.2s\n",
      "[CV 4/10; 50/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 50/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 10/10; 26/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 26/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.2min\n",
      "[CV 8/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.738 total time= 1.2min\n",
      "[CV 8/10; 28/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 8/10; 28/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.739 total time=  33.0s\n",
      "[CV 6/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.768 total time=  58.8s\n",
      "[CV 4/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 4/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.741 total time= 1.0min\n",
      "[CV 1/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.754 total time=  46.7s\n",
      "[CV 6/10; 31/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 31/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.759 total time=  43.1s\n",
      "[CV 4/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.2min\n",
      "[CV 2/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.760 total time= 1.3min\n",
      "[CV 10/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.732 total time= 1.2min\n",
      "[CV 1/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.8min\n",
      "[CV 9/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.8min\n",
      "[CV 7/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.751 total time= 1.8min\n",
      "[CV 9/10; 37/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 37/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.747 total time=  28.2s\n",
      "[CV 5/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.735 total time=  33.6s\n",
      "[CV 3/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 3/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.758 total time=  34.4s\n",
      "[CV 1/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 1/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.757 total time=  36.9s\n",
      "[CV 9/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.747 total time=  37.3s\n",
      "[CV 7/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.750 total time=  44.7s\n",
      "[CV 5/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.737 total time=  44.0s\n",
      "[CV 3/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.753 total time=  58.5s\n",
      "[CV 1/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.754 total time= 1.2min\n",
      "[CV 9/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.2min\n",
      "[CV 7/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.746 total time= 1.3min\n",
      "[CV 6/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.768 total time=  33.2s\n",
      "[CV 3/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.755 total time= 1.0min\n",
      "[CV 1/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.756 total time= 1.0min\n",
      "[CV 9/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.741 total time=  56.7s\n",
      "[CV 9/10; 49/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 9/10; 49/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.743 total time=  42.0s\n",
      "[CV 7/10; 50/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 50/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.749 total time= 1.2min\n",
      "[CV 5/10; 51/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 51/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.736 total time= 1.2min\n",
      "[CV 2/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.757 total time= 1.2min\n",
      "[CV 10/10; 27/54] START learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 27/54] END learning_rate=0.035, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.730 total time= 1.3min\n",
      "[CV 4/10; 29/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 29/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.741 total time=  59.7s\n",
      "[CV 2/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.760 total time=  59.0s\n",
      "[CV 10/10; 30/54] START learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 30/54] END learning_rate=0.035, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.732 total time= 1.0min\n",
      "[CV 2/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.2min\n",
      "[CV 10/10; 32/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 32/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.732 total time= 1.2min\n",
      "[CV 8/10; 33/54] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 8/10; 33/54] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.737 total time= 1.3min\n",
      "[CV 6/10; 34/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 6/10; 34/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.760 total time= 1.1min\n",
      "[CV 4/10; 35/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 35/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.8min\n",
      "[CV 2/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 2/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.760 total time= 1.9min\n",
      "[CV 10/10; 36/54] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 10/10; 36/54] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.732 total time= 1.8min\n",
      "[CV 7/10; 38/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 38/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.750 total time=  33.1s\n",
      "[CV 5/10; 39/54] START learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 39/54] END learning_rate=0.05, max_depth=10, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.737 total time=  34.0s\n",
      "[CV 3/10; 40/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 40/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.753 total time=  37.1s\n",
      "[CV 1/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.754 total time=  42.5s\n",
      "[CV 9/10; 41/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 41/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.739 total time=  43.7s\n",
      "[CV 7/10; 42/54] START learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 7/10; 42/54] END learning_rate=0.05, max_depth=10, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.746 total time=  44.7s\n",
      "[CV 5/10; 43/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 5/10; 43/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.737 total time=  57.0s\n",
      "[CV 3/10; 44/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 44/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.2min\n",
      "[CV 1/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 1/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.757 total time= 1.2min\n",
      "[CV 9/10; 45/54] START learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1\n",
      "[CV 9/10; 45/54] END learning_rate=0.05, max_depth=10, n_estimators=3000, num_leaves=136, objective=regression_l1;, score=0.744 total time= 1.3min\n",
      "[CV 10/10; 46/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 46/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=62, objective=regression_l1;, score=0.730 total time=  33.7s\n",
      "[CV 8/10; 47/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 47/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.0min\n",
      "[CV 5/10; 48/54] START learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1\n",
      "[CV 5/10; 48/54] END learning_rate=0.05, max_depth=15, n_estimators=1500, num_leaves=136, objective=regression_l1;, score=0.736 total time= 1.0min\n",
      "[CV 3/10; 49/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 3/10; 49/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.753 total time=  46.4s\n",
      "[CV 10/10; 49/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1\n",
      "[CV 10/10; 49/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=62, objective=regression_l1;, score=0.731 total time=  43.5s\n",
      "[CV 8/10; 50/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 50/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.2min\n",
      "[CV 6/10; 51/54] START learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1\n",
      "[CV 6/10; 51/54] END learning_rate=0.05, max_depth=15, n_estimators=2000, num_leaves=136, objective=regression_l1;, score=0.771 total time= 1.2min\n",
      "[CV 4/10; 52/54] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1\n",
      "[CV 4/10; 52/54] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=62, objective=regression_l1;, score=0.743 total time= 1.0min\n",
      "[CV 2/10; 53/54] START learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 53/54] END learning_rate=0.05, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator across ALL searched params:\n",
      " LGBMRegressor(learning_rate=0.02, max_depth=15, n_estimators=3000,\n",
      "              num_leaves=124, objective='regression_l1', random_state=420)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "lgbm = lightgbm.LGBMRegressor(random_state = 420, n_jobs = -1)\n",
    "\n",
    "# Using many estimators with low learning rates\n",
    "lgbm_params = {'learning_rate': [0.02, 0.035, 0.05],\n",
    "                           'num_leaves':[62, 124, 136],\n",
    "                           'max_depth': [10, 15],\n",
    "                           'n_estimators': [1500, 2000, 3000],\n",
    "                           'objective': ['regression_l1']}\n",
    "\n",
    "lgbm_grid_search = GridSearchCV(estimator = lgbm, \n",
    "                                param_grid =  lgbm_params,\n",
    "                                cv=10,\n",
    "                                verbose = 10,\n",
    "                                scoring = make_scorer(nrmsle, greater_is_better=True),\n",
    "                                n_jobs=-1)\n",
    "\n",
    "lgbm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"The best estimator across ALL searched params:\\n\", lgbm_grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator across ALL searched params:\n",
      " LGBMRegressor(learning_rate=0.02, max_depth=15, n_estimators=3000,\n",
      "              num_leaves=124, objective='regression_l1', random_state=420)\n",
      "The best score of all model parameters' combination on model: 0.7488\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>108.504072</td>\n",
       "      <td>2.775470</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748817</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>65.138029</td>\n",
       "      <td>1.879507</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748763</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>122.188814</td>\n",
       "      <td>2.510783</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748735</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>109.664678</td>\n",
       "      <td>2.630751</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.748682</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>71.569562</td>\n",
       "      <td>1.622713</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748602</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>105.529463</td>\n",
       "      <td>2.605351</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.748537</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>43.761588</td>\n",
       "      <td>1.243919</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748513</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>75.818199</td>\n",
       "      <td>1.692896</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748462</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>68.650922</td>\n",
       "      <td>1.712815</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748441</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>71.140385</td>\n",
       "      <td>1.512507</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.748432</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>73.919105</td>\n",
       "      <td>1.621277</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.748425</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>60.172224</td>\n",
       "      <td>1.521903</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.748339</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>57.546008</td>\n",
       "      <td>1.229749</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.748333</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>63.946633</td>\n",
       "      <td>1.552420</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748238</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>76.141595</td>\n",
       "      <td>2.335241</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.748233</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32.155961</td>\n",
       "      <td>1.132340</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748194</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>97.641257</td>\n",
       "      <td>1.689114</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.748125</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>81.930688</td>\n",
       "      <td>2.937947</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.748091</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>48.624142</td>\n",
       "      <td>1.587125</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.748058</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>69.742080</td>\n",
       "      <td>2.126828</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.748058</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.722246</td>\n",
       "      <td>1.161177</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.748026</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>72.322694</td>\n",
       "      <td>1.684640</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.747963</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.678981</td>\n",
       "      <td>1.926233</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.747881</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>45.833322</td>\n",
       "      <td>1.340130</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.747875</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>34.319594</td>\n",
       "      <td>1.044041</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.747830</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>59.350826</td>\n",
       "      <td>1.347235</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.747795</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>32.928832</td>\n",
       "      <td>0.972699</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.747692</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>72.828998</td>\n",
       "      <td>2.247943</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.747689</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>105.266735</td>\n",
       "      <td>2.547300</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.747609</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43.833892</td>\n",
       "      <td>1.513771</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.747595</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>61.597893</td>\n",
       "      <td>1.729559</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.747591</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>69.202305</td>\n",
       "      <td>2.117274</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.747520</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>44.603979</td>\n",
       "      <td>1.143230</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.747465</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>47.011931</td>\n",
       "      <td>1.496599</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.747461</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32.658448</td>\n",
       "      <td>0.883820</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.747452</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>70.497831</td>\n",
       "      <td>1.720876</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.747447</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>36.433195</td>\n",
       "      <td>1.039576</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.747441</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>56.653364</td>\n",
       "      <td>1.919293</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.747431</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>27.649173</td>\n",
       "      <td>0.898625</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.747395</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>58.651395</td>\n",
       "      <td>1.848101</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.747394</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>38.231899</td>\n",
       "      <td>1.362744</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.747390</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.950977</td>\n",
       "      <td>1.000628</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.747379</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>36.377273</td>\n",
       "      <td>1.177529</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.747379</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>59.531913</td>\n",
       "      <td>1.405609</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.747361</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>31.253975</td>\n",
       "      <td>0.946041</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.747184</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>43.196717</td>\n",
       "      <td>1.259716</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.747033</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56.222559</td>\n",
       "      <td>1.957762</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>34.825323</td>\n",
       "      <td>0.952797</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.746925</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>59.989827</td>\n",
       "      <td>1.847373</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.746898</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42.763574</td>\n",
       "      <td>1.461376</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.746851</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>32.337618</td>\n",
       "      <td>0.995524</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.746839</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>68.115784</td>\n",
       "      <td>2.069803</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.746839</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.965296</td>\n",
       "      <td>1.303119</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.746801</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.962283</td>\n",
       "      <td>0.910468</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.746613</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  \\\n",
       "16     108.504072         2.775470   \n",
       "15      65.138029         1.879507   \n",
       "17     122.188814         2.510783   \n",
       "35     109.664678         2.630751   \n",
       "13      71.569562         1.622713   \n",
       "34     105.529463         2.605351   \n",
       "12      43.761588         1.243919   \n",
       "14      75.818199         1.692896   \n",
       "11      68.650922         1.712815   \n",
       "31      71.140385         1.512507   \n",
       "32      73.919105         1.621277   \n",
       "29      60.172224         1.521903   \n",
       "28      57.546008         1.229749   \n",
       "10      63.946633         1.552420   \n",
       "8       76.141595         2.335241   \n",
       "9       32.155961         1.132340   \n",
       "53      97.641257         1.689114   \n",
       "7       81.930688         2.937947   \n",
       "5       48.624142         1.587125   \n",
       "25      69.742080         2.126828   \n",
       "2       33.722246         1.161177   \n",
       "50      72.322694         1.684640   \n",
       "4       47.678981         1.926233   \n",
       "22      45.833322         1.340130   \n",
       "19      34.319594         1.044041   \n",
       "47      59.350826         1.347235   \n",
       "38      32.928832         0.972699   \n",
       "44      72.828998         2.247943   \n",
       "52     105.266735         2.547300   \n",
       "41      43.833892         1.513771   \n",
       "33      61.597893         1.729559   \n",
       "26      69.202305         2.117274   \n",
       "30      44.603979         1.143230   \n",
       "23      47.011931         1.496599   \n",
       "27      32.658448         0.883820   \n",
       "49      70.497831         1.720876   \n",
       "20      36.433195         1.039576   \n",
       "42      56.653364         1.919293   \n",
       "36      27.649173         0.898625   \n",
       "24      58.651395         1.848101   \n",
       "21      38.231899         1.362744   \n",
       "1       34.950977         1.000628   \n",
       "39      36.377273         1.177529   \n",
       "46      59.531913         1.405609   \n",
       "18      31.253975         0.946041   \n",
       "48      43.196717         1.259716   \n",
       "6       56.222559         1.957762   \n",
       "45      34.825323         0.952797   \n",
       "51      59.989827         1.847373   \n",
       "40      42.763574         1.461376   \n",
       "37      32.337618         0.995524   \n",
       "43      68.115784         2.069803   \n",
       "3       35.965296         1.303119   \n",
       "0       28.962283         0.910468   \n",
       "\n",
       "                                               params  mean_test_score  \\\n",
       "16  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.748817   \n",
       "15  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.748763   \n",
       "17  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.748735   \n",
       "35  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.748682   \n",
       "13  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.748602   \n",
       "34  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.748537   \n",
       "12  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.748513   \n",
       "14  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.748462   \n",
       "11  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.748441   \n",
       "31  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.748432   \n",
       "32  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.748425   \n",
       "29  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.748339   \n",
       "28  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.748333   \n",
       "10  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.748238   \n",
       "8   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.748233   \n",
       "9   {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.748194   \n",
       "53  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.748125   \n",
       "7   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.748091   \n",
       "5   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.748058   \n",
       "25  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.748058   \n",
       "2   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.748026   \n",
       "50  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.747963   \n",
       "4   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.747881   \n",
       "22  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.747875   \n",
       "19  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.747830   \n",
       "47  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.747795   \n",
       "38  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.747692   \n",
       "44  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.747689   \n",
       "52  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.747609   \n",
       "41  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.747595   \n",
       "33  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.747591   \n",
       "26  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.747520   \n",
       "30  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.747465   \n",
       "23  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.747461   \n",
       "27  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.747452   \n",
       "49  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.747447   \n",
       "20  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.747441   \n",
       "42  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.747431   \n",
       "36  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.747395   \n",
       "24  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.747394   \n",
       "21  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.747390   \n",
       "1   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.747379   \n",
       "39  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.747379   \n",
       "46  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.747361   \n",
       "18  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.747184   \n",
       "48  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.747033   \n",
       "6   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.747000   \n",
       "45  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.746925   \n",
       "51  {'learning_rate': 0.05, 'max_depth': 15, 'n_es...         0.746898   \n",
       "40  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.746851   \n",
       "37  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.746839   \n",
       "43  {'learning_rate': 0.05, 'max_depth': 10, 'n_es...         0.746839   \n",
       "3   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.746801   \n",
       "0   {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.746613   \n",
       "\n",
       "    rank_test_score  \n",
       "16                1  \n",
       "15                2  \n",
       "17                3  \n",
       "35                4  \n",
       "13                5  \n",
       "34                6  \n",
       "12                7  \n",
       "14                8  \n",
       "11                9  \n",
       "31               10  \n",
       "32               11  \n",
       "29               12  \n",
       "28               13  \n",
       "10               14  \n",
       "8                15  \n",
       "9                16  \n",
       "53               17  \n",
       "7                18  \n",
       "5                19  \n",
       "25               20  \n",
       "2                21  \n",
       "50               22  \n",
       "4                23  \n",
       "22               24  \n",
       "19               25  \n",
       "47               26  \n",
       "38               27  \n",
       "44               28  \n",
       "52               29  \n",
       "41               30  \n",
       "33               31  \n",
       "26               32  \n",
       "30               33  \n",
       "23               34  \n",
       "27               35  \n",
       "49               36  \n",
       "20               37  \n",
       "42               38  \n",
       "36               39  \n",
       "24               40  \n",
       "21               41  \n",
       "1                42  \n",
       "39               43  \n",
       "46               44  \n",
       "18               45  \n",
       "48               46  \n",
       "6                47  \n",
       "45               48  \n",
       "51               49  \n",
       "40               50  \n",
       "37               51  \n",
       "43               52  \n",
       "3                53  \n",
       "0                54  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The best estimator across ALL searched params:\\n\", lgbm_grid_search.best_estimator_)\n",
    "\n",
    "print(\"The best score of all model parameters' combination on model: {:.4f}\".format(lgbm_grid_search.best_score_))\n",
    "display(pd.DataFrame(lgbm_grid_search.cv_results_).sort_values(by=['rank_test_score'])[['mean_fit_time', 'mean_score_time', \n",
    "                                           'params', 'mean_test_score', 'rank_test_score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgbm_grid_search.best_estimator_\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='Feature importance', ylabel='Features'>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1130.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(font_scale = 1)\n",
    "lightgbm.plot_importance(model, height=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving-exporting the results to a .csv file\n",
    "pd.DataFrame(predictions, columns = ['Price']).to_csv('submission_lgbm_type_removed.csv', index=False)\n",
    "# 0.75772"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling without the Print: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_df[data_df['Set'] == 'train']\n",
    "y_train = X_train['Price']\n",
    "X_train =  X_train.drop(['Set', 'Price', 'Print'], axis=1)\n",
    "\n",
    "X_test = data_df[data_df['Set'] == 'test']\n",
    "#y_test = X_test['Price']\n",
    "X_test =  X_test.drop(['Set', 'Price', 'Print'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in X_train.select_dtypes(include=['object']):\n",
    "    X_test[col_name] = X_test[col_name].astype('category')\n",
    "    X_train[col_name] = X_train[col_name].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "The best estimator across ALL searched params:\n",
      " LGBMRegressor(learning_rate=0.035, max_depth=15, n_estimators=3000,\n",
      "              num_leaves=124, objective='regression_l1', random_state=420)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "lgbm = lightgbm.LGBMRegressor(random_state = 420, n_jobs = -1)\n",
    "\n",
    "# Using many estimators with low learning rates\n",
    "lgbm_params = {'learning_rate': [0.02, 0.035],\n",
    "                           'num_leaves':[124],\n",
    "                           'max_depth': [15],\n",
    "                           'n_estimators': [2000, 3000],\n",
    "                           'objective': ['regression_l1']}\n",
    "\n",
    "lgbm_grid_search = GridSearchCV(estimator = lgbm, \n",
    "                                param_grid =  lgbm_params,\n",
    "                                cv=10,\n",
    "                                verbose = 10,\n",
    "                                scoring = make_scorer(nrmsle, greater_is_better=True),\n",
    "                                n_jobs=-1)\n",
    "\n",
    "lgbm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"The best estimator across ALL searched params:\\n\", lgbm_grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator across ALL searched params:\n",
      " LGBMRegressor(learning_rate=0.035, max_depth=15, n_estimators=3000,\n",
      "              num_leaves=124, objective='regression_l1', random_state=420)\n",
      "The best score of all model parameters' combination on model: 0.7414\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116.848466</td>\n",
       "      <td>2.604575</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.741371</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73.869622</td>\n",
       "      <td>1.665040</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.741136</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116.083604</td>\n",
       "      <td>3.092865</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.740885</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80.648749</td>\n",
       "      <td>1.463908</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.740498</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  \\\n",
       "3     116.848466         2.604575   \n",
       "2      73.869622         1.665040   \n",
       "1     116.083604         3.092865   \n",
       "0      80.648749         1.463908   \n",
       "\n",
       "                                              params  mean_test_score  \\\n",
       "3  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.741371   \n",
       "2  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.741136   \n",
       "1  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.740885   \n",
       "0  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.740498   \n",
       "\n",
       "   rank_test_score  \n",
       "3                1  \n",
       "2                2  \n",
       "1                3  \n",
       "0                4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The best estimator across ALL searched params:\\n\", lgbm_grid_search.best_estimator_)\n",
    "\n",
    "print(\"The best score of all model parameters' combination on model: {:.4f}\".format(lgbm_grid_search.best_score_))\n",
    "display(pd.DataFrame(lgbm_grid_search.cv_results_).sort_values(by=['rank_test_score'])[['mean_fit_time', 'mean_score_time', \n",
    "                                           'params', 'mean_test_score', 'rank_test_score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgbm_grid_search.best_estimator_\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='Feature importance', ylabel='Features'>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1130.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(font_scale = 1)\n",
    "lightgbm.plot_importance(model, height=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving-exporting the results to a .csv file\n",
    "pd.DataFrame(predictions, columns = ['Price']).to_csv('submission_lgbm_print_removed.csv', index=False)\n",
    "# 0.75335"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling without all of the three \"non-important\" features: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_df[data_df['Set'] == 'train']\n",
    "y_train = X_train['Price']\n",
    "X_train =  X_train.drop(['Set', 'Price', 'Print', 'Type', 'No. Authors'], axis=1)\n",
    "\n",
    "X_test = data_df[data_df['Set'] == 'test']\n",
    "#y_test = X_test['Price']\n",
    "X_test =  X_test.drop(['Set', 'Price', 'Print', 'Type', 'No. Authors'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in X_train.select_dtypes(include=['object']):\n",
    "    X_test[col_name] = X_test[col_name].astype('category')\n",
    "    X_train[col_name] = X_train[col_name].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "The best estimator across ALL searched params:\n",
      " LGBMRegressor(learning_rate=0.035, max_depth=15, n_estimators=3000,\n",
      "              num_leaves=124, objective='regression_l1', random_state=420)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "lgbm = lightgbm.LGBMRegressor(random_state = 420, n_jobs = -1)\n",
    "\n",
    "# Using many estimators with low learning rates\n",
    "lgbm_params = {'learning_rate': [0.02, 0.035],\n",
    "                           'num_leaves':[124],\n",
    "                           'max_depth': [10, 15],\n",
    "                           'n_estimators': [2000, 3000],\n",
    "                           'objective': ['regression_l1']}\n",
    "\n",
    "lgbm_grid_search = GridSearchCV(estimator = lgbm, \n",
    "                                param_grid =  lgbm_params,\n",
    "                                cv=10,\n",
    "                                verbose = 10,\n",
    "                                scoring = make_scorer(nrmsle, greater_is_better=True),\n",
    "                                n_jobs=-1)\n",
    "\n",
    "lgbm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"The best estimator across ALL searched params:\\n\", lgbm_grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator across ALL searched params:\n",
      " LGBMRegressor(learning_rate=0.035, max_depth=15, n_estimators=3000,\n",
      "              num_leaves=124, objective='regression_l1', random_state=420)\n",
      "The best score of all model parameters' combination on model: 0.7398\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>91.900071</td>\n",
       "      <td>1.789789</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.739831</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>65.223878</td>\n",
       "      <td>1.722024</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.739798</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117.017870</td>\n",
       "      <td>2.084824</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.739678</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71.708829</td>\n",
       "      <td>1.475056</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.739560</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69.157411</td>\n",
       "      <td>2.110314</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.739080</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.495301</td>\n",
       "      <td>2.196973</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.739064</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.018190</td>\n",
       "      <td>1.384308</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.738840</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44.078613</td>\n",
       "      <td>1.355546</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 10, 'n_es...</td>\n",
       "      <td>0.738811</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  \\\n",
       "7      91.900071         1.789789   \n",
       "6      65.223878         1.722024   \n",
       "3     117.017870         2.084824   \n",
       "2      71.708829         1.475056   \n",
       "5      69.157411         2.110314   \n",
       "1      67.495301         2.196973   \n",
       "4      50.018190         1.384308   \n",
       "0      44.078613         1.355546   \n",
       "\n",
       "                                              params  mean_test_score  \\\n",
       "7  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.739831   \n",
       "6  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.739798   \n",
       "3  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.739678   \n",
       "2  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.739560   \n",
       "5  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.739080   \n",
       "1  {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.739064   \n",
       "4  {'learning_rate': 0.035, 'max_depth': 10, 'n_e...         0.738840   \n",
       "0  {'learning_rate': 0.02, 'max_depth': 10, 'n_es...         0.738811   \n",
       "\n",
       "   rank_test_score  \n",
       "7                1  \n",
       "6                2  \n",
       "3                3  \n",
       "2                4  \n",
       "5                5  \n",
       "1                6  \n",
       "4                7  \n",
       "0                8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The best estimator across ALL searched params:\\n\", lgbm_grid_search.best_estimator_)\n",
    "\n",
    "print(\"The best score of all model parameters' combination on model: {:.4f}\".format(lgbm_grid_search.best_score_))\n",
    "display(pd.DataFrame(lgbm_grid_search.cv_results_).sort_values(by=['rank_test_score'])[['mean_fit_time', 'mean_score_time', \n",
    "                                           'params', 'mean_test_score', 'rank_test_score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgbm_grid_search.best_estimator_\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='Feature importance', ylabel='Features'>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1130.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(font_scale = 1)\n",
    "lightgbm.plot_importance(model, height=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving-exporting the results to a .csv file\n",
    "pd.DataFrame(predictions, columns = ['Price']).to_csv('submission_lgbm_non_important_removed.csv', index=False)\n",
    "# 0.75277"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FInal Modelling for the best LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_df[data_df['Set'] == 'train']\n",
    "y_train = X_train['Price']\n",
    "X_train =  X_train.drop(['Set', 'Price'], axis=1)\n",
    "\n",
    "X_test = data_df[data_df['Set'] == 'test']\n",
    "#y_test = X_test['Price']\n",
    "X_test =  X_test.drop(['Set', 'Price'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in X_train.select_dtypes(include=['object']):\n",
    "    X_test[col_name] = X_test[col_name].astype('category')\n",
    "    X_train[col_name] = X_train[col_name].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "[CV 3/10; 1/60] START learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 1/60] END learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.1min\n",
      "[CV 9/10; 1/60] START learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 1/60] END learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.2min\n",
      "[CV 8/10; 2/60] START learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 2/60] END learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.4min\n",
      "[CV 5/10; 3/60] START learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 3/60] END learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.7min\n",
      "[CV 3/10; 4/60] START learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 4/60] END learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.0min\n",
      "[CV 1/10; 5/60] START learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 5/60] END learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.2min\n",
      "[CV 9/10; 5/60] START learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 5/60] END learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.1min\n",
      "[CV 7/10; 6/60] START learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 6/60] END learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.4min\n",
      "[CV 5/10; 7/60] START learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 7/60] END learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.7min\n",
      "[CV 3/10; 8/60] START learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 8/60] END learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.0min\n",
      "[CV 2/10; 9/60] START learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 9/60] END learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.762 total time= 1.1min\n",
      "[CV 8/10; 9/60] START learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 9/60] END learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.1min\n",
      "[CV 6/10; 10/60] START learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 10/60] END learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.771 total time= 1.4min\n",
      "[CV 3/10; 11/60] START learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 11/60] END learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.7min\n",
      "[CV 1/10; 12/60] START learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 12/60] END learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.0min\n",
      "[CV 9/10; 12/60] START learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 12/60] END learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.0min\n",
      "[CV 2/10; 14/60] START learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 14/60] END learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.762 total time= 1.2min\n",
      "[CV 10/10; 14/60] START learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 14/60] END learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.730 total time= 1.2min\n",
      "[CV 8/10; 15/60] START learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 15/60] END learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.4min\n",
      "[CV 5/10; 16/60] START learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 16/60] END learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.7min\n",
      "[CV 3/10; 17/60] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 17/60] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.1min\n",
      "[CV 1/10; 18/60] START learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 18/60] END learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.4min\n",
      "[CV 9/10; 18/60] START learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 18/60] END learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.4min\n",
      "[CV 7/10; 19/60] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 19/60] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.7min\n",
      "[CV 5/10; 20/60] START learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 20/60] END learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.9min\n",
      "[CV 3/10; 21/60] START learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 21/60] END learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.1min\n",
      "[CV 9/10; 21/60] START learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 21/60] END learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.1min\n",
      "[CV 7/10; 22/60] START learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 22/60] END learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.751 total time= 1.4min\n",
      "[CV 5/10; 23/60] START learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 23/60] END learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.7min\n",
      "[CV 3/10; 24/60] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 24/60] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.0min\n",
      "[CV 1/10; 25/60] START learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 25/60] END learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.756 total time=  58.4s\n",
      "[CV 5/10; 1/60] START learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 1/60] END learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.1min\n",
      "[CV 2/10; 2/60] START learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 2/60] END learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.761 total time= 1.4min\n",
      "[CV 9/10; 2/60] START learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 2/60] END learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.745 total time= 1.4min\n",
      "[CV 7/10; 3/60] START learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 3/60] END learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.7min\n",
      "[CV 6/10; 4/60] START learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 4/60] END learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.770 total time= 2.0min\n",
      "[CV 4/10; 5/60] START learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 5/60] END learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.2min\n",
      "[CV 2/10; 6/60] START learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 6/60] END learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.762 total time= 1.4min\n",
      "[CV 10/10; 6/60] START learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 6/60] END learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.731 total time= 1.4min\n",
      "[CV 8/10; 7/60] START learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 7/60] END learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.6min\n",
      "[CV 6/10; 8/60] START learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 8/60] END learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.770 total time= 1.9min\n",
      "[CV 4/10; 9/60] START learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 9/60] END learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.1min\n",
      "[CV 1/10; 10/60] START learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 10/60] END learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.4min\n",
      "[CV 9/10; 10/60] START learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 10/60] END learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.4min\n",
      "[CV 7/10; 11/60] START learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 11/60] END learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.751 total time= 1.7min\n",
      "[CV 5/10; 12/60] START learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 12/60] END learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.0min\n",
      "[CV 3/10; 13/60] START learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 13/60] END learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.0min\n",
      "[CV 9/10; 13/60] START learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 13/60] END learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.742 total time=  55.0s\n",
      "[CV 7/10; 14/60] START learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 14/60] END learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.751 total time= 1.2min\n",
      "[CV 5/10; 15/60] START learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 15/60] END learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.4min\n",
      "[CV 3/10; 16/60] START learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 16/60] END learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.7min\n",
      "[CV 10/10; 16/60] START learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 16/60] END learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.731 total time= 1.6min\n",
      "[CV 2/10; 18/60] START learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 18/60] END learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.761 total time= 1.4min\n",
      "[CV 10/10; 18/60] START learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 18/60] END learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.4min\n",
      "[CV 8/10; 19/60] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 19/60] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.7min\n",
      "[CV 6/10; 20/60] START learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 20/60] END learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.772 total time= 2.0min\n",
      "[CV 4/10; 21/60] START learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 21/60] END learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.1min\n",
      "[CV 10/10; 21/60] START learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 21/60] END learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.1min\n",
      "[CV 8/10; 22/60] START learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 22/60] END learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.4min\n",
      "[CV 6/10; 23/60] START learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 23/60] END learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.773 total time= 1.7min\n",
      "[CV 4/10; 24/60] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 24/60] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 2.0min\n",
      "[CV 2/10; 25/60] START learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 25/60] END learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.760 total time=  57.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/10; 1/60] START learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 1/60] END learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.1min\n",
      "[CV 1/10; 2/60] START learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 2/60] END learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.4min\n",
      "[CV 10/10; 2/60] START learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 2/60] END learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.730 total time= 1.4min\n",
      "[CV 8/10; 3/60] START learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 3/60] END learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.7min\n",
      "[CV 5/10; 4/60] START learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 4/60] END learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.739 total time= 2.0min\n",
      "[CV 3/10; 5/60] START learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 5/60] END learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.2min\n",
      "[CV 1/10; 6/60] START learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 6/60] END learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.3min\n",
      "[CV 9/10; 6/60] START learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 6/60] END learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.745 total time= 1.3min\n",
      "[CV 7/10; 7/60] START learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 7/60] END learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.751 total time= 1.6min\n",
      "[CV 5/10; 8/60] START learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 8/60] END learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.9min\n",
      "[CV 3/10; 9/60] START learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 9/60] END learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.1min\n",
      "[CV 9/10; 9/60] START learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 9/60] END learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.2min\n",
      "[CV 8/10; 10/60] START learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 10/60] END learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.4min\n",
      "[CV 5/10; 11/60] START learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 11/60] END learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.7min\n",
      "[CV 4/10; 12/60] START learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 12/60] END learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.0min\n",
      "[CV 2/10; 13/60] START learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 13/60] END learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.762 total time=  58.8s\n",
      "[CV 6/10; 13/60] START learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 13/60] END learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.770 total time=  56.4s\n",
      "[CV 4/10; 14/60] START learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 14/60] END learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.2min\n",
      "[CV 2/10; 15/60] START learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 15/60] END learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.761 total time= 1.5min\n",
      "[CV 10/10; 15/60] START learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 15/60] END learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.731 total time= 1.4min\n",
      "[CV 8/10; 16/60] START learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 16/60] END learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.7min\n",
      "[CV 7/10; 17/60] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 17/60] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.1min\n",
      "[CV 6/10; 18/60] START learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 18/60] END learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.772 total time= 1.4min\n",
      "[CV 4/10; 19/60] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 19/60] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.6min\n",
      "[CV 1/10; 20/60] START learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 20/60] END learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.9min\n",
      "[CV 9/10; 20/60] START learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 20/60] END learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.9min\n",
      "[CV 1/10; 22/60] START learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 22/60] END learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.5min\n",
      "[CV 9/10; 22/60] START learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 22/60] END learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.4min\n",
      "[CV 7/10; 23/60] START learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 23/60] END learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.752 total time= 1.7min\n",
      "[CV 6/10; 24/60] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 24/60] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.773 total time= 2.0min\n",
      "[CV 4/10; 25/60] START learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 25/60] END learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.741 total time=  58.6s\n",
      "[CV 8/10; 1/60] START learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 1/60] END learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.1min\n",
      "[CV 10/10; 1/60] START learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 1/60] END learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.730 total time= 1.1min\n",
      "[CV 7/10; 2/60] START learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 2/60] END learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.4min\n",
      "[CV 6/10; 3/60] START learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 3/60] END learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.769 total time= 1.7min\n",
      "[CV 4/10; 4/60] START learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 4/60] END learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.0min\n",
      "[CV 2/10; 5/60] START learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 5/60] END learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.762 total time= 1.2min\n",
      "[CV 10/10; 5/60] START learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 5/60] END learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.730 total time= 1.1min\n",
      "[CV 8/10; 6/60] START learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 6/60] END learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.4min\n",
      "[CV 6/10; 7/60] START learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 7/60] END learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.770 total time= 1.6min\n",
      "[CV 4/10; 8/60] START learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 8/60] END learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.9min\n",
      "[CV 1/10; 9/60] START learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 9/60] END learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.1min\n",
      "[CV 7/10; 9/60] START learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 9/60] END learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.1min\n",
      "[CV 5/10; 10/60] START learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 10/60] END learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.5min\n",
      "[CV 4/10; 11/60] START learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 11/60] END learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.7min\n",
      "[CV 2/10; 12/60] START learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 12/60] END learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.762 total time= 1.9min\n",
      "[CV 10/10; 12/60] START learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 12/60] END learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.731 total time= 1.9min\n",
      "[CV 1/10; 14/60] START learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 14/60] END learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.2min\n",
      "[CV 9/10; 14/60] START learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 14/60] END learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.2min\n",
      "[CV 7/10; 15/60] START learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 15/60] END learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.752 total time= 1.5min\n",
      "[CV 6/10; 16/60] START learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 16/60] END learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.771 total time= 1.7min\n",
      "[CV 4/10; 17/60] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 17/60] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.1min\n",
      "[CV 10/10; 17/60] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 17/60] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.1min\n",
      "[CV 8/10; 18/60] START learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 18/60] END learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.4min\n",
      "[CV 6/10; 19/60] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 19/60] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.772 total time= 1.7min\n",
      "[CV 4/10; 20/60] START learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 20/60] END learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.9min\n",
      "[CV 2/10; 21/60] START learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 21/60] END learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.762 total time= 1.1min\n",
      "[CV 7/10; 21/60] START learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 21/60] END learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.751 total time= 1.1min\n",
      "[CV 5/10; 22/60] START learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 22/60] END learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.4min\n",
      "[CV 3/10; 23/60] START learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 23/60] END learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.7min\n",
      "[CV 1/10; 24/60] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 24/60] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.0min\n",
      "[CV 9/10; 24/60] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 24/60] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/10; 1/60] START learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 1/60] END learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.2min\n",
      "[CV 4/10; 2/60] START learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 2/60] END learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.4min\n",
      "[CV 2/10; 3/60] START learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 3/60] END learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.761 total time= 1.7min\n",
      "[CV 9/10; 3/60] START learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 3/60] END learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.745 total time= 1.6min\n",
      "[CV 7/10; 4/60] START learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 4/60] END learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.751 total time= 2.0min\n",
      "[CV 5/10; 5/60] START learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 5/60] END learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.1min\n",
      "[CV 4/10; 6/60] START learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 6/60] END learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.4min\n",
      "[CV 1/10; 7/60] START learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 7/60] END learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.6min\n",
      "[CV 9/10; 7/60] START learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 7/60] END learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.7min\n",
      "[CV 7/10; 8/60] START learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 8/60] END learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.752 total time= 1.9min\n",
      "[CV 5/10; 9/60] START learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 9/60] END learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.1min\n",
      "[CV 3/10; 10/60] START learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 10/60] END learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.4min\n",
      "[CV 1/10; 11/60] START learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 11/60] END learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.7min\n",
      "[CV 9/10; 11/60] START learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 11/60] END learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.7min\n",
      "[CV 7/10; 12/60] START learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 12/60] END learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.752 total time= 1.9min\n",
      "[CV 7/10; 13/60] START learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 13/60] END learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.751 total time=  57.7s\n",
      "[CV 5/10; 14/60] START learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 14/60] END learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.2min\n",
      "[CV 3/10; 15/60] START learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 15/60] END learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.4min\n",
      "[CV 1/10; 16/60] START learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 16/60] END learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.6min\n",
      "[CV 9/10; 16/60] START learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 16/60] END learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.732 total time= 1.6min\n",
      "[CV 9/10; 17/60] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 17/60] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.1min\n",
      "[CV 7/10; 18/60] START learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 18/60] END learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.4min\n",
      "[CV 5/10; 19/60] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 19/60] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.7min\n",
      "[CV 3/10; 20/60] START learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 20/60] END learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.759 total time= 1.9min\n",
      "[CV 1/10; 21/60] START learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 21/60] END learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.1min\n",
      "[CV 5/10; 21/60] START learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 21/60] END learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.1min\n",
      "[CV 3/10; 22/60] START learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 22/60] END learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.4min\n",
      "[CV 1/10; 23/60] START learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 23/60] END learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.7min\n",
      "[CV 9/10; 23/60] START learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 23/60] END learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.7min\n",
      "[CV 7/10; 24/60] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 24/60] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.752 total time= 2.0min\n",
      "[CV 7/10; 25/60] START learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 25/60] END learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.753 total time=  56.4s\n",
      "[CV 2/10; 1/60] START learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 1/60] END learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.761 total time= 1.2min\n",
      "[CV 5/10; 2/60] START learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 2/60] END learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.5min\n",
      "[CV 3/10; 3/60] START learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 3/60] END learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.7min\n",
      "[CV 1/10; 4/60] START learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 4/60] END learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 2.0min\n",
      "[CV 10/10; 4/60] START learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 4/60] END learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.731 total time= 2.0min\n",
      "[CV 8/10; 5/60] START learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 5/60] END learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.1min\n",
      "[CV 6/10; 6/60] START learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 6/60] END learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.770 total time= 1.3min\n",
      "[CV 3/10; 7/60] START learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 7/60] END learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.6min\n",
      "[CV 1/10; 8/60] START learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 8/60] END learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.9min\n",
      "[CV 9/10; 8/60] START learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 8/60] END learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.9min\n",
      "[CV 10/10; 9/60] START learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 9/60] END learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.731 total time= 1.1min\n",
      "[CV 7/10; 10/60] START learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 10/60] END learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.751 total time= 1.4min\n",
      "[CV 6/10; 11/60] START learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 11/60] END learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.771 total time= 1.7min\n",
      "[CV 3/10; 12/60] START learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 12/60] END learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.9min\n",
      "[CV 1/10; 13/60] START learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 13/60] END learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.756 total time=  58.2s\n",
      "[CV 5/10; 13/60] START learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 13/60] END learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.739 total time=  57.5s\n",
      "[CV 3/10; 14/60] START learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 14/60] END learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.2min\n",
      "[CV 1/10; 15/60] START learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 15/60] END learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.4min\n",
      "[CV 9/10; 15/60] START learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 15/60] END learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.711 total time= 1.4min\n",
      "[CV 7/10; 16/60] START learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 16/60] END learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.752 total time= 1.7min\n",
      "[CV 5/10; 17/60] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 17/60] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.1min\n",
      "[CV 3/10; 18/60] START learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 18/60] END learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.759 total time= 1.4min\n",
      "[CV 1/10; 19/60] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 19/60] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.7min\n",
      "[CV 9/10; 19/60] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 19/60] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.6min\n",
      "[CV 7/10; 20/60] START learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 20/60] END learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.9min\n",
      "[CV 6/10; 21/60] START learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 21/60] END learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.772 total time= 1.1min\n",
      "[CV 4/10; 22/60] START learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 22/60] END learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.4min\n",
      "[CV 2/10; 23/60] START learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 23/60] END learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.763 total time= 1.7min\n",
      "[CV 10/10; 23/60] START learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 23/60] END learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.7min\n",
      "[CV 8/10; 24/60] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 24/60] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.0min\n",
      "[CV 8/10; 25/60] START learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 25/60] END learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.740 total time=  56.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 6/10; 1/60] START learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 1/60] END learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.768 total time= 1.2min\n",
      "[CV 6/10; 2/60] START learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 2/60] END learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.769 total time= 1.5min\n",
      "[CV 4/10; 3/60] START learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 3/60] END learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.7min\n",
      "[CV 2/10; 4/60] START learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 4/60] END learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.761 total time= 2.0min\n",
      "[CV 9/10; 4/60] START learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 4/60] END learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.746 total time= 2.0min\n",
      "[CV 7/10; 5/60] START learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 5/60] END learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.1min\n",
      "[CV 5/10; 6/60] START learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 6/60] END learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.4min\n",
      "[CV 4/10; 7/60] START learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 7/60] END learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.6min\n",
      "[CV 2/10; 8/60] START learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 8/60] END learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.761 total time= 2.0min\n",
      "[CV 10/10; 8/60] START learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 8/60] END learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.732 total time= 1.9min\n",
      "[CV 2/10; 10/60] START learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 10/60] END learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.762 total time= 1.4min\n",
      "[CV 10/10; 10/60] START learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 10/60] END learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.731 total time= 1.4min\n",
      "[CV 8/10; 11/60] START learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 11/60] END learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.7min\n",
      "[CV 6/10; 12/60] START learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 12/60] END learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.772 total time= 2.0min\n",
      "[CV 4/10; 13/60] START learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 13/60] END learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.741 total time=  59.3s\n",
      "[CV 10/10; 13/60] START learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 13/60] END learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.730 total time=  57.4s\n",
      "[CV 8/10; 14/60] START learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 14/60] END learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.2min\n",
      "[CV 6/10; 15/60] START learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 15/60] END learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.771 total time= 1.5min\n",
      "[CV 4/10; 16/60] START learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 16/60] END learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.7min\n",
      "[CV 2/10; 17/60] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 17/60] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.761 total time= 1.1min\n",
      "[CV 8/10; 17/60] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 17/60] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.1min\n",
      "[CV 5/10; 18/60] START learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 18/60] END learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.4min\n",
      "[CV 3/10; 19/60] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 19/60] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.759 total time= 1.7min\n",
      "[CV 2/10; 20/60] START learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 20/60] END learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.761 total time= 2.0min\n",
      "[CV 10/10; 20/60] START learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 20/60] END learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.9min\n",
      "[CV 2/10; 22/60] START learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 22/60] END learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.762 total time= 1.4min\n",
      "[CV 10/10; 22/60] START learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 22/60] END learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.4min\n",
      "[CV 8/10; 23/60] START learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 23/60] END learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.7min\n",
      "[CV 5/10; 24/60] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 24/60] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.0min\n",
      "[CV 3/10; 25/60] START learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 25/60] END learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.755 total time=  57.2s\n",
      "[CV 9/10; 25/60] START learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 25/60] END learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.742 total time=  55.1s\n",
      "[CV 7/10; 1/60] START learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 1/60] END learning_rate=0.01, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.1min\n",
      "[CV 3/10; 2/60] START learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 2/60] END learning_rate=0.01, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.4min\n",
      "[CV 1/10; 3/60] START learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 3/60] END learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.7min\n",
      "[CV 10/10; 3/60] START learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 3/60] END learning_rate=0.01, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.731 total time= 1.7min\n",
      "[CV 8/10; 4/60] START learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 4/60] END learning_rate=0.01, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.0min\n",
      "[CV 6/10; 5/60] START learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 5/60] END learning_rate=0.01, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.770 total time= 1.1min\n",
      "[CV 3/10; 6/60] START learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 6/60] END learning_rate=0.01, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.4min\n",
      "[CV 2/10; 7/60] START learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 7/60] END learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.762 total time= 1.7min\n",
      "[CV 10/10; 7/60] START learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 7/60] END learning_rate=0.01, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.731 total time= 1.7min\n",
      "[CV 8/10; 8/60] START learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 8/60] END learning_rate=0.01, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.9min\n",
      "[CV 6/10; 9/60] START learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 9/60] END learning_rate=0.01, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.771 total time= 1.1min\n",
      "[CV 4/10; 10/60] START learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 10/60] END learning_rate=0.01, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.4min\n",
      "[CV 2/10; 11/60] START learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 11/60] END learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.762 total time= 1.7min\n",
      "[CV 10/10; 11/60] START learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 11/60] END learning_rate=0.01, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.731 total time= 1.7min\n",
      "[CV 8/10; 12/60] START learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 12/60] END learning_rate=0.01, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.0min\n",
      "[CV 8/10; 13/60] START learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 13/60] END learning_rate=0.02, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.738 total time=  57.7s\n",
      "[CV 6/10; 14/60] START learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 14/60] END learning_rate=0.02, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.770 total time= 1.2min\n",
      "[CV 4/10; 15/60] START learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 15/60] END learning_rate=0.02, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.5min\n",
      "[CV 2/10; 16/60] START learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 16/60] END learning_rate=0.02, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.761 total time= 1.7min\n",
      "[CV 1/10; 17/60] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 17/60] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.1min\n",
      "[CV 6/10; 17/60] START learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 17/60] END learning_rate=0.02, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.771 total time= 1.1min\n",
      "[CV 4/10; 18/60] START learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 18/60] END learning_rate=0.02, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.4min\n",
      "[CV 2/10; 19/60] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 19/60] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.761 total time= 1.7min\n",
      "[CV 10/10; 19/60] START learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 19/60] END learning_rate=0.02, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.6min\n",
      "[CV 8/10; 20/60] START learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 20/60] END learning_rate=0.02, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.9min\n",
      "[CV 8/10; 21/60] START learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 21/60] END learning_rate=0.02, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.1min\n",
      "[CV 6/10; 22/60] START learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 22/60] END learning_rate=0.02, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.773 total time= 1.4min\n",
      "[CV 4/10; 23/60] START learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 23/60] END learning_rate=0.02, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.7min\n",
      "[CV 2/10; 24/60] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 24/60] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.763 total time= 2.0min\n",
      "[CV 10/10; 24/60] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 24/60] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.734 total time= 2.0min\n",
      "[CV 2/10; 26/60] START learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 26/60] END learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.761 total time= 1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 10/10; 25/60] START learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 25/60] END learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.732 total time=  56.3s\n",
      "[CV 8/10; 26/60] START learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 26/60] END learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.1min\n",
      "[CV 6/10; 27/60] START learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 27/60] END learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.772 total time= 1.4min\n",
      "[CV 4/10; 28/60] START learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 28/60] END learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.7min\n",
      "[CV 2/10; 29/60] START learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 29/60] END learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.761 total time= 1.1min\n",
      "[CV 8/10; 29/60] START learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 29/60] END learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.1min\n",
      "[CV 6/10; 30/60] START learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 30/60] END learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.770 total time= 1.3min\n",
      "[CV 4/10; 31/60] START learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 31/60] END learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.6min\n",
      "[CV 2/10; 32/60] START learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 32/60] END learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.761 total time= 1.9min\n",
      "[CV 10/10; 32/60] START learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 32/60] END learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.734 total time= 1.9min\n",
      "[CV 1/10; 34/60] START learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 34/60] END learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.4min\n",
      "[CV 9/10; 34/60] START learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 34/60] END learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.4min\n",
      "[CV 7/10; 35/60] START learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 35/60] END learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.751 total time= 1.7min\n",
      "[CV 5/10; 36/60] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 36/60] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.0min\n",
      "[CV 3/10; 37/60] START learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 37/60] END learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.756 total time=  58.1s\n",
      "[CV 9/10; 37/60] START learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 37/60] END learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.745 total time=  55.4s\n",
      "[CV 7/10; 38/60] START learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 38/60] END learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.2min\n",
      "[CV 5/10; 39/60] START learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 39/60] END learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.4min\n",
      "[CV 4/10; 40/60] START learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 40/60] END learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.6min\n",
      "[CV 1/10; 41/60] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 41/60] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.1min\n",
      "[CV 7/10; 41/60] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 41/60] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.753 total time= 1.1min\n",
      "[CV 5/10; 42/60] START learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 42/60] END learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.4min\n",
      "[CV 3/10; 43/60] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 43/60] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.6min\n",
      "[CV 1/10; 44/60] START learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 44/60] END learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.9min\n",
      "[CV 9/10; 44/60] START learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 44/60] END learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.746 total time= 1.9min\n",
      "[CV 9/10; 45/60] START learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 45/60] END learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.1min\n",
      "[CV 7/10; 46/60] START learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 46/60] END learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.751 total time= 1.4min\n",
      "[CV 5/10; 47/60] START learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 47/60] END learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.7min\n",
      "[CV 4/10; 48/60] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 48/60] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.0min\n",
      "[CV 2/10; 49/60] START learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 49/60] END learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.761 total time=  57.1s\n",
      "[CV 6/10; 49/60] START learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 49/60] END learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.771 total time=  53.8s\n",
      "[CV 1/10; 26/60] START learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 26/60] END learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.2min\n",
      "[CV 9/10; 26/60] START learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 26/60] END learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.1min\n",
      "[CV 7/10; 27/60] START learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 27/60] END learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.753 total time= 1.4min\n",
      "[CV 5/10; 28/60] START learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 28/60] END learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.6min\n",
      "[CV 3/10; 29/60] START learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 29/60] END learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.1min\n",
      "[CV 1/10; 30/60] START learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 30/60] END learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.755 total time= 1.4min\n",
      "[CV 9/10; 30/60] START learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 30/60] END learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.4min\n",
      "[CV 7/10; 31/60] START learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 31/60] END learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.753 total time= 1.6min\n",
      "[CV 5/10; 32/60] START learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 32/60] END learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.9min\n",
      "[CV 3/10; 33/60] START learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 33/60] END learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.1min\n",
      "[CV 9/10; 33/60] START learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 33/60] END learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.2min\n",
      "[CV 8/10; 34/60] START learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 34/60] END learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.4min\n",
      "[CV 6/10; 35/60] START learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 35/60] END learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.774 total time= 1.7min\n",
      "[CV 4/10; 36/60] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 36/60] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.0min\n",
      "[CV 2/10; 37/60] START learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 37/60] END learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.760 total time=  55.5s\n",
      "[CV 5/10; 37/60] START learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 37/60] END learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.742 total time=  55.2s\n",
      "[CV 3/10; 38/60] START learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 38/60] END learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.2min\n",
      "[CV 1/10; 39/60] START learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 39/60] END learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.4min\n",
      "[CV 9/10; 39/60] START learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 39/60] END learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.745 total time= 1.4min\n",
      "[CV 7/10; 40/60] START learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 40/60] END learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.6min\n",
      "[CV 5/10; 41/60] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 41/60] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.1min\n",
      "[CV 3/10; 42/60] START learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 42/60] END learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.4min\n",
      "[CV 1/10; 43/60] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 43/60] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.6min\n",
      "[CV 9/10; 43/60] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 43/60] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.6min\n",
      "[CV 7/10; 44/60] START learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 44/60] END learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.754 total time= 1.9min\n",
      "[CV 6/10; 45/60] START learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 45/60] END learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.770 total time= 1.1min\n",
      "[CV 4/10; 46/60] START learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 46/60] END learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.4min\n",
      "[CV 2/10; 47/60] START learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 47/60] END learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.761 total time= 1.7min\n",
      "[CV 10/10; 47/60] START learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 47/60] END learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.734 total time= 1.7min\n",
      "[CV 8/10; 48/60] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 48/60] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.0min\n",
      "[CV 8/10; 49/60] START learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 49/60] END learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.739 total time=  56.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/10; 25/60] START learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 25/60] END learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.740 total time=  57.2s\n",
      "[CV 4/10; 26/60] START learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 26/60] END learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.2min\n",
      "[CV 2/10; 27/60] START learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 27/60] END learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.760 total time= 1.4min\n",
      "[CV 10/10; 27/60] START learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 27/60] END learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.4min\n",
      "[CV 8/10; 28/60] START learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 28/60] END learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.6min\n",
      "[CV 6/10; 29/60] START learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 29/60] END learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.771 total time= 1.1min\n",
      "[CV 4/10; 30/60] START learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 30/60] END learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.4min\n",
      "[CV 2/10; 31/60] START learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 31/60] END learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.761 total time= 1.7min\n",
      "[CV 1/10; 32/60] START learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 32/60] END learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.755 total time= 1.9min\n",
      "[CV 9/10; 32/60] START learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 32/60] END learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.9min\n",
      "[CV 10/10; 33/60] START learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 33/60] END learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.1min\n",
      "[CV 7/10; 34/60] START learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 34/60] END learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.4min\n",
      "[CV 5/10; 35/60] START learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 35/60] END learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.7min\n",
      "[CV 3/10; 36/60] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 36/60] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.0min\n",
      "[CV 1/10; 37/60] START learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 37/60] END learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.755 total time=  57.0s\n",
      "[CV 6/10; 37/60] START learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 37/60] END learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.771 total time=  54.4s\n",
      "[CV 4/10; 38/60] START learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 38/60] END learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.2min\n",
      "[CV 2/10; 39/60] START learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 39/60] END learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.760 total time= 1.4min\n",
      "[CV 10/10; 39/60] START learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 39/60] END learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.734 total time= 1.4min\n",
      "[CV 8/10; 40/60] START learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 40/60] END learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.6min\n",
      "[CV 6/10; 41/60] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 41/60] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.771 total time= 1.1min\n",
      "[CV 4/10; 42/60] START learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 42/60] END learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.3min\n",
      "[CV 2/10; 43/60] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 43/60] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.763 total time= 1.6min\n",
      "[CV 10/10; 43/60] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 43/60] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.734 total time= 1.6min\n",
      "[CV 8/10; 44/60] START learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 44/60] END learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.9min\n",
      "[CV 7/10; 45/60] START learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 45/60] END learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.751 total time= 1.1min\n",
      "[CV 5/10; 46/60] START learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 46/60] END learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.4min\n",
      "[CV 3/10; 47/60] START learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 47/60] END learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.7min\n",
      "[CV 1/10; 48/60] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 48/60] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.755 total time= 2.0min\n",
      "[CV 9/10; 48/60] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 48/60] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.9min\n",
      "[CV 1/10; 50/60] START learning_rate=0.04, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 50/60] END learning_rate=0.04, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.1min[CV 6/10; 25/60] START learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 25/60] END learning_rate=0.03, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.772 total time=  55.6s\n",
      "[CV 3/10; 26/60] START learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 26/60] END learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.755 total time= 1.2min\n",
      "[CV 1/10; 27/60] START learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 27/60] END learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.4min\n",
      "[CV 9/10; 27/60] START learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 27/60] END learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.3min\n",
      "[CV 7/10; 28/60] START learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 28/60] END learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.753 total time= 1.6min\n",
      "[CV 5/10; 29/60] START learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 29/60] END learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.1min\n",
      "[CV 3/10; 30/60] START learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 30/60] END learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.4min\n",
      "[CV 1/10; 31/60] START learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 31/60] END learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.755 total time= 1.7min\n",
      "[CV 9/10; 31/60] START learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 31/60] END learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.6min\n",
      "[CV 7/10; 32/60] START learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 32/60] END learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.753 total time= 1.9min\n",
      "[CV 6/10; 33/60] START learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 33/60] END learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.773 total time= 1.1min\n",
      "[CV 4/10; 34/60] START learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 34/60] END learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.4min\n",
      "[CV 2/10; 35/60] START learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 35/60] END learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.759 total time= 1.7min\n",
      "[CV 10/10; 35/60] START learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 35/60] END learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.734 total time= 1.7min\n",
      "[CV 8/10; 36/60] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 36/60] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 2.0min\n",
      "[CV 8/10; 37/60] START learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 37/60] END learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.737 total time=  57.8s\n",
      "[CV 6/10; 38/60] START learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 38/60] END learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.771 total time= 1.1min\n",
      "[CV 4/10; 39/60] START learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 39/60] END learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.4min\n",
      "[CV 2/10; 40/60] START learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 40/60] END learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.760 total time= 1.6min\n",
      "[CV 10/10; 40/60] START learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 40/60] END learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.734 total time= 1.6min\n",
      "[CV 10/10; 41/60] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 41/60] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.1min\n",
      "[CV 8/10; 42/60] START learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 42/60] END learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.4min\n",
      "[CV 6/10; 43/60] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 43/60] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.771 total time= 1.6min\n",
      "[CV 4/10; 44/60] START learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 44/60] END learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.9min\n",
      "[CV 2/10; 45/60] START learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 45/60] END learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.762 total time= 1.1min\n",
      "[CV 8/10; 45/60] START learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 45/60] END learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.1min\n",
      "[CV 6/10; 46/60] START learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 46/60] END learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.770 total time= 1.4min\n",
      "[CV 4/10; 47/60] START learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 47/60] END learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.7min\n",
      "[CV 2/10; 48/60] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 48/60] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.761 total time= 2.0min\n",
      "[CV 10/10; 48/60] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 48/60] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.734 total time= 2.0min\n",
      "[CV 2/10; 50/60] START learning_rate=0.04, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 50/60] END learning_rate=0.04, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.761 total time= 1.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 6/10; 26/60] START learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 26/60] END learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.772 total time= 1.2min\n",
      "[CV 4/10; 27/60] START learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 27/60] END learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.4min\n",
      "[CV 2/10; 28/60] START learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 28/60] END learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.760 total time= 1.6min\n",
      "[CV 10/10; 28/60] START learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 28/60] END learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.6min\n",
      "[CV 10/10; 29/60] START learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 29/60] END learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.1min\n",
      "[CV 8/10; 30/60] START learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 30/60] END learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.4min\n",
      "[CV 6/10; 31/60] START learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 31/60] END learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.770 total time= 1.6min\n",
      "[CV 4/10; 32/60] START learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 32/60] END learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.9min\n",
      "[CV 2/10; 33/60] START learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 33/60] END learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.759 total time= 1.1min\n",
      "[CV 7/10; 33/60] START learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 33/60] END learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.1min\n",
      "[CV 5/10; 34/60] START learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 34/60] END learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.4min\n",
      "[CV 3/10; 35/60] START learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 35/60] END learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.7min\n",
      "[CV 1/10; 36/60] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 36/60] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.0min\n",
      "[CV 9/10; 36/60] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 36/60] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.9min\n",
      "[CV 1/10; 38/60] START learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 38/60] END learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.1min\n",
      "[CV 9/10; 38/60] START learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 38/60] END learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.745 total time= 1.1min\n",
      "[CV 7/10; 39/60] START learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 39/60] END learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.750 total time= 1.4min\n",
      "[CV 5/10; 40/60] START learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 40/60] END learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.6min\n",
      "[CV 3/10; 41/60] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 41/60] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.1min\n",
      "[CV 1/10; 42/60] START learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 42/60] END learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.4min\n",
      "[CV 9/10; 42/60] START learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 42/60] END learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.745 total time= 1.3min\n",
      "[CV 7/10; 43/60] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 43/60] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.754 total time= 1.6min\n",
      "[CV 5/10; 44/60] START learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 44/60] END learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.9min\n",
      "[CV 3/10; 45/60] START learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 45/60] END learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.1min\n",
      "[CV 10/10; 45/60] START learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 45/60] END learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.734 total time= 1.2min\n",
      "[CV 8/10; 46/60] START learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 46/60] END learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.4min\n",
      "[CV 6/10; 47/60] START learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 47/60] END learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.770 total time= 1.7min\n",
      "[CV 3/10; 48/60] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 48/60] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.756 total time= 2.0min\n",
      "[CV 1/10; 49/60] START learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 49/60] END learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.757 total time=  55.6s\n",
      "[CV 5/10; 49/60] START learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 49/60] END learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.736 total time=  53.5s\n",
      "[CV 3/10; 50/60] START learning_rate=0.04, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 50/60] END learning_rate=0.04, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.2min\n",
      "[CV 5/10; 26/60] START learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 26/60] END learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.2min\n",
      "[CV 3/10; 27/60] START learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 27/60] END learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.755 total time= 1.4min\n",
      "[CV 1/10; 28/60] START learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 28/60] END learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.6min\n",
      "[CV 9/10; 28/60] START learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 28/60] END learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.6min\n",
      "[CV 9/10; 29/60] START learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 29/60] END learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.1min\n",
      "[CV 7/10; 30/60] START learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 30/60] END learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.752 total time= 1.3min\n",
      "[CV 5/10; 31/60] START learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 31/60] END learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.6min\n",
      "[CV 3/10; 32/60] START learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 32/60] END learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.9min\n",
      "[CV 1/10; 33/60] START learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 33/60] END learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.1min\n",
      "[CV 5/10; 33/60] START learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 33/60] END learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.1min\n",
      "[CV 3/10; 34/60] START learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 34/60] END learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.4min\n",
      "[CV 1/10; 35/60] START learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 35/60] END learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.7min\n",
      "[CV 9/10; 35/60] START learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 35/60] END learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.7min\n",
      "[CV 7/10; 36/60] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 36/60] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.751 total time= 2.0min\n",
      "[CV 7/10; 37/60] START learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 37/60] END learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.749 total time=  54.6s\n",
      "[CV 5/10; 38/60] START learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 38/60] END learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.2min\n",
      "[CV 3/10; 39/60] START learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 39/60] END learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.4min\n",
      "[CV 1/10; 40/60] START learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 40/60] END learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.6min\n",
      "[CV 9/10; 40/60] START learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 40/60] END learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.745 total time= 1.6min\n",
      "[CV 9/10; 41/60] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 41/60] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.745 total time= 1.1min\n",
      "[CV 7/10; 42/60] START learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 42/60] END learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.753 total time= 1.4min\n",
      "[CV 5/10; 43/60] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 43/60] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.6min\n",
      "[CV 3/10; 44/60] START learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 44/60] END learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 1.9min\n",
      "[CV 1/10; 45/60] START learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 45/60] END learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.755 total time= 1.1min\n",
      "[CV 5/10; 45/60] START learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 45/60] END learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.1min\n",
      "[CV 3/10; 46/60] START learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 46/60] END learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.4min\n",
      "[CV 1/10; 47/60] START learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 47/60] END learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.755 total time= 1.7min\n",
      "[CV 9/10; 47/60] START learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 47/60] END learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.7min\n",
      "[CV 7/10; 48/60] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 48/60] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.752 total time= 2.0min\n",
      "[CV 7/10; 49/60] START learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 49/60] END learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.750 total time=  53.3s\n",
      "[CV 5/10; 50/60] START learning_rate=0.04, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 50/60] END learning_rate=0.04, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.736 total time= 1.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 7/10; 26/60] START learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 26/60] END learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.753 total time= 1.2min\n",
      "[CV 5/10; 27/60] START learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 27/60] END learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.4min\n",
      "[CV 3/10; 28/60] START learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 28/60] END learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.756 total time= 1.6min\n",
      "[CV 1/10; 29/60] START learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 29/60] END learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.754 total time= 1.1min\n",
      "[CV 7/10; 29/60] START learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 29/60] END learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.752 total time= 1.1min\n",
      "[CV 5/10; 30/60] START learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 30/60] END learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.4min\n",
      "[CV 3/10; 31/60] START learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 31/60] END learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.6min\n",
      "[CV 10/10; 31/60] START learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 31/60] END learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.734 total time= 1.6min\n",
      "[CV 8/10; 32/60] START learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 32/60] END learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.9min\n",
      "[CV 8/10; 33/60] START learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 33/60] END learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.1min\n",
      "[CV 6/10; 34/60] START learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 34/60] END learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.774 total time= 1.4min\n",
      "[CV 4/10; 35/60] START learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 35/60] END learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.7min\n",
      "[CV 2/10; 36/60] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 36/60] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.759 total time= 2.0min\n",
      "[CV 10/10; 36/60] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 36/60] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.734 total time= 2.0min\n",
      "[CV 2/10; 38/60] START learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 38/60] END learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.760 total time= 1.1min\n",
      "[CV 10/10; 38/60] START learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 38/60] END learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.1min\n",
      "[CV 8/10; 39/60] START learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 39/60] END learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.4min\n",
      "[CV 6/10; 40/60] START learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 40/60] END learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.771 total time= 1.6min\n",
      "[CV 4/10; 41/60] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 41/60] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.1min\n",
      "[CV 2/10; 42/60] START learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 42/60] END learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.763 total time= 1.4min\n",
      "[CV 10/10; 42/60] START learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 42/60] END learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.4min\n",
      "[CV 8/10; 43/60] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 43/60] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 1.7min\n",
      "[CV 6/10; 44/60] START learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 44/60] END learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.771 total time= 1.9min\n",
      "[CV 4/10; 45/60] START learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 45/60] END learning_rate=0.035, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.1min\n",
      "[CV 2/10; 46/60] START learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 46/60] END learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.761 total time= 1.4min\n",
      "[CV 10/10; 46/60] START learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 46/60] END learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.734 total time= 1.4min\n",
      "[CV 8/10; 47/60] START learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 47/60] END learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.7min\n",
      "[CV 6/10; 48/60] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 48/60] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.770 total time= 2.0min\n",
      "[CV 3/10; 49/60] START learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 49/60] END learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.758 total time=  56.5s\n",
      "[CV 9/10; 49/60] START learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 49/60] END learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time=  54.1s\n",
      "[CV 7/10; 50/60] START learning_rate=0.04, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 50/60] END learning_rate=0.04, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.751 total time= 1.1min\n",
      "[CV 10/10; 26/60] START learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 26/60] END learning_rate=0.03, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.732 total time= 1.2min\n",
      "[CV 8/10; 27/60] START learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 27/60] END learning_rate=0.03, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 1.4min\n",
      "[CV 6/10; 28/60] START learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 28/60] END learning_rate=0.03, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.772 total time= 1.6min\n",
      "[CV 4/10; 29/60] START learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 29/60] END learning_rate=0.03, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 1.1min\n",
      "[CV 2/10; 30/60] START learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 30/60] END learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.761 total time= 1.4min\n",
      "[CV 10/10; 30/60] START learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 30/60] END learning_rate=0.03, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.4min\n",
      "[CV 8/10; 31/60] START learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 31/60] END learning_rate=0.03, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.6min\n",
      "[CV 6/10; 32/60] START learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 32/60] END learning_rate=0.03, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.770 total time= 1.9min\n",
      "[CV 4/10; 33/60] START learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 33/60] END learning_rate=0.03, max_depth=18, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 1.1min\n",
      "[CV 2/10; 34/60] START learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 34/60] END learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.759 total time= 1.4min\n",
      "[CV 10/10; 34/60] START learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 34/60] END learning_rate=0.03, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.733 total time= 1.4min\n",
      "[CV 8/10; 35/60] START learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 35/60] END learning_rate=0.03, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.7min\n",
      "[CV 6/10; 36/60] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 36/60] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.774 total time= 2.0min\n",
      "[CV 4/10; 37/60] START learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 37/60] END learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.740 total time=  56.7s\n",
      "[CV 10/10; 37/60] START learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 37/60] END learning_rate=0.035, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.733 total time=  55.4s\n",
      "[CV 8/10; 38/60] START learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 38/60] END learning_rate=0.035, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.2min\n",
      "[CV 6/10; 39/60] START learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 39/60] END learning_rate=0.035, max_depth=12, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.771 total time= 1.4min\n",
      "[CV 3/10; 40/60] START learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 40/60] END learning_rate=0.035, max_depth=12, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 1.6min\n",
      "[CV 2/10; 41/60] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 41/60] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.763 total time= 1.1min\n",
      "[CV 8/10; 41/60] START learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 41/60] END learning_rate=0.035, max_depth=15, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.738 total time= 1.1min\n",
      "[CV 6/10; 42/60] START learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 42/60] END learning_rate=0.035, max_depth=15, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.771 total time= 1.4min\n",
      "[CV 4/10; 43/60] START learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 43/60] END learning_rate=0.035, max_depth=15, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 1.6min\n",
      "[CV 2/10; 44/60] START learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 44/60] END learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.763 total time= 1.9min\n",
      "[CV 10/10; 44/60] START learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 44/60] END learning_rate=0.035, max_depth=15, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.734 total time= 1.9min\n",
      "[CV 1/10; 46/60] START learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 46/60] END learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.755 total time= 1.4min\n",
      "[CV 9/10; 46/60] START learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 46/60] END learning_rate=0.035, max_depth=18, n_estimators=2500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 1.4min\n",
      "[CV 7/10; 47/60] START learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 47/60] END learning_rate=0.035, max_depth=18, n_estimators=3000, num_leaves=124, objective=regression_l1;, score=0.752 total time= 1.7min\n",
      "[CV 5/10; 48/60] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 48/60] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.0min\n",
      "[CV 4/10; 49/60] START learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 49/60] END learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.743 total time=  55.6s\n",
      "[CV 10/10; 49/60] START learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 49/60] END learning_rate=0.04, max_depth=12, n_estimators=2000, num_leaves=124, objective=regression_l1;, score=0.728 total time=  54.4s\n",
      "[CV 8/10; 50/60] START learning_rate=0.04, max_depth=12, n_estimators=2500, num_leaves=124, objective=regression_l1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator across ALL searched params:\n",
      " LGBMRegressor(learning_rate=0.04, max_depth=18, n_estimators=3500,\n",
      "              num_leaves=124, objective='regression_l1', random_state=420)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "lgbm = lightgbm.LGBMRegressor(random_state = 420, n_jobs = -1)\n",
    "\n",
    "# Using many estimators with low learning rates\n",
    "lgbm_params = {'learning_rate': [0.01, 0.02, 0.03, 0.035, 0.04],\n",
    "                           'num_leaves':[124],\n",
    "                           'max_depth': [12, 15, 18],\n",
    "                           'n_estimators': [2000, 2500, 3000, 3500],\n",
    "                           'objective': ['regression_l1']}\n",
    "\n",
    "lgbm_grid_search = GridSearchCV(estimator = lgbm, \n",
    "                                param_grid =  lgbm_params,\n",
    "                                cv=10,\n",
    "                                verbose = 10,\n",
    "                                scoring = make_scorer(nrmsle, greater_is_better=True),\n",
    "                                n_jobs=-1)\n",
    "\n",
    "lgbm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"The best estimator across ALL searched params:\\n\", lgbm_grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator across ALL searched params:\n",
      " LGBMRegressor(learning_rate=0.04, max_depth=18, n_estimators=3500,\n",
      "              num_leaves=124, objective='regression_l1', random_state=420)\n",
      "The best score of all model parameters' combination on model: 0.7504\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>145.201953</td>\n",
       "      <td>2.303159</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750436</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>110.909664</td>\n",
       "      <td>2.936009</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.750431</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>180.215694</td>\n",
       "      <td>4.261945</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750385</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>116.052360</td>\n",
       "      <td>2.882599</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750298</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>122.738013</td>\n",
       "      <td>3.598641</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750270</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100.178908</td>\n",
       "      <td>2.451278</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750217</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>115.866784</td>\n",
       "      <td>2.889992</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750179</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>114.961685</td>\n",
       "      <td>2.815270</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750153</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>77.106095</td>\n",
       "      <td>1.979178</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750147</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>99.901606</td>\n",
       "      <td>2.526900</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750116</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>79.413887</td>\n",
       "      <td>2.084609</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.750110</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>83.542163</td>\n",
       "      <td>2.075406</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750083</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>83.701663</td>\n",
       "      <td>2.054589</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750039</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>63.228236</td>\n",
       "      <td>1.676717</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.750035</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>112.345064</td>\n",
       "      <td>2.847590</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.750003</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>99.506546</td>\n",
       "      <td>2.279563</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.749902</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>97.118326</td>\n",
       "      <td>2.514841</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749898</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>94.763789</td>\n",
       "      <td>2.487765</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 15, 'n_e...</td>\n",
       "      <td>0.749892</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>66.268104</td>\n",
       "      <td>1.653173</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.749880</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>80.428974</td>\n",
       "      <td>2.098642</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749843</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>66.060577</td>\n",
       "      <td>1.631059</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.749843</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>119.378372</td>\n",
       "      <td>3.239143</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749828</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>110.851489</td>\n",
       "      <td>2.858512</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749802</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>95.474656</td>\n",
       "      <td>2.513590</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749778</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117.819822</td>\n",
       "      <td>2.618526</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 12, 'n_es...</td>\n",
       "      <td>0.749769</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>82.869197</td>\n",
       "      <td>1.968594</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.749745</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>102.156063</td>\n",
       "      <td>2.803115</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749741</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>66.497464</td>\n",
       "      <td>1.640828</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.749718</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>79.890959</td>\n",
       "      <td>2.159251</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749712</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>63.885615</td>\n",
       "      <td>1.626668</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749705</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>63.651144</td>\n",
       "      <td>1.744866</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749675</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>111.953749</td>\n",
       "      <td>2.817649</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>84.649053</td>\n",
       "      <td>2.186925</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749666</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.202982</td>\n",
       "      <td>2.206119</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 12, 'n_es...</td>\n",
       "      <td>0.749589</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>96.099143</td>\n",
       "      <td>2.451528</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749569</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>83.512935</td>\n",
       "      <td>2.093437</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 18, 'n_e...</td>\n",
       "      <td>0.749545</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>115.585091</td>\n",
       "      <td>2.878207</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 18, 'n_e...</td>\n",
       "      <td>0.749511</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>100.046384</td>\n",
       "      <td>2.486149</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 18, 'n_e...</td>\n",
       "      <td>0.749502</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>79.325160</td>\n",
       "      <td>1.844816</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749498</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>65.721596</td>\n",
       "      <td>1.665579</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 18, 'n_e...</td>\n",
       "      <td>0.749457</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83.829462</td>\n",
       "      <td>1.751798</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 12, 'n_es...</td>\n",
       "      <td>0.749404</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>94.909549</td>\n",
       "      <td>2.757930</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 12, 'n_es...</td>\n",
       "      <td>0.749359</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>79.511614</td>\n",
       "      <td>2.088632</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749356</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>81.505664</td>\n",
       "      <td>2.395935</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 12, 'n_es...</td>\n",
       "      <td>0.749313</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67.439467</td>\n",
       "      <td>1.674569</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 15, 'n_es...</td>\n",
       "      <td>0.749289</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>68.752543</td>\n",
       "      <td>2.001228</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 12, 'n_es...</td>\n",
       "      <td>0.749238</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>55.103906</td>\n",
       "      <td>1.693349</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 12, 'n_es...</td>\n",
       "      <td>0.749167</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>93.254518</td>\n",
       "      <td>2.748918</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 12, 'n_e...</td>\n",
       "      <td>0.749102</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>80.235450</td>\n",
       "      <td>2.414725</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 12, 'n_e...</td>\n",
       "      <td>0.749067</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>69.801969</td>\n",
       "      <td>1.971079</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 12, 'n_es...</td>\n",
       "      <td>0.748980</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>67.112667</td>\n",
       "      <td>2.060917</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 12, 'n_e...</td>\n",
       "      <td>0.748968</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66.664784</td>\n",
       "      <td>1.440929</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 12, 'n_es...</td>\n",
       "      <td>0.748968</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>54.345803</td>\n",
       "      <td>1.659283</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 12, 'n_e...</td>\n",
       "      <td>0.748876</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>123.658730</td>\n",
       "      <td>3.244359</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 12, 'n_es...</td>\n",
       "      <td>0.748803</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>93.231814</td>\n",
       "      <td>2.408779</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 12, 'n_es...</td>\n",
       "      <td>0.748801</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>66.493154</td>\n",
       "      <td>2.153547</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 12, 'n_es...</td>\n",
       "      <td>0.748735</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>53.268963</td>\n",
       "      <td>1.723130</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 12, 'n_es...</td>\n",
       "      <td>0.748642</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>56.236141</td>\n",
       "      <td>1.615087</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 12, 'n_es...</td>\n",
       "      <td>0.748601</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>96.558314</td>\n",
       "      <td>2.830725</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 12, 'n_es...</td>\n",
       "      <td>0.748115</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>83.337355</td>\n",
       "      <td>2.463004</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 12, 'n_es...</td>\n",
       "      <td>0.745823</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  \\\n",
       "59     145.201953         2.303159   \n",
       "43     110.909664         2.936009   \n",
       "58     180.215694         4.261945   \n",
       "23     116.052360         2.882599   \n",
       "57     122.738013         3.598641   \n",
       "22     100.178908         2.451278   \n",
       "35     115.866784         2.889992   \n",
       "11     114.961685         2.815270   \n",
       "56      77.106095         1.979178   \n",
       "34      99.901606         2.526900   \n",
       "41      79.413887         2.084609   \n",
       "21      83.542163         2.075406   \n",
       "33      83.701663         2.054589   \n",
       "40      63.228236         1.676717   \n",
       "19     112.345064         2.847590   \n",
       "10      99.506546         2.279563   \n",
       "18      97.118326         2.514841   \n",
       "42      94.763789         2.487765   \n",
       "32      66.268104         1.653173   \n",
       "17      80.428974         2.098642   \n",
       "20      66.060577         1.631059   \n",
       "55     119.378372         3.239143   \n",
       "31     110.851489         2.858512   \n",
       "30      95.474656         2.513590   \n",
       "3      117.819822         2.618526   \n",
       "9       82.869197         1.968594   \n",
       "54     102.156063         2.803115   \n",
       "8       66.497464         1.640828   \n",
       "29      79.890959         2.159251   \n",
       "16      63.885615         1.626668   \n",
       "28      63.651144         1.744866   \n",
       "7      111.953749         2.817649   \n",
       "53      84.649053         2.186925   \n",
       "2      100.202982         2.206119   \n",
       "6       96.099143         2.451528   \n",
       "45      83.512935         2.093437   \n",
       "47     115.585091         2.878207   \n",
       "46     100.046384         2.486149   \n",
       "52      79.325160         1.844816   \n",
       "44      65.721596         1.665579   \n",
       "1       83.829462         1.751798   \n",
       "27      94.909549         2.757930   \n",
       "5       79.511614         2.088632   \n",
       "26      81.505664         2.395935   \n",
       "4       67.439467         1.674569   \n",
       "25      68.752543         2.001228   \n",
       "24      55.103906         1.693349   \n",
       "39      93.254518         2.748918   \n",
       "38      80.235450         2.414725   \n",
       "13      69.801969         1.971079   \n",
       "37      67.112667         2.060917   \n",
       "0       66.664784         1.440929   \n",
       "36      54.345803         1.659283   \n",
       "51     123.658730         3.244359   \n",
       "50      93.231814         2.408779   \n",
       "49      66.493154         2.153547   \n",
       "48      53.268963         1.723130   \n",
       "12      56.236141         1.615087   \n",
       "15      96.558314         2.830725   \n",
       "14      83.337355         2.463004   \n",
       "\n",
       "                                               params  mean_test_score  \\\n",
       "59  {'learning_rate': 0.04, 'max_depth': 18, 'n_es...         0.750436   \n",
       "43  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.750431   \n",
       "58  {'learning_rate': 0.04, 'max_depth': 18, 'n_es...         0.750385   \n",
       "23  {'learning_rate': 0.02, 'max_depth': 18, 'n_es...         0.750298   \n",
       "57  {'learning_rate': 0.04, 'max_depth': 18, 'n_es...         0.750270   \n",
       "22  {'learning_rate': 0.02, 'max_depth': 18, 'n_es...         0.750217   \n",
       "35  {'learning_rate': 0.03, 'max_depth': 18, 'n_es...         0.750179   \n",
       "11  {'learning_rate': 0.01, 'max_depth': 18, 'n_es...         0.750153   \n",
       "56  {'learning_rate': 0.04, 'max_depth': 18, 'n_es...         0.750147   \n",
       "34  {'learning_rate': 0.03, 'max_depth': 18, 'n_es...         0.750116   \n",
       "41  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.750110   \n",
       "21  {'learning_rate': 0.02, 'max_depth': 18, 'n_es...         0.750083   \n",
       "33  {'learning_rate': 0.03, 'max_depth': 18, 'n_es...         0.750039   \n",
       "40  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.750035   \n",
       "19  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.750003   \n",
       "10  {'learning_rate': 0.01, 'max_depth': 18, 'n_es...         0.749902   \n",
       "18  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.749898   \n",
       "42  {'learning_rate': 0.035, 'max_depth': 15, 'n_e...         0.749892   \n",
       "32  {'learning_rate': 0.03, 'max_depth': 18, 'n_es...         0.749880   \n",
       "17  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.749843   \n",
       "20  {'learning_rate': 0.02, 'max_depth': 18, 'n_es...         0.749843   \n",
       "55  {'learning_rate': 0.04, 'max_depth': 15, 'n_es...         0.749828   \n",
       "31  {'learning_rate': 0.03, 'max_depth': 15, 'n_es...         0.749802   \n",
       "30  {'learning_rate': 0.03, 'max_depth': 15, 'n_es...         0.749778   \n",
       "3   {'learning_rate': 0.01, 'max_depth': 12, 'n_es...         0.749769   \n",
       "9   {'learning_rate': 0.01, 'max_depth': 18, 'n_es...         0.749745   \n",
       "54  {'learning_rate': 0.04, 'max_depth': 15, 'n_es...         0.749741   \n",
       "8   {'learning_rate': 0.01, 'max_depth': 18, 'n_es...         0.749718   \n",
       "29  {'learning_rate': 0.03, 'max_depth': 15, 'n_es...         0.749712   \n",
       "16  {'learning_rate': 0.02, 'max_depth': 15, 'n_es...         0.749705   \n",
       "28  {'learning_rate': 0.03, 'max_depth': 15, 'n_es...         0.749675   \n",
       "7   {'learning_rate': 0.01, 'max_depth': 15, 'n_es...         0.749672   \n",
       "53  {'learning_rate': 0.04, 'max_depth': 15, 'n_es...         0.749666   \n",
       "2   {'learning_rate': 0.01, 'max_depth': 12, 'n_es...         0.749589   \n",
       "6   {'learning_rate': 0.01, 'max_depth': 15, 'n_es...         0.749569   \n",
       "45  {'learning_rate': 0.035, 'max_depth': 18, 'n_e...         0.749545   \n",
       "47  {'learning_rate': 0.035, 'max_depth': 18, 'n_e...         0.749511   \n",
       "46  {'learning_rate': 0.035, 'max_depth': 18, 'n_e...         0.749502   \n",
       "52  {'learning_rate': 0.04, 'max_depth': 15, 'n_es...         0.749498   \n",
       "44  {'learning_rate': 0.035, 'max_depth': 18, 'n_e...         0.749457   \n",
       "1   {'learning_rate': 0.01, 'max_depth': 12, 'n_es...         0.749404   \n",
       "27  {'learning_rate': 0.03, 'max_depth': 12, 'n_es...         0.749359   \n",
       "5   {'learning_rate': 0.01, 'max_depth': 15, 'n_es...         0.749356   \n",
       "26  {'learning_rate': 0.03, 'max_depth': 12, 'n_es...         0.749313   \n",
       "4   {'learning_rate': 0.01, 'max_depth': 15, 'n_es...         0.749289   \n",
       "25  {'learning_rate': 0.03, 'max_depth': 12, 'n_es...         0.749238   \n",
       "24  {'learning_rate': 0.03, 'max_depth': 12, 'n_es...         0.749167   \n",
       "39  {'learning_rate': 0.035, 'max_depth': 12, 'n_e...         0.749102   \n",
       "38  {'learning_rate': 0.035, 'max_depth': 12, 'n_e...         0.749067   \n",
       "13  {'learning_rate': 0.02, 'max_depth': 12, 'n_es...         0.748980   \n",
       "37  {'learning_rate': 0.035, 'max_depth': 12, 'n_e...         0.748968   \n",
       "0   {'learning_rate': 0.01, 'max_depth': 12, 'n_es...         0.748968   \n",
       "36  {'learning_rate': 0.035, 'max_depth': 12, 'n_e...         0.748876   \n",
       "51  {'learning_rate': 0.04, 'max_depth': 12, 'n_es...         0.748803   \n",
       "50  {'learning_rate': 0.04, 'max_depth': 12, 'n_es...         0.748801   \n",
       "49  {'learning_rate': 0.04, 'max_depth': 12, 'n_es...         0.748735   \n",
       "48  {'learning_rate': 0.04, 'max_depth': 12, 'n_es...         0.748642   \n",
       "12  {'learning_rate': 0.02, 'max_depth': 12, 'n_es...         0.748601   \n",
       "15  {'learning_rate': 0.02, 'max_depth': 12, 'n_es...         0.748115   \n",
       "14  {'learning_rate': 0.02, 'max_depth': 12, 'n_es...         0.745823   \n",
       "\n",
       "    rank_test_score  \n",
       "59                1  \n",
       "43                2  \n",
       "58                3  \n",
       "23                4  \n",
       "57                5  \n",
       "22                6  \n",
       "35                7  \n",
       "11                8  \n",
       "56                9  \n",
       "34               10  \n",
       "41               11  \n",
       "21               12  \n",
       "33               13  \n",
       "40               14  \n",
       "19               15  \n",
       "10               16  \n",
       "18               17  \n",
       "42               18  \n",
       "32               19  \n",
       "17               20  \n",
       "20               21  \n",
       "55               22  \n",
       "31               23  \n",
       "30               24  \n",
       "3                25  \n",
       "9                26  \n",
       "54               27  \n",
       "8                28  \n",
       "29               29  \n",
       "16               30  \n",
       "28               31  \n",
       "7                32  \n",
       "53               33  \n",
       "2                34  \n",
       "6                35  \n",
       "45               36  \n",
       "47               37  \n",
       "46               38  \n",
       "52               39  \n",
       "44               40  \n",
       "1                41  \n",
       "27               42  \n",
       "5                43  \n",
       "26               44  \n",
       "4                45  \n",
       "25               46  \n",
       "24               47  \n",
       "39               48  \n",
       "38               49  \n",
       "13               50  \n",
       "37               51  \n",
       "0                52  \n",
       "36               53  \n",
       "51               54  \n",
       "50               55  \n",
       "49               56  \n",
       "48               57  \n",
       "12               58  \n",
       "15               59  \n",
       "14               60  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The best estimator across ALL searched params:\\n\", lgbm_grid_search.best_estimator_)\n",
    "\n",
    "print(\"The best score of all model parameters' combination on model: {:.4f}\".format(lgbm_grid_search.best_score_))\n",
    "display(pd.DataFrame(lgbm_grid_search.cv_results_).sort_values(by=['rank_test_score'])[['mean_fit_time', 'mean_score_time', \n",
    "                                           'params', 'mean_test_score', 'rank_test_score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgbm_grid_search.best_estimator_\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='Feature importance', ylabel='Features'>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1130.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(font_scale = 1)\n",
    "lightgbm.plot_importance(model, height=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving-exporting the results to a .csv file\n",
    "pd.DataFrame(predictions, columns = ['Price']).to_csv('submission_lgbm_best.csv', index=False)\n",
    "# 0.75984"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More tight grid-search for the speicific model based on the results of the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n",
      "[CV 8/10; 1/36] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 1/36] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.4min\n",
      "[CV 2/10; 2/36] START learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 2/36] END learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.763 total time= 2.6min\n",
      "[CV 9/10; 2/36] START learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 2/36] END learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.3min\n",
      "[CV 7/10; 3/36] START learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 3/36] END learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.751 total time= 2.3min\n",
      "[CV 7/10; 4/36] START learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 4/36] END learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.752 total time= 2.1min\n",
      "[CV 6/10; 5/36] START learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 5/36] END learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.771 total time= 2.2min\n",
      "[CV 3/10; 6/36] START learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 6/36] END learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.4min\n",
      "[CV 2/10; 7/36] START learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 7/36] END learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.761 total time= 2.1min\n",
      "[CV 8/10; 7/36] START learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 7/36] END learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 2.1min\n",
      "[CV 6/10; 8/36] START learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 8/36] END learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.771 total time= 2.2min\n",
      "[CV 3/10; 9/36] START learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 9/36] END learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 2.9min\n",
      "[CV 2/10; 10/36] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 10/36] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.759 total time= 2.8min\n",
      "[CV 8/10; 10/36] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 10/36] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 2.6min\n",
      "[CV 7/10; 11/36] START learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 11/36] END learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.751 total time= 3.0min\n",
      "[CV 5/10; 12/36] START learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 12/36] END learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 3.3min\n",
      "[CV 2/10; 13/36] START learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 13/36] END learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.763 total time= 2.7min\n",
      "[CV 1/10; 14/36] START learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 14/36] END learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.757 total time= 2.7min\n",
      "[CV 9/10; 14/36] START learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 14/36] END learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.744 total time= 2.5min\n",
      "[CV 8/10; 15/36] START learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 15/36] END learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.739 total time= 2.5min\n",
      "[CV 6/10; 16/36] START learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 16/36] END learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.773 total time= 2.7min\n",
      "[CV 3/10; 17/36] START learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 17/36] END learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.755 total time= 3.1min\n",
      "[CV 1/10; 18/36] START learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 18/36] END learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.754 total time= 2.9min\n",
      "[CV 9/10; 18/36] START learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 18/36] END learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 2.8min\n",
      "[CV 7/10; 19/36] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 19/36] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.752 total time= 2.7min\n",
      "[CV 5/10; 20/36] START learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 20/36] END learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.5min\n",
      "[CV 3/10; 21/36] START learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 21/36] END learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 2.9min\n",
      "[CV 1/10; 22/36] START learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 22/36] END learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.756 total time= 2.1min\n",
      "[CV 8/10; 22/36] START learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 22/36] END learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.3min\n",
      "[CV 6/10; 23/36] START learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 23/36] END learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.771 total time= 2.7min\n",
      "[CV 4/10; 24/36] START learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 24/36] END learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.3min\n",
      "[CV 1/10; 25/36] START learning_rate=0.035, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 25/36] END learning_rate=0.035, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 2.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 7/10; 1/36] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 1/36] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.752 total time= 2.4min\n",
      "[CV 4/10; 2/36] START learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 2/36] END learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.741 total time= 2.6min\n",
      "[CV 3/10; 3/36] START learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 3/36] END learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.3min\n",
      "[CV 9/10; 3/36] START learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 3/36] END learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.3min\n",
      "[CV 9/10; 4/36] START learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 4/36] END learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.0min\n",
      "[CV 7/10; 5/36] START learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 5/36] END learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.752 total time= 2.2min\n",
      "[CV 6/10; 6/36] START learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 6/36] END learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.771 total time= 2.3min\n",
      "[CV 3/10; 7/36] START learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 7/36] END learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.756 total time= 2.1min\n",
      "[CV 10/10; 7/36] START learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 7/36] END learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.733 total time= 2.1min\n",
      "[CV 8/10; 8/36] START learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 8/36] END learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.741 total time= 2.2min\n",
      "[CV 5/10; 9/36] START learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 9/36] END learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 3.0min\n",
      "[CV 3/10; 10/36] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 10/36] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.8min\n",
      "[CV 9/10; 10/36] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 10/36] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.744 total time= 2.5min\n",
      "[CV 6/10; 11/36] START learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 11/36] END learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.774 total time= 3.0min\n",
      "[CV 3/10; 12/36] START learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 12/36] END learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 3.3min\n",
      "[CV 1/10; 13/36] START learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 13/36] END learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 2.7min\n",
      "[CV 9/10; 13/36] START learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 13/36] END learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.744 total time= 2.4min\n",
      "[CV 7/10; 14/36] START learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 14/36] END learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.752 total time= 2.6min\n",
      "[CV 5/10; 15/36] START learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 15/36] END learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.4min\n",
      "[CV 3/10; 16/36] START learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 16/36] END learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.755 total time= 2.8min\n",
      "[CV 1/10; 17/36] START learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 17/36] END learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.754 total time= 3.0min\n",
      "[CV 9/10; 17/36] START learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 17/36] END learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.744 total time= 2.8min\n",
      "[CV 7/10; 18/36] START learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 18/36] END learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.752 total time= 2.8min\n",
      "[CV 5/10; 19/36] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 19/36] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.6min\n",
      "[CV 3/10; 20/36] START learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 20/36] END learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.756 total time= 2.6min\n",
      "[CV 1/10; 21/36] START learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 21/36] END learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.755 total time= 2.9min\n",
      "[CV 9/10; 21/36] START learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 21/36] END learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 2.4min\n",
      "[CV 7/10; 22/36] START learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 22/36] END learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.752 total time= 2.3min\n",
      "[CV 5/10; 23/36] START learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 23/36] END learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.7min\n",
      "[CV 3/10; 24/36] START learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 24/36] END learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.759 total time= 2.3min\n",
      "[CV 2/10; 25/36] START learning_rate=0.035, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 25/36] END learning_rate=0.035, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.761 total time= 2.3min\n",
      "[CV 4/10; 1/36] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 1/36] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 2.4min\n",
      "[CV 5/10; 2/36] START learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 2/36] END learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.6min\n",
      "[CV 4/10; 3/36] START learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 3/36] END learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 2.4min\n",
      "[CV 2/10; 4/36] START learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 4/36] END learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.764 total time= 2.1min\n",
      "[CV 6/10; 4/36] START learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 4/36] END learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.771 total time= 2.1min\n",
      "[CV 4/10; 5/36] START learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 5/36] END learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.2min\n",
      "[CV 2/10; 6/36] START learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 6/36] END learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.764 total time= 2.4min\n",
      "[CV 10/10; 6/36] START learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 6/36] END learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.732 total time= 2.4min\n",
      "[CV 2/10; 8/36] START learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 8/36] END learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.761 total time= 2.2min\n",
      "[CV 10/10; 8/36] START learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 8/36] END learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.733 total time= 2.2min\n",
      "[CV 8/10; 9/36] START learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 9/36] END learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 3.0min\n",
      "[CV 6/10; 10/36] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 10/36] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.774 total time= 2.8min\n",
      "[CV 3/10; 11/36] START learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 11/36] END learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.6min\n",
      "[CV 1/10; 12/36] START learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 12/36] END learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 3.4min\n",
      "[CV 9/10; 12/36] START learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 12/36] END learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 3.2min\n",
      "[CV 7/10; 13/36] START learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 13/36] END learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.752 total time= 2.5min\n",
      "[CV 5/10; 14/36] START learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 14/36] END learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.7min\n",
      "[CV 3/10; 15/36] START learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 15/36] END learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.5min\n",
      "[CV 1/10; 16/36] START learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 16/36] END learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.754 total time= 2.2min\n",
      "[CV 8/10; 16/36] START learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 16/36] END learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.741 total time= 2.7min\n",
      "[CV 5/10; 17/36] START learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 17/36] END learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.740 total time= 3.1min\n",
      "[CV 3/10; 18/36] START learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 18/36] END learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.755 total time= 2.9min\n",
      "[CV 1/10; 19/36] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 19/36] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.755 total time= 2.4min\n",
      "[CV 8/10; 19/36] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 19/36] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.7min\n",
      "[CV 6/10; 20/36] START learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 20/36] END learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.770 total time= 2.5min\n",
      "[CV 4/10; 21/36] START learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 21/36] END learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 3.0min\n",
      "[CV 2/10; 22/36] START learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 22/36] END learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.762 total time= 2.2min\n",
      "[CV 10/10; 22/36] START learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 22/36] END learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.731 total time= 2.2min\n",
      "[CV 8/10; 23/36] START learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 23/36] END learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.7min\n",
      "[CV 5/10; 24/36] START learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 24/36] END learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.3min\n",
      "[CV 3/10; 25/36] START learning_rate=0.035, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 25/36] END learning_rate=0.035, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/10; 1/36] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 1/36] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.3min\n",
      "[CV 9/10; 1/36] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 1/36] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.4min\n",
      "[CV 8/10; 2/36] START learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 2/36] END learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.3min\n",
      "[CV 6/10; 3/36] START learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 3/36] END learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.773 total time= 2.3min\n",
      "[CV 3/10; 4/36] START learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 4/36] END learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.1min\n",
      "[CV 1/10; 5/36] START learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 5/36] END learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.2min\n",
      "[CV 9/10; 5/36] START learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 5/36] END learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.2min\n",
      "[CV 7/10; 6/36] START learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 6/36] END learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.752 total time= 2.4min\n",
      "[CV 7/10; 7/36] START learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 7/36] END learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.753 total time= 2.1min\n",
      "[CV 5/10; 8/36] START learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 8/36] END learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.2min\n",
      "[CV 4/10; 9/36] START learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 9/36] END learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.9min\n",
      "[CV 1/10; 10/36] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 10/36] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.8min\n",
      "[CV 7/10; 10/36] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 10/36] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.751 total time= 2.6min\n",
      "[CV 5/10; 11/36] START learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 11/36] END learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.742 total time= 3.0min\n",
      "[CV 4/10; 12/36] START learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 12/36] END learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 3.3min\n",
      "[CV 3/10; 13/36] START learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 13/36] END learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.7min\n",
      "[CV 10/10; 13/36] START learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 13/36] END learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.732 total time= 2.5min\n",
      "[CV 8/10; 14/36] START learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 14/36] END learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.739 total time= 2.5min\n",
      "[CV 6/10; 15/36] START learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 15/36] END learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.772 total time= 2.4min\n",
      "[CV 4/10; 16/36] START learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 16/36] END learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.745 total time= 2.8min\n",
      "[CV 2/10; 17/36] START learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 17/36] END learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.761 total time= 3.1min\n",
      "[CV 10/10; 17/36] START learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 17/36] END learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.733 total time= 2.8min\n",
      "[CV 8/10; 18/36] START learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 18/36] END learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 2.8min\n",
      "[CV 6/10; 19/36] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 19/36] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.770 total time= 2.7min\n",
      "[CV 4/10; 20/36] START learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 20/36] END learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.6min\n",
      "[CV 2/10; 21/36] START learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 21/36] END learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.761 total time= 2.9min\n",
      "[CV 10/10; 21/36] START learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 21/36] END learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.734 total time= 2.4min\n",
      "[CV 9/10; 22/36] START learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 22/36] END learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.2min\n",
      "[CV 7/10; 23/36] START learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 23/36] END learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.752 total time= 2.8min\n",
      "[CV 6/10; 24/36] START learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 24/36] END learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.771 total time= 2.3min\n",
      "[CV 4/10; 25/36] START learning_rate=0.035, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 25/36] END learning_rate=0.035, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.744 total time= 2.4min\n",
      "[CV 2/10; 1/36] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 1/36] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.763 total time= 2.4min\n",
      "[CV 3/10; 2/36] START learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 2/36] END learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.6min\n",
      "[CV 10/10; 2/36] START learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 2/36] END learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.734 total time= 2.2min\n",
      "[CV 8/10; 3/36] START learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 3/36] END learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.3min\n",
      "[CV 8/10; 4/36] START learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 4/36] END learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.1min\n",
      "[CV 5/10; 5/36] START learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 5/36] END learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.2min\n",
      "[CV 4/10; 6/36] START learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 6/36] END learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.3min\n",
      "[CV 1/10; 7/36] START learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 7/36] END learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.1min\n",
      "[CV 6/10; 7/36] START learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 7/36] END learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.771 total time= 2.1min\n",
      "[CV 3/10; 8/36] START learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 8/36] END learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.756 total time= 2.2min\n",
      "[CV 2/10; 9/36] START learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 9/36] END learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.761 total time= 2.9min\n",
      "[CV 10/10; 9/36] START learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 9/36] END learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.734 total time= 3.2min\n",
      "[CV 1/10; 11/36] START learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 11/36] END learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.7min\n",
      "[CV 10/10; 11/36] START learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 11/36] END learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.734 total time= 3.2min\n",
      "[CV 8/10; 12/36] START learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 12/36] END learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 3.2min\n",
      "[CV 6/10; 13/36] START learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 13/36] END learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.772 total time= 2.5min\n",
      "[CV 3/10; 14/36] START learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 14/36] END learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.7min\n",
      "[CV 1/10; 15/36] START learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 15/36] END learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.757 total time= 2.6min\n",
      "[CV 9/10; 15/36] START learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 15/36] END learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 2.5min\n",
      "[CV 7/10; 16/36] START learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 16/36] END learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.752 total time= 2.8min\n",
      "[CV 6/10; 17/36] START learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 17/36] END learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.773 total time= 3.1min\n",
      "[CV 4/10; 18/36] START learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 18/36] END learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.745 total time= 2.9min\n",
      "[CV 2/10; 19/36] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 19/36] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.761 total time= 2.4min\n",
      "[CV 9/10; 19/36] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 19/36] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.7min\n",
      "[CV 7/10; 20/36] START learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 20/36] END learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.752 total time= 2.5min\n",
      "[CV 5/10; 21/36] START learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 21/36] END learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 3.0min\n",
      "[CV 3/10; 22/36] START learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 22/36] END learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.759 total time= 2.2min\n",
      "[CV 2/10; 23/36] START learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 23/36] END learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.762 total time= 2.4min\n",
      "[CV 10/10; 23/36] START learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 23/36] END learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.731 total time= 2.7min\n",
      "[CV 7/10; 24/36] START learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 24/36] END learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.752 total time= 2.3min\n",
      "[CV 5/10; 25/36] START learning_rate=0.035, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 25/36] END learning_rate=0.035, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/10; 1/36] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 1/36] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.4min\n",
      "[CV 1/10; 2/36] START learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 2/36] END learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.6min\n",
      "[CV 1/10; 3/36] START learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 3/36] END learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.4min\n",
      "[CV 1/10; 4/36] START learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 4/36] END learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.757 total time= 2.1min\n",
      "[CV 5/10; 4/36] START learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 4/36] END learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.1min\n",
      "[CV 3/10; 5/36] START learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 5/36] END learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.2min\n",
      "[CV 1/10; 6/36] START learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 6/36] END learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.3min\n",
      "[CV 9/10; 6/36] START learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 6/36] END learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.3min\n",
      "[CV 9/10; 7/36] START learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 7/36] END learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.744 total time= 2.1min\n",
      "[CV 7/10; 8/36] START learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 8/36] END learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.753 total time= 2.3min\n",
      "[CV 6/10; 9/36] START learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 9/36] END learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.771 total time= 3.0min\n",
      "[CV 4/10; 10/36] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 10/36] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.9min\n",
      "[CV 10/10; 10/36] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 10/36] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.734 total time= 2.5min\n",
      "[CV 8/10; 11/36] START learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 11/36] END learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.741 total time= 3.1min\n",
      "[CV 6/10; 12/36] START learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 12/36] END learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.774 total time= 3.2min\n",
      "[CV 4/10; 13/36] START learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 13/36] END learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.745 total time= 2.7min\n",
      "[CV 2/10; 14/36] START learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 14/36] END learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.763 total time= 2.7min\n",
      "[CV 10/10; 14/36] START learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 14/36] END learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.732 total time= 2.4min\n",
      "[CV 7/10; 15/36] START learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 15/36] END learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.752 total time= 2.4min\n",
      "[CV 5/10; 16/36] START learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 16/36] END learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.8min\n",
      "[CV 4/10; 17/36] START learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 17/36] END learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.745 total time= 3.2min\n",
      "[CV 2/10; 18/36] START learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 18/36] END learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.761 total time= 3.0min\n",
      "[CV 10/10; 18/36] START learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 18/36] END learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.733 total time= 2.8min\n",
      "[CV 10/10; 19/36] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 19/36] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.734 total time= 2.7min\n",
      "[CV 8/10; 20/36] START learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 20/36] END learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.4min\n",
      "[CV 6/10; 21/36] START learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 21/36] END learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.770 total time= 2.9min\n",
      "[CV 4/10; 22/36] START learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 22/36] END learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.2min\n",
      "[CV 1/10; 23/36] START learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 23/36] END learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.756 total time= 2.4min\n",
      "[CV 9/10; 23/36] START learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 23/36] END learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.7min\n",
      "[CV 8/10; 24/36] START learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 24/36] END learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.741 total time= 2.3min\n",
      "[CV 6/10; 25/36] START learning_rate=0.035, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 25/36] END learning_rate=0.035, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.773 total time= 2.4min\n",
      "[CV 6/10; 1/36] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 1/36] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.773 total time= 2.3min\n",
      "[CV 10/10; 1/36] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 1/36] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.734 total time= 2.4min\n",
      "[CV 7/10; 2/36] START learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 2/36] END learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.751 total time= 2.3min\n",
      "[CV 5/10; 3/36] START learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 3/36] END learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.3min\n",
      "[CV 4/10; 4/36] START learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 4/36] END learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.1min\n",
      "[CV 2/10; 5/36] START learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 5/36] END learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.764 total time= 2.2min\n",
      "[CV 10/10; 5/36] START learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 5/36] END learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.732 total time= 2.2min\n",
      "[CV 8/10; 6/36] START learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 6/36] END learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.3min\n",
      "[CV 5/10; 7/36] START learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 7/36] END learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.1min\n",
      "[CV 4/10; 8/36] START learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 8/36] END learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.2min\n",
      "[CV 1/10; 9/36] START learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 9/36] END learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.9min\n",
      "[CV 9/10; 9/36] START learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 9/36] END learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.744 total time= 3.3min\n",
      "[CV 2/10; 11/36] START learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 11/36] END learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.759 total time= 2.6min\n",
      "[CV 9/10; 11/36] START learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 11/36] END learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.744 total time= 3.2min\n",
      "[CV 7/10; 12/36] START learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 12/36] END learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.751 total time= 3.2min\n",
      "[CV 5/10; 13/36] START learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 13/36] END learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.6min\n",
      "[CV 4/10; 14/36] START learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 14/36] END learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.745 total time= 2.7min\n",
      "[CV 2/10; 15/36] START learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 15/36] END learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.763 total time= 2.6min\n",
      "[CV 10/10; 15/36] START learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 15/36] END learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.732 total time= 2.5min\n",
      "[CV 10/10; 16/36] START learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 16/36] END learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.733 total time= 2.7min\n",
      "[CV 7/10; 17/36] START learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 17/36] END learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.752 total time= 3.2min\n",
      "[CV 6/10; 18/36] START learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 18/36] END learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.773 total time= 2.9min\n",
      "[CV 4/10; 19/36] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 19/36] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.4min\n",
      "[CV 2/10; 20/36] START learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 20/36] END learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.761 total time= 2.9min\n",
      "[CV 10/10; 20/36] START learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 20/36] END learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.734 total time= 2.4min\n",
      "[CV 8/10; 21/36] START learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 21/36] END learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.9min\n",
      "[CV 5/10; 22/36] START learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 22/36] END learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.2min\n",
      "[CV 3/10; 23/36] START learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 23/36] END learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.759 total time= 2.4min\n",
      "[CV 1/10; 24/36] START learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 24/36] END learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.756 total time= 2.7min\n",
      "[CV 9/10; 24/36] START learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 24/36] END learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.3min\n",
      "[CV 7/10; 25/36] START learning_rate=0.035, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 25/36] END learning_rate=0.035, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.754 total time= 2.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/10; 1/36] START learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 1/36] END learning_rate=0.02, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.4min\n",
      "[CV 6/10; 2/36] START learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 2/36] END learning_rate=0.02, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.773 total time= 2.6min\n",
      "[CV 2/10; 3/36] START learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 3/36] END learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.763 total time= 2.4min\n",
      "[CV 10/10; 3/36] START learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 3/36] END learning_rate=0.02, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.734 total time= 2.3min\n",
      "[CV 10/10; 4/36] START learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 4/36] END learning_rate=0.02, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.732 total time= 2.0min\n",
      "[CV 8/10; 5/36] START learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 5/36] END learning_rate=0.02, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.2min\n",
      "[CV 5/10; 6/36] START learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 6/36] END learning_rate=0.02, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.4min\n",
      "[CV 4/10; 7/36] START learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 7/36] END learning_rate=0.02, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.1min\n",
      "[CV 1/10; 8/36] START learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 8/36] END learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.758 total time= 2.3min\n",
      "[CV 9/10; 8/36] START learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 8/36] END learning_rate=0.02, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.744 total time= 2.2min\n",
      "[CV 7/10; 9/36] START learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 9/36] END learning_rate=0.02, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.753 total time= 3.0min\n",
      "[CV 5/10; 10/36] START learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 10/36] END learning_rate=0.03, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.742 total time= 3.0min\n",
      "[CV 4/10; 11/36] START learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 11/36] END learning_rate=0.03, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.742 total time= 2.6min\n",
      "[CV 2/10; 12/36] START learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 12/36] END learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.759 total time= 3.4min\n",
      "[CV 10/10; 12/36] START learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 12/36] END learning_rate=0.03, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.734 total time= 3.3min\n",
      "[CV 8/10; 13/36] START learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 13/36] END learning_rate=0.03, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.739 total time= 2.5min\n",
      "[CV 6/10; 14/36] START learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 14/36] END learning_rate=0.03, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.772 total time= 2.7min\n",
      "[CV 4/10; 15/36] START learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 15/36] END learning_rate=0.03, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.745 total time= 2.5min\n",
      "[CV 2/10; 16/36] START learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 16/36] END learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.761 total time= 2.3min\n",
      "[CV 9/10; 16/36] START learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 16/36] END learning_rate=0.03, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.744 total time= 2.8min\n",
      "[CV 8/10; 17/36] START learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 17/36] END learning_rate=0.03, max_depth=22, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.741 total time= 3.1min\n",
      "[CV 5/10; 18/36] START learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 5/10; 18/36] END learning_rate=0.03, max_depth=22, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.9min\n",
      "[CV 3/10; 19/36] START learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 3/10; 19/36] END learning_rate=0.035, max_depth=18, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.756 total time= 2.4min\n",
      "[CV 1/10; 20/36] START learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 1/10; 20/36] END learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.755 total time= 2.9min\n",
      "[CV 9/10; 20/36] START learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 9/10; 20/36] END learning_rate=0.035, max_depth=18, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.741 total time= 2.4min\n",
      "[CV 7/10; 21/36] START learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 7/10; 21/36] END learning_rate=0.035, max_depth=18, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.752 total time= 3.0min\n",
      "[CV 6/10; 22/36] START learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 6/10; 22/36] END learning_rate=0.035, max_depth=20, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.771 total time= 2.2min\n",
      "[CV 4/10; 23/36] START learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1\n",
      "[CV 4/10; 23/36] END learning_rate=0.035, max_depth=20, n_estimators=3750, num_leaves=124, objective=regression_l1;, score=0.743 total time= 2.4min\n",
      "[CV 2/10; 24/36] START learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 2/10; 24/36] END learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.762 total time= 2.7min\n",
      "[CV 10/10; 24/36] START learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1\n",
      "[CV 10/10; 24/36] END learning_rate=0.035, max_depth=20, n_estimators=4000, num_leaves=124, objective=regression_l1;, score=0.731 total time= 2.3min\n",
      "[CV 8/10; 25/36] START learning_rate=0.035, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1\n",
      "[CV 8/10; 25/36] END learning_rate=0.035, max_depth=22, n_estimators=3500, num_leaves=124, objective=regression_l1;, score=0.740 total time= 2.5min\n",
      "The best estimator across ALL searched params:\n",
      " LGBMRegressor(learning_rate=0.04, max_depth=22, n_estimators=4000,\n",
      "              num_leaves=124, objective='regression_l1', random_state=420)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "lgbm = lightgbm.LGBMRegressor(random_state = 420, n_jobs = -1)\n",
    "\n",
    "# Using many estimators with low learning rates\n",
    "lgbm_params = {'learning_rate': [0.02, 0.03, 0.035, 0.04],\n",
    "                           'num_leaves':[124],\n",
    "                           'max_depth': [18, 20, 22],\n",
    "                           'n_estimators': [3500, 3750, 4000],\n",
    "                           'objective': ['regression_l1']}\n",
    "\n",
    "lgbm_grid_search = GridSearchCV(estimator = lgbm, \n",
    "                                param_grid =  lgbm_params,\n",
    "                                cv=10,\n",
    "                                verbose = 10,\n",
    "                                scoring = make_scorer(nrmsle, greater_is_better=True),\n",
    "                                n_jobs=-1)\n",
    "\n",
    "lgbm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"The best estimator across ALL searched params:\\n\", lgbm_grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator across ALL searched params:\n",
      " LGBMRegressor(learning_rate=0.04, max_depth=22, n_estimators=4000,\n",
      "              num_leaves=124, objective='regression_l1', random_state=420)\n",
      "The best score of all model parameters' combination on model: 0.7506\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>148.586348</td>\n",
       "      <td>2.362689</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 22, 'n_es...</td>\n",
       "      <td>0.750604</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>130.508386</td>\n",
       "      <td>3.516438</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 22, 'n_es...</td>\n",
       "      <td>0.750597</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>122.085660</td>\n",
       "      <td>2.817046</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 22, 'n_es...</td>\n",
       "      <td>0.750521</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>133.632622</td>\n",
       "      <td>2.992995</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750457</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>167.959406</td>\n",
       "      <td>3.523344</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750440</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>121.341240</td>\n",
       "      <td>2.955567</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750436</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>151.805331</td>\n",
       "      <td>3.558750</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 22, 'n_e...</td>\n",
       "      <td>0.750372</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>171.561890</td>\n",
       "      <td>4.101946</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 22, 'n_e...</td>\n",
       "      <td>0.750368</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>144.140115</td>\n",
       "      <td>3.735433</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 22, 'n_e...</td>\n",
       "      <td>0.750325</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145.289103</td>\n",
       "      <td>3.167706</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750303</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139.436378</td>\n",
       "      <td>3.091418</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750298</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137.947652</td>\n",
       "      <td>3.263466</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750295</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>177.533315</td>\n",
       "      <td>3.579660</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 22, 'n_es...</td>\n",
       "      <td>0.750294</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>144.705380</td>\n",
       "      <td>3.854327</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 20, 'n_es...</td>\n",
       "      <td>0.750256</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>131.329279</td>\n",
       "      <td>3.001306</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 22, 'n_es...</td>\n",
       "      <td>0.750229</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>193.054453</td>\n",
       "      <td>4.486458</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750220</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>169.840268</td>\n",
       "      <td>3.747934</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750205</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>122.281368</td>\n",
       "      <td>2.772044</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 22, 'n_es...</td>\n",
       "      <td>0.750200</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>153.981579</td>\n",
       "      <td>3.251766</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 20, 'n_es...</td>\n",
       "      <td>0.750200</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>151.447464</td>\n",
       "      <td>2.729367</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 20, 'n_es...</td>\n",
       "      <td>0.750194</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>159.484377</td>\n",
       "      <td>3.483417</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 18, 'n_es...</td>\n",
       "      <td>0.750179</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130.244660</td>\n",
       "      <td>2.897692</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 20, 'n_es...</td>\n",
       "      <td>0.750139</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>137.639057</td>\n",
       "      <td>3.041472</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 20, 'n_es...</td>\n",
       "      <td>0.750128</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121.074942</td>\n",
       "      <td>2.670685</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 20, 'n_es...</td>\n",
       "      <td>0.750114</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>140.529348</td>\n",
       "      <td>3.491024</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 20, 'n_e...</td>\n",
       "      <td>0.749877</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>150.713822</td>\n",
       "      <td>3.787830</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 20, 'n_e...</td>\n",
       "      <td>0.749824</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>128.705483</td>\n",
       "      <td>3.572664</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 20, 'n_e...</td>\n",
       "      <td>0.749799</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>154.659763</td>\n",
       "      <td>3.807709</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 22, 'n_es...</td>\n",
       "      <td>0.749784</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>168.445472</td>\n",
       "      <td>3.016711</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 22, 'n_es...</td>\n",
       "      <td>0.749780</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>180.835394</td>\n",
       "      <td>2.955358</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 22, 'n_es...</td>\n",
       "      <td>0.749773</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>165.651607</td>\n",
       "      <td>3.871995</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 18, 'n_e...</td>\n",
       "      <td>0.749567</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>149.817522</td>\n",
       "      <td>3.384131</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 18, 'n_e...</td>\n",
       "      <td>0.749556</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>150.395102</td>\n",
       "      <td>3.174686</td>\n",
       "      <td>{'learning_rate': 0.035, 'max_depth': 18, 'n_e...</td>\n",
       "      <td>0.749511</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>132.089614</td>\n",
       "      <td>3.324286</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 20, 'n_es...</td>\n",
       "      <td>0.749380</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>165.007004</td>\n",
       "      <td>4.107224</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 20, 'n_es...</td>\n",
       "      <td>0.749376</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>147.672364</td>\n",
       "      <td>3.288655</td>\n",
       "      <td>{'learning_rate': 0.04, 'max_depth': 20, 'n_es...</td>\n",
       "      <td>0.749351</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  \\\n",
       "35     148.586348         2.362689   \n",
       "34     130.508386         3.516438   \n",
       "33     122.085660         2.817046   \n",
       "28     133.632622         2.992995   \n",
       "29     167.959406         3.523344   \n",
       "27     121.341240         2.955567   \n",
       "26     151.805331         3.558750   \n",
       "25     171.561890         4.101946   \n",
       "24     144.140115         3.735433   \n",
       "1      145.289103         3.167706   \n",
       "0      139.436378         3.091418   \n",
       "2      137.947652         3.263466   \n",
       "8      177.533315         3.579660   \n",
       "14     144.705380         3.854327   \n",
       "7      131.329279         3.001306   \n",
       "11     193.054453         4.486458   \n",
       "10     169.840268         3.747934   \n",
       "6      122.281368         2.772044   \n",
       "13     153.981579         3.251766   \n",
       "12     151.447464         2.729367   \n",
       "9      159.484377         3.483417   \n",
       "4      130.244660         2.897692   \n",
       "5      137.639057         3.041472   \n",
       "3      121.074942         2.670685   \n",
       "23     140.529348         3.491024   \n",
       "22     150.713822         3.787830   \n",
       "21     128.705483         3.572664   \n",
       "15     154.659763         3.807709   \n",
       "17     168.445472         3.016711   \n",
       "16     180.835394         2.955358   \n",
       "20     165.651607         3.871995   \n",
       "19     149.817522         3.384131   \n",
       "18     150.395102         3.174686   \n",
       "31     132.089614         3.324286   \n",
       "32     165.007004         4.107224   \n",
       "30     147.672364         3.288655   \n",
       "\n",
       "                                               params  mean_test_score  \\\n",
       "35  {'learning_rate': 0.04, 'max_depth': 22, 'n_es...         0.750604   \n",
       "34  {'learning_rate': 0.04, 'max_depth': 22, 'n_es...         0.750597   \n",
       "33  {'learning_rate': 0.04, 'max_depth': 22, 'n_es...         0.750521   \n",
       "28  {'learning_rate': 0.04, 'max_depth': 18, 'n_es...         0.750457   \n",
       "29  {'learning_rate': 0.04, 'max_depth': 18, 'n_es...         0.750440   \n",
       "27  {'learning_rate': 0.04, 'max_depth': 18, 'n_es...         0.750436   \n",
       "26  {'learning_rate': 0.035, 'max_depth': 22, 'n_e...         0.750372   \n",
       "25  {'learning_rate': 0.035, 'max_depth': 22, 'n_e...         0.750368   \n",
       "24  {'learning_rate': 0.035, 'max_depth': 22, 'n_e...         0.750325   \n",
       "1   {'learning_rate': 0.02, 'max_depth': 18, 'n_es...         0.750303   \n",
       "0   {'learning_rate': 0.02, 'max_depth': 18, 'n_es...         0.750298   \n",
       "2   {'learning_rate': 0.02, 'max_depth': 18, 'n_es...         0.750295   \n",
       "8   {'learning_rate': 0.02, 'max_depth': 22, 'n_es...         0.750294   \n",
       "14  {'learning_rate': 0.03, 'max_depth': 20, 'n_es...         0.750256   \n",
       "7   {'learning_rate': 0.02, 'max_depth': 22, 'n_es...         0.750229   \n",
       "11  {'learning_rate': 0.03, 'max_depth': 18, 'n_es...         0.750220   \n",
       "10  {'learning_rate': 0.03, 'max_depth': 18, 'n_es...         0.750205   \n",
       "6   {'learning_rate': 0.02, 'max_depth': 22, 'n_es...         0.750200   \n",
       "13  {'learning_rate': 0.03, 'max_depth': 20, 'n_es...         0.750200   \n",
       "12  {'learning_rate': 0.03, 'max_depth': 20, 'n_es...         0.750194   \n",
       "9   {'learning_rate': 0.03, 'max_depth': 18, 'n_es...         0.750179   \n",
       "4   {'learning_rate': 0.02, 'max_depth': 20, 'n_es...         0.750139   \n",
       "5   {'learning_rate': 0.02, 'max_depth': 20, 'n_es...         0.750128   \n",
       "3   {'learning_rate': 0.02, 'max_depth': 20, 'n_es...         0.750114   \n",
       "23  {'learning_rate': 0.035, 'max_depth': 20, 'n_e...         0.749877   \n",
       "22  {'learning_rate': 0.035, 'max_depth': 20, 'n_e...         0.749824   \n",
       "21  {'learning_rate': 0.035, 'max_depth': 20, 'n_e...         0.749799   \n",
       "15  {'learning_rate': 0.03, 'max_depth': 22, 'n_es...         0.749784   \n",
       "17  {'learning_rate': 0.03, 'max_depth': 22, 'n_es...         0.749780   \n",
       "16  {'learning_rate': 0.03, 'max_depth': 22, 'n_es...         0.749773   \n",
       "20  {'learning_rate': 0.035, 'max_depth': 18, 'n_e...         0.749567   \n",
       "19  {'learning_rate': 0.035, 'max_depth': 18, 'n_e...         0.749556   \n",
       "18  {'learning_rate': 0.035, 'max_depth': 18, 'n_e...         0.749511   \n",
       "31  {'learning_rate': 0.04, 'max_depth': 20, 'n_es...         0.749380   \n",
       "32  {'learning_rate': 0.04, 'max_depth': 20, 'n_es...         0.749376   \n",
       "30  {'learning_rate': 0.04, 'max_depth': 20, 'n_es...         0.749351   \n",
       "\n",
       "    rank_test_score  \n",
       "35                1  \n",
       "34                2  \n",
       "33                3  \n",
       "28                4  \n",
       "29                5  \n",
       "27                6  \n",
       "26                7  \n",
       "25                8  \n",
       "24                9  \n",
       "1                10  \n",
       "0                11  \n",
       "2                12  \n",
       "8                13  \n",
       "14               14  \n",
       "7                15  \n",
       "11               16  \n",
       "10               17  \n",
       "6                18  \n",
       "13               19  \n",
       "12               20  \n",
       "9                21  \n",
       "4                22  \n",
       "5                23  \n",
       "3                24  \n",
       "23               25  \n",
       "22               26  \n",
       "21               27  \n",
       "15               28  \n",
       "17               29  \n",
       "16               30  \n",
       "20               31  \n",
       "19               32  \n",
       "18               33  \n",
       "31               34  \n",
       "32               35  \n",
       "30               36  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The best estimator across ALL searched params:\\n\", lgbm_grid_search.best_estimator_)\n",
    "\n",
    "print(\"The best score of all model parameters' combination on model: {:.4f}\".format(lgbm_grid_search.best_score_))\n",
    "display(pd.DataFrame(lgbm_grid_search.cv_results_).sort_values(by=['rank_test_score'])[['mean_fit_time', 'mean_score_time', \n",
    "                                           'params', 'mean_test_score', 'rank_test_score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgbm_grid_search.best_estimator_\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='Feature importance', ylabel='Features'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1130.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(font_scale = 1)\n",
    "lightgbm.plot_importance(model, height=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving-exporting the results to a .csv file\n",
    "pd.DataFrame(predictions, columns = ['Price']).to_csv('submission_lgbm_best_final.csv', index=False)\n",
    "# 0.76038"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DSC510_3010.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
